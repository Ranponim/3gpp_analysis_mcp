"""
Database Query Optimization Tests

PostgreSQLRepositoryì˜ ì¿¼ë¦¬ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ìµœì í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
from datetime import datetime, timezone

import pytest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)



class TestDatabaseOptimization:
    """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    def test_fetch_peg_data_query_analysis(
        self,
        mock_performance_database_repository,
        performance_logger
    ):
        """fetch_peg_data ì¿¼ë¦¬ ë¶„ì„"""
        # í˜„ì¬ PostgreSQLRepositoryì˜ fetch_peg_data ë©”ì„œë“œ ë¶„ì„
        repo = mock_performance_database_repository
        
        # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
        table_name = "summary"
        columns = {
            "time": "datetime",
            "peg_name": "peg_name",
            "value": "value",
            "ne": "ne",
            "cellid": "cellid",
            "host": "host"
        }
        time_range = (
            datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
            datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
        )
        filters = {
            "ne": "nvgnb#10001",
            "cellid": ["cell_001", "cell_002", "cell_003"],
            "host": "192.168.1.100"
        }
        
        # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        result = repo.fetch_peg_data(
            table_name=table_name,
            columns=columns,
            time_range=time_range,
            filters=filters,
            limit=1000
        )
        end_time = time.time()
        
        query_time = end_time - start_time
        
        performance_logger.info("âœ… fetch_peg_data ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ")
        performance_logger.info(f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„: {query_time*1000:.2f}ms")
        performance_logger.info(f"ë°˜í™˜ëœ ë ˆì½”ë“œ ìˆ˜: {len(result) if hasattr(result, '__len__') else 'N/A'}")
        
        # ì¿¼ë¦¬ ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        assert query_time < 0.1, f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {query_time*1000:.2f}ms"
        
    def test_optimized_query_structure_analysis(self, performance_logger):
        """ìµœì í™”ëœ ì¿¼ë¦¬ êµ¬ì¡° ë¶„ì„"""
        # PostgreSQLRepositoryì˜ í˜„ì¬ ì¿¼ë¦¬ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ìµœì í™” ë°©ì•ˆ ì œì‹œ
        
        performance_logger.info("=== ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™” ë¶„ì„ ===")
        
        # 1. ì¸ë±ìŠ¤ ìµœì í™” ê¶Œì¥ì‚¬í•­
        recommended_indexes = [
            "CREATE INDEX CONCURRENTLY idx_summary_datetime_ne ON summary (datetime, ne);",
            "CREATE INDEX CONCURRENTLY idx_summary_cellid_datetime ON summary (cellid, datetime);",
            "CREATE INDEX CONCURRENTLY idx_summary_peg_name_datetime ON summary (peg_name, datetime);",
            "CREATE INDEX CONCURRENTLY idx_summary_composite ON summary (ne, cellid, datetime, peg_name);"
        ]
        
        performance_logger.info("ğŸ“Š ê¶Œì¥ ì¸ë±ìŠ¤:")
        for idx, query in enumerate(recommended_indexes, 1):
            performance_logger.info(f"  {idx}. {query}")
        
        # 2. ì¿¼ë¦¬ ìµœì í™” íŒ¨í„´
        optimization_patterns = [
            "ì‹œê°„ ë²”ìœ„ í•„í„°ë¥¼ WHERE ì ˆ ìµœìš°ì„ ìœ¼ë¡œ ë°°ì¹˜",
            "ë³µí•© ì¸ë±ìŠ¤ í™œìš©ì„ ìœ„í•œ í•„í„° ìˆœì„œ ìµœì í™”",
            "LIMIT ì ˆì„ í†µí•œ ê²°ê³¼ ì§‘í•© í¬ê¸° ì œí•œ",
            "SELECT ì ˆì—ì„œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ (SELECT *  í”¼í•˜ê¸°)",
            "JOIN ëŒ€ì‹  EXISTS ì‚¬ìš© ê³ ë ¤ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)"
        ]
        
        performance_logger.info("ğŸ”§ ì¿¼ë¦¬ ìµœì í™” íŒ¨í„´:")
        for idx, pattern in enumerate(optimization_patterns, 1):
            performance_logger.info(f"  {idx}. {pattern}")
        
        # 3. ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼
        performance_improvements = {
            "ì¸ë±ìŠ¤ ìµœì í™”": "50-80% ì¿¼ë¦¬ ì‹œê°„ ë‹¨ì¶•",
            "í•„í„° ìˆœì„œ ìµœì í™”": "10-20% ì¿¼ë¦¬ ì‹œê°„ ë‹¨ì¶•", 
            "ì»¬ëŸ¼ ì„ íƒ ìµœì í™”": "5-15% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ",
            "LIMIT ìµœì í™”": "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹œ 90% ì´ìƒ ê°œì„ "
        }
        
        performance_logger.info("ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
        for optimization, improvement in performance_improvements.items():
            performance_logger.info(f"  â€¢ {optimization}: {improvement}")
        
        performance_logger.info("âœ… ì¿¼ë¦¬ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ")
    
    def test_connection_pooling_optimization(
        self,
        mock_performance_database_repository,
        performance_logger
    ):
        """ì»¤ë„¥ì…˜ í’€ë§ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        repo = mock_performance_database_repository
        
        # ì—°ì† ì¿¼ë¦¬ ì‹¤í–‰ìœ¼ë¡œ ì»¤ë„¥ì…˜ í’€ íš¨ê³¼ ì¸¡ì •
        num_queries = 10
        query_times = []
        
        for i in range(num_queries):
            start_time = time.time()
            
            # ë‹¤ì–‘í•œ í•„í„° ì¡°ê±´ìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
            result = repo.fetch_peg_data(
                table_name="summary",
                columns={"datetime": "datetime", "peg_name": "peg_name", "value": "value"},
                time_range=(
                    datetime(2025, 1, 1, 9+i, 0, tzinfo=timezone.utc),
                    datetime(2025, 1, 1, 10+i, 0, tzinfo=timezone.utc)
                ),
                filters={"cellid": [f"cell_{j:03d}" for j in range(i+1, i+4)]},
                limit=100
            )
            
            end_time = time.time()
            query_times.append(end_time - start_time)
        
        # í†µê³„ ë¶„ì„
        avg_time = sum(query_times) / len(query_times)
        min_time = min(query_times)
        max_time = max(query_times)
        
        performance_logger.info("âœ… ì»¤ë„¥ì…˜ í’€ë§ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        performance_logger.info(f"ì´ ì¿¼ë¦¬ ìˆ˜: {num_queries}")
        performance_logger.info(f"í‰ê·  ì¿¼ë¦¬ ì‹œê°„: {avg_time*1000:.2f}ms")
        performance_logger.info(f"ìµœì†Œ ì¿¼ë¦¬ ì‹œê°„: {min_time*1000:.2f}ms")
        performance_logger.info(f"ìµœëŒ€ ì¿¼ë¦¬ ì‹œê°„: {max_time*1000:.2f}ms")
        performance_logger.info(f"ì‹œê°„ í¸ì°¨: {(max_time-min_time)*1000:.2f}ms")
        
        # ì»¤ë„¥ì…˜ í’€ íš¨ìœ¨ì„± ê²€ì¦
        time_variance = max_time - min_time
        assert time_variance < 0.05, f"ì¿¼ë¦¬ ì‹œê°„ í¸ì°¨ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {time_variance*1000:.2f}ms"
        assert avg_time < 0.02, f"í‰ê·  ì¿¼ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {avg_time*1000:.2f}ms"
    
    def test_query_parameter_optimization(
        self,
        mock_performance_database_repository,
        performance_logger
    ):
        """ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìµœì í™” í…ŒìŠ¤íŠ¸"""
        repo = mock_performance_database_repository
        
        # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_scenarios = [
            {
                "name": "ì†ŒëŸ‰ ë°ì´í„° (LIMIT 10)",
                "limit": 10,
                "filters": {"cellid": ["cell_001"]}
            },
            {
                "name": "ì¤‘ê°„ ë°ì´í„° (LIMIT 100)",
                "limit": 100,
                "filters": {"cellid": ["cell_001", "cell_002", "cell_003"]}
            },
            {
                "name": "ëŒ€ëŸ‰ ë°ì´í„° (LIMIT 1000)",
                "limit": 1000,
                "filters": {"ne": "nvgnb#10001"}
            },
            {
                "name": "ë³µí•© í•„í„°",
                "limit": 500,
                "filters": {
                    "ne": "nvgnb#10001",
                    "cellid": ["cell_001", "cell_002"],
                    "host": "192.168.1.100"
                }
            }
        ]
        
        performance_logger.info("=== ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìµœì í™” í…ŒìŠ¤íŠ¸ ===")
        
        for scenario in test_scenarios:
            start_time = time.time()
            
            result = repo.fetch_peg_data(
                table_name="summary",
                columns={
                    "datetime": "datetime",
                    "peg_name": "peg_name", 
                    "value": "value",
                    "ne": "ne",
                    "cellid": "cellid"
                },
                time_range=(
                    datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
                    datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
                ),
                filters=scenario["filters"],
                limit=scenario["limit"]
            )
            
            end_time = time.time()
            query_time = end_time - start_time
            
            performance_logger.info(f"ğŸ“Š {scenario['name']}:")
            performance_logger.info(f"   ì‹¤í–‰ì‹œê°„: {query_time*1000:.2f}ms")
            performance_logger.info(f"   LIMIT: {scenario['limit']}")
            performance_logger.info(f"   í•„í„° ìˆ˜: {len(scenario['filters'])}")
        
        performance_logger.info("âœ… ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def test_memory_efficient_data_retrieval(
        self,
        mock_performance_database_repository,
        memory_tracker,
        performance_logger
    ):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        repo = mock_performance_database_repository
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        initial_memory = memory_tracker.get_memory_usage()
        performance_logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory['current_mb']:.2f}MB")
        
        # ëŒ€ëŸ‰ ë°ì´í„° ì¡°íšŒ
        start_time = time.time()
        result = repo.fetch_peg_data(
            table_name="summary",
            columns={
                "datetime": "datetime",
                "peg_name": "peg_name",
                "value": "value",
                "ne": "ne",
                "cellid": "cellid",
                "host": "host"
            },
            time_range=(
                datetime(2025, 1, 1, 0, 0, tzinfo=timezone.utc),
                datetime(2025, 1, 2, 23, 59, tzinfo=timezone.utc)
            ),
            filters={},  # í•„í„° ì—†ìŒ - ìµœëŒ€ ë°ì´í„°
            limit=5000   # ëŒ€ëŸ‰ ë°ì´í„°
        )
        end_time = time.time()
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
        final_memory = memory_tracker.get_memory_usage()
        
        query_time = end_time - start_time
        memory_increase = final_memory['current_mb'] - initial_memory['current_mb']
        
        performance_logger.info("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ì¡°íšŒ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        performance_logger.info(f"ì¿¼ë¦¬ ì‹¤í–‰ì‹œê°„: {query_time*1000:.2f}ms")
        performance_logger.info(f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰: {memory_increase:.2f}MB")
        performance_logger.info(f"í”¼í¬ ë©”ëª¨ë¦¬: {final_memory['peak_mb']:.2f}MB")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê²€ì¦
        assert memory_increase < 50, f"ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {memory_increase:.2f}MB"
        assert final_memory['peak_mb'] < 200, f"í”¼í¬ ë©”ëª¨ë¦¬ê°€ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤: {final_memory['peak_mb']:.2f}MB"
    
    def test_database_query_benchmark_comparison(
        self,
        benchmark,
        mock_performance_database_repository,
        performance_logger
    ):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ"""
        repo = mock_performance_database_repository
        
        # ê¸°ë³¸ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬
        def basic_query():
            return repo.fetch_peg_data(
                table_name="summary",
                columns={"datetime": "datetime", "peg_name": "peg_name", "value": "value"},
                time_range=(
                    datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
                    datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
                ),
                filters={"cellid": ["cell_001"]},
                limit=100
            )
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark.pedantic(basic_query, iterations=20, rounds=5)
        
        performance_logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        performance_logger.info("ìƒì„¸ ê²°ê³¼ëŠ” pytest-benchmark ë¦¬í¬íŠ¸ ì°¸ì¡°")
        
        # ê²°ê³¼ ê²€ì¦
        assert result is not None, "ì¿¼ë¦¬ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤"


if __name__ == '__main__':
    pytest.main([__file__, '-v', '--benchmark-only'])
