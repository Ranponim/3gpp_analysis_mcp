"""
Data Processing Algorithm Optimization Tests

DataProcessorì™€ PEGCalculatorì˜ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
from datetime import datetime, timezone

import numpy as np
import pandas as pd
import pytest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)

from services import PEGCalculator
from utils import DataProcessor


class TestDataProcessingOptimization:
    """ë°ì´í„° ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    def test_peg_calculator_vectorized_operations(
        self,
        benchmark,
        large_peg_dataset,
        performance_logger
    ):
        """PEGCalculator ë²¡í„°í™” ì—°ì‚° ìµœì í™” í…ŒìŠ¤íŠ¸"""
        peg_calculator = PEGCalculator()
        
        # ê¸°ë³¸ ì§‘ê³„ ë²¤ì¹˜ë§ˆí¬
        def aggregate_basic():
            return peg_calculator.aggregate_peg_data(
                large_peg_dataset,
                aggregation_method='mean'
            )
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark.pedantic(aggregate_basic, iterations=10, rounds=3)
        
        # ê²°ê³¼ ê²€ì¦
        assert isinstance(result, dict)
        assert len(result) > 0
        
        performance_logger.info("âœ… PEGCalculator ë²¡í„°í™” ì—°ì‚° ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        performance_logger.info("ìƒì„¸ ê²°ê³¼ëŠ” pytest-benchmark ë¦¬í¬íŠ¸ ì°¸ì¡°")
    
    def test_peg_calculator_algorithm_optimization(
        self,
        large_peg_dataset,
        performance_logger
    ):
        """PEG ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„"""
        peg_calculator = PEGCalculator()
        
        # ë‹¤ì–‘í•œ ì§‘ê³„ ë°©ë²• ì„±ëŠ¥ ë¹„êµ
        aggregation_methods = ['sum', 'mean', 'min', 'max', 'count']
        performance_results = {}
        
        for method in aggregation_methods:
            start_time = time.time()
            
            result = peg_calculator.aggregate_peg_data(
                large_peg_dataset,
                aggregation_method=method
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            performance_results[method] = {
                'execution_time_ms': execution_time * 1000,
                'result_count': len(result) if result else 0
            }
            
            performance_logger.info(f"ğŸ“Š {method} ì§‘ê³„: {execution_time*1000:.2f}ms, ê²°ê³¼: {len(result) if result else 0}ê°œ")
        
        # ê°€ì¥ ë¹ ë¥¸ ì§‘ê³„ ë°©ë²• ì‹ë³„
        fastest_method = min(performance_results.keys(), 
                           key=lambda k: performance_results[k]['execution_time_ms'])
        
        performance_logger.info("âœ… PEG ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ë¶„ì„ ì™„ë£Œ")
        performance_logger.info(f"ğŸš€ ê°€ì¥ ë¹ ë¥¸ ì§‘ê³„ ë°©ë²•: {fastest_method} "
                              f"({performance_results[fastest_method]['execution_time_ms']:.2f}ms)")
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        for method, metrics in performance_results.items():
            assert metrics['execution_time_ms'] < 100, f"{method} ì§‘ê³„ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤: {metrics['execution_time_ms']:.2f}ms"
    
    def test_data_processor_pandas_optimization(
        self,
        benchmark,
        large_peg_dataset,
        performance_logger
    ):
        """DataProcessor pandas ìµœì í™” í…ŒìŠ¤íŠ¸"""
        data_processor = DataProcessor()
        
        # ë°ì´í„°ë¥¼ N-1ê³¼ Nìœ¼ë¡œ ë¶„í• 
        mid_point = len(large_peg_dataset) // 2
        n1_data = large_peg_dataset.iloc[:mid_point].to_dict('records')
        n_data = large_peg_dataset.iloc[mid_point:].to_dict('records')
        
        # Mock LLM ë¶„ì„ ê²°ê³¼
        mock_llm_results = {
            'summary': 'ìµœì í™” í…ŒìŠ¤íŠ¸: ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ ë¶„ì„',
            'insights': 'pandas ë²¡í„°í™” ì—°ì‚°ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ '
        }
        
        def process_data():
            return data_processor.process_data(
                n_minus_1_data=n1_data,
                n_data=n_data,
                llm_analysis_results=mock_llm_results
            )
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark.pedantic(process_data, iterations=5, rounds=3)
        
        # ê²°ê³¼ ê²€ì¦
        assert isinstance(result, list)
        assert len(result) > 0
        
        performance_logger.info("âœ… DataProcessor pandas ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
    
    def test_memory_efficient_dataframe_operations(
        self,
        large_peg_dataset,
        memory_tracker,
        performance_logger
    ):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ DataFrame ì—°ì‚° í…ŒìŠ¤íŠ¸"""
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        initial_memory = memory_tracker.get_memory_usage()
        performance_logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory['current_mb']:.2f}MB")
        
        # DataFrame ìµœì í™” ê¸°ë²• í…ŒìŠ¤íŠ¸
        performance_logger.info("=== DataFrame ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ===")
        
        # 1. ë°ì´í„° íƒ€ì… ìµœì í™”
        optimized_df = large_peg_dataset.copy()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ìµœì í™” ì „)
        memory_tracker.update_peak()
        memory_tracker.get_memory_usage()
        
        # ë°ì´í„° íƒ€ì… ìµœì í™” ì ìš©
        if 'value' in optimized_df.columns:
            # float64 â†’ float32ë¡œ ë‹¤ìš´ìºìŠ¤íŒ… (ë©”ëª¨ë¦¬ 50% ì ˆì•½)
            optimized_df['value'] = pd.to_numeric(optimized_df['value'], downcast='float')
        
        if 'peg_name' in optimized_df.columns:
            # object â†’ categoryë¡œ ë³€í™˜ (ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì— íš¨ê³¼ì )
            optimized_df['peg_name'] = optimized_df['peg_name'].astype('category')
        
        if 'cellid' in optimized_df.columns:
            optimized_df['cellid'] = optimized_df['cellid'].astype('category')
        
        if 'ne' in optimized_df.columns:
            optimized_df['ne'] = optimized_df['ne'].astype('category')
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ìµœì í™” í›„)
        memory_tracker.update_peak()
        memory_tracker.get_memory_usage()
        
        # 2. ë²¡í„°í™” ì—°ì‚° í…ŒìŠ¤íŠ¸
        start_time = time.time()
        
        # ê·¸ë£¹ë³„ ì§‘ê³„ (ë²¡í„°í™” ì—°ì‚°)
        aggregated_data = optimized_df.groupby(['peg_name']).agg({
            'value': ['mean', 'sum', 'count', 'std']
        }).round(2)
        
        end_time = time.time()
        aggregation_time = end_time - start_time
        
        # 3. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê²€ì¦
        original_memory = large_peg_dataset.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        optimized_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        memory_savings = original_memory - optimized_memory
        memory_savings_pct = (memory_savings / original_memory) * 100
        
        performance_logger.info("ğŸ“Š DataFrame ë©”ëª¨ë¦¬ ìµœì í™” ê²°ê³¼:")
        performance_logger.info(f"   ì›ë³¸ DataFrame: {original_memory:.2f}MB")
        performance_logger.info(f"   ìµœì í™” DataFrame: {optimized_memory:.2f}MB")
        performance_logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_savings:.2f}MB ({memory_savings_pct:.1f}%)")
        performance_logger.info(f"   ë²¡í„°í™” ì§‘ê³„ ì‹œê°„: {aggregation_time*1000:.2f}ms")
        
        # ìµœì í™” íš¨ê³¼ ê²€ì¦
        assert memory_savings > 0, "ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
        assert memory_savings_pct > 10, f"ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ê°€ ë¯¸ë¯¸í•©ë‹ˆë‹¤: {memory_savings_pct:.1f}%"
        assert aggregation_time < 0.1, f"ë²¡í„°í™” ì§‘ê³„ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤: {aggregation_time*1000:.2f}ms"
        
        performance_logger.info("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ DataFrame ì—°ì‚° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def test_chunked_data_processing(
        self,
        performance_logger
    ):
        """ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„± (10,000ê°œ ë ˆì½”ë“œ)
        large_dataset = generate_large_test_dataset(size=10000)
        
        performance_logger.info("=== ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===")
        performance_logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {len(large_dataset)}ê°œ ë ˆì½”ë“œ")
        
        # 1. ì „ì²´ ë°ì´í„° í•œë²ˆì— ì²˜ë¦¬
        start_time = time.time()
        full_result = large_dataset.groupby('peg_name').agg({
            'value': ['mean', 'sum', 'count']
        })
        full_processing_time = time.time() - start_time
        
        # 2. ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
        chunk_size = 1000
        chunk_results = []
        
        start_time = time.time()
        for i in range(0, len(large_dataset), chunk_size):
            chunk = large_dataset.iloc[i:i+chunk_size]
            chunk_result = chunk.groupby('peg_name').agg({
                'value': ['mean', 'sum', 'count']
            })
            chunk_results.append(chunk_result)
        
        # ì²­í¬ ê²°ê³¼ ë³‘í•©
        if chunk_results:
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°ì„ ìœ„í•œ ë³‘í•©
            combined_result = pd.concat(chunk_results).groupby('peg_name').agg({
                ('value', 'sum'): 'sum',
                ('value', 'count'): 'sum'
            })
            combined_result[('value', 'mean')] = (
                combined_result[('value', 'sum')] / combined_result[('value', 'count')]
            )
        
        chunk_processing_time = time.time() - start_time
        
        # ê²°ê³¼ ë¹„êµ
        performance_logger.info("ğŸ“Š ì²˜ë¦¬ ë°©ì‹ë³„ ì„±ëŠ¥ ë¹„êµ:")
        performance_logger.info(f"   ì „ì²´ ì²˜ë¦¬: {full_processing_time*1000:.2f}ms")
        performance_logger.info(f"   ì²­í¬ ì²˜ë¦¬: {chunk_processing_time*1000:.2f}ms")
        performance_logger.info(f"   ì²­í¬ í¬ê¸°: {chunk_size}ê°œ")
        performance_logger.info(f"   ì²­í¬ ìˆ˜: {len(chunk_results)}ê°œ")
        
        # ì„±ëŠ¥ ê°œì„  ì—¬ë¶€ í™•ì¸
        if chunk_processing_time < full_processing_time:
            improvement = ((full_processing_time - chunk_processing_time) / full_processing_time) * 100
            performance_logger.info(f"ğŸš€ ì²­í¬ ì²˜ë¦¬ê°€ {improvement:.1f}% ë” ë¹ ë¦„")
        else:
            performance_logger.info("ğŸ“ ì „ì²´ ì²˜ë¦¬ê°€ ë” íš¨ìœ¨ì  (ì‘ì€ ë°ì´í„°ì…‹)")
        
        performance_logger.info("âœ… ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def test_optimized_change_rate_calculation(
        self,
        benchmark,
        large_peg_dataset,
        performance_logger
    ):
        """ìµœì í™”ëœ ë³€í™”ìœ¨ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        DataProcessor()
        
        # ë°ì´í„°ë¥¼ N-1ê³¼ Nìœ¼ë¡œ ë¶„í• 
        mid_point = len(large_peg_dataset) // 2
        n1_data = large_peg_dataset.iloc[:mid_point].to_dict('records')
        n_data = large_peg_dataset.iloc[mid_point:].to_dict('records')
        
        # ë³€í™”ìœ¨ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬
        def calculate_change_rates():
            # DataProcessorì˜ _merge_peg_dataì™€ _calculate_change_rates ì‹œë®¬ë ˆì´ì…˜
            n1_dict = {}
            n_dict = {}
            
            # N-1 ë°ì´í„° ê·¸ë£¹í™” (ë²¡í„°í™”)
            n1_df = pd.DataFrame(n1_data)
            if len(n1_df) > 0:
                n1_grouped = n1_df.groupby('peg_name')['value'].mean()
                n1_dict = n1_grouped.to_dict()
            
            # N ë°ì´í„° ê·¸ë£¹í™” (ë²¡í„°í™”)
            n_df = pd.DataFrame(n_data)
            if len(n_df) > 0:
                n_grouped = n_df.groupby('peg_name')['value'].mean()
                n_dict = n_grouped.to_dict()
            
            # ë³€í™”ìœ¨ ê³„ì‚° (ë²¡í„°í™”)
            change_rates = {}
            for peg_name in set(list(n1_dict.keys()) + list(n_dict.keys())):
                n1_value = n1_dict.get(peg_name, 0)
                n_value = n_dict.get(peg_name, 0)
                
                if n1_value != 0:
                    change_rate = ((n_value - n1_value) / n1_value) * 100
                else:
                    change_rate = 0.0 if n_value == 0 else float('inf')
                
                change_rates[peg_name] = round(change_rate, 2)
            
            return change_rates
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark.pedantic(calculate_change_rates, iterations=10, rounds=3)
        
        # ê²°ê³¼ ê²€ì¦
        assert isinstance(result, dict)
        assert len(result) > 0
        
        performance_logger.info("âœ… ìµœì í™”ëœ ë³€í™”ìœ¨ ê³„ì‚° ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
    
    def test_dataframe_memory_optimization(
        self,
        large_peg_dataset,
        memory_tracker,
        performance_logger
    ):
        """DataFrame ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        initial_memory = memory_tracker.get_memory_usage()
        
        performance_logger.info("=== DataFrame ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ===")
        performance_logger.info(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory['current_mb']:.2f}MB")
        
        # ì›ë³¸ DataFrame ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        original_memory = large_peg_dataset.memory_usage(deep=True).sum() / 1024 / 1024
        performance_logger.info(f"ì›ë³¸ DataFrame: {original_memory:.2f}MB")
        
        # 1. ë°ì´í„° íƒ€ì… ìµœì í™”
        optimized_df = large_peg_dataset.copy()
        
        # float64 â†’ float32 ë‹¤ìš´ìºìŠ¤íŒ…
        if 'value' in optimized_df.columns:
            optimized_df['value'] = pd.to_numeric(optimized_df['value'], downcast='float')
        
        # ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì„ categoryë¡œ ë³€í™˜
        categorical_columns = ['peg_name', 'cellid', 'ne', 'host']
        for col in categorical_columns:
            if col in optimized_df.columns:
                optimized_df[col] = optimized_df[col].astype('category')
        
        # ìµœì í™”ëœ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        optimized_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
        memory_savings = original_memory - optimized_memory
        memory_savings_pct = (memory_savings / original_memory) * 100
        
        performance_logger.info("ğŸ“Š ë©”ëª¨ë¦¬ ìµœì í™” ê²°ê³¼:")
        performance_logger.info(f"   ìµœì í™” DataFrame: {optimized_memory:.2f}MB")
        performance_logger.info(f"   ë©”ëª¨ë¦¬ ì ˆì•½: {memory_savings:.2f}MB ({memory_savings_pct:.1f}%)")
        
        # 2. ì—°ì‚° ì„±ëŠ¥ ë¹„êµ
        # ì›ë³¸ DataFrame ì—°ì‚°
        start_time = time.time()
        large_peg_dataset.groupby('peg_name')['value'].mean()
        original_time = time.time() - start_time
        
        # ìµœì í™”ëœ DataFrame ì—°ì‚°
        start_time = time.time()
        optimized_df.groupby('peg_name')['value'].mean()
        optimized_time = time.time() - start_time
        
        # ì„±ëŠ¥ ë¹„êµ
        if optimized_time < original_time:
            speed_improvement = ((original_time - optimized_time) / original_time) * 100
            performance_logger.info(f"ğŸš€ ì—°ì‚° ì†ë„ ê°œì„ : {speed_improvement:.1f}%")
        else:
            performance_logger.info("ğŸ“ ì—°ì‚° ì†ë„ëŠ” ìœ ì‚¬í•¨")
        
        performance_logger.info(f"   ì›ë³¸ ì—°ì‚° ì‹œê°„: {original_time*1000:.2f}ms")
        performance_logger.info(f"   ìµœì í™” ì—°ì‚° ì‹œê°„: {optimized_time*1000:.2f}ms")
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
        final_memory = memory_tracker.get_memory_usage()
        performance_logger.info(f"ìµœì¢… ë©”ëª¨ë¦¬: {final_memory['current_mb']:.2f}MB")
        
        # ìµœì í™” íš¨ê³¼ ê²€ì¦
        assert memory_savings_pct > 20, f"ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ê°€ ë¯¸ë¯¸í•©ë‹ˆë‹¤: {memory_savings_pct:.1f}%"
        # ë©”ëª¨ë¦¬ ìµœì í™”ê°€ ì£¼ ëª©ì ì´ë¯€ë¡œ ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜ëŠ” í—ˆìš© (20% ì´ë‚´)
        assert optimized_time <= original_time * 1.2, f"ìµœì í™” í›„ ì„±ëŠ¥ì´ 20% ì´ìƒ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤: {optimized_time*1000:.2f}ms vs {original_time*1000:.2f}ms"
        
        performance_logger.info("âœ… DataFrame ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


def generate_large_test_dataset(size: int = 10000) -> pd.DataFrame:
    """ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
    
    base_time = datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc)
    
    data = {
        'datetime': [base_time + pd.Timedelta(minutes=i) for i in range(size)],
        'peg_name': np.random.choice([
            'preamble_count', 'response_count', 'success_count', 
            'failure_count', 'handover_count'
        ], size),
        'value': np.random.normal(1000, 200, size),
        'ne': np.random.choice([f'nvgnb#{10000+i}' for i in range(20)], size),
        'cellid': np.random.choice([f'cell_{i:03d}' for i in range(50)], size),
        'host': np.random.choice([f'192.168.1.{i}' for i in range(1, 21)], size)
    }
    
    return pd.DataFrame(data)


if __name__ == '__main__':
    pytest.main([__file__, '-v', '--benchmark-only'])
