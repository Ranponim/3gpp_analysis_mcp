"""
LLM Call Efficiency and Prompt Optimization Tests

LLM í˜¸ì¶œ íš¨ìœ¨ì„±ê³¼ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
from datetime import datetime, timezone

import pytest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, project_root)

from models import LLMAnalysisResult
from services import LLMAnalysisService


class TestLLMOptimization:
    """LLM ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    def test_prompt_efficiency_analysis(
        self,
        mock_performance_llm_repository,
        performance_logger
    ):
        """í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì„± ë¶„ì„"""
        llm_service = LLMAnalysisService(
            llm_repository=mock_performance_llm_repository
        )
        
        # ë‹¤ì–‘í•œ ë¶„ì„ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ ë¶„ì„
        analysis_types = ['overall', 'enhanced', 'specific']
        
        # ìƒ˜í”Œ ë°ì´í„°
        import pandas as pd
        sample_df = pd.DataFrame([
            {'peg_name': 'preamble_count', 'n_minus_1_value': 1000, 'n_value': 1100, 'change_pct': 10.0},
            {'peg_name': 'response_count', 'n_minus_1_value': 950, 'n_value': 1000, 'change_pct': 5.26}
        ])
        
        # ì‹œê°„ ë²”ìœ„
        from models import TimeRange
        test_range = TimeRange(
            start_time=datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
            end_time=datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
        )
        
        performance_logger.info("=== í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì„± ë¶„ì„ ===")
        
        prompt_analysis = {}
        for analysis_type in analysis_types:
            # í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            # LLM ì„œë¹„ìŠ¤ í˜¸ì¶œ
            result = llm_service.analyze_peg_data(
                processed_df=sample_df,
                n1_range=test_range,
                n_range=test_range,
                analysis_type=analysis_type
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # Mockì—ì„œ í† í° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            estimated_tokens = mock_performance_llm_repository.estimate_tokens.return_value
            
            prompt_analysis[analysis_type] = {
                'processing_time_ms': processing_time * 1000,
                'estimated_tokens': estimated_tokens,
                'result_type': type(result).__name__
            }
            
            performance_logger.info(f"ğŸ“Š {analysis_type} ë¶„ì„:")
            performance_logger.info(f"   ì²˜ë¦¬ì‹œê°„: {processing_time*1000:.2f}ms")
            performance_logger.info(f"   ì¶”ì • í† í°: {estimated_tokens}")
        
        # ê°€ì¥ íš¨ìœ¨ì ì¸ ë¶„ì„ íƒ€ì… ì‹ë³„
        most_efficient = min(prompt_analysis.keys(), 
                           key=lambda k: prompt_analysis[k]['processing_time_ms'])
        
        performance_logger.info(f"ğŸš€ ê°€ì¥ íš¨ìœ¨ì ì¸ ë¶„ì„ íƒ€ì…: {most_efficient}")
        performance_logger.info("âœ… í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì„± ë¶„ì„ ì™„ë£Œ")
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        for analysis_type, metrics in prompt_analysis.items():
            assert metrics['processing_time_ms'] < 1000, f"{analysis_type} ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤"
            assert metrics['estimated_tokens'] > 0, f"{analysis_type} í† í° ì¶”ì • ì‹¤íŒ¨"
    
    def test_llm_call_optimization_benchmark(
        self,
        benchmark,
        mock_performance_llm_repository,
        performance_logger
    ):
        """LLM í˜¸ì¶œ ìµœì í™” ë²¤ì¹˜ë§ˆí¬"""
        llm_service = LLMAnalysisService(
            llm_repository=mock_performance_llm_repository
        )
        
        # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
        import pandas as pd
        test_df = pd.DataFrame([
            {'peg_name': 'test_peg', 'n_minus_1_value': 500, 'n_value': 550, 'change_pct': 10.0}
        ])
        
        from models import TimeRange
        test_range = TimeRange(
            start_time=datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
            end_time=datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
        )
        
        # LLM í˜¸ì¶œ ë²¤ì¹˜ë§ˆí¬
        def llm_analysis():
            return llm_service.analyze_peg_data(
                processed_df=test_df,
                n1_range=test_range,
                n_range=test_range,
                analysis_type='enhanced'
            )
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = benchmark.pedantic(llm_analysis, iterations=10, rounds=3)
        
        # ê²°ê³¼ ê²€ì¦
        assert isinstance(result, LLMAnalysisResult)
        assert result.model_used is not None
        
        performance_logger.info("âœ… LLM í˜¸ì¶œ ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
    
    def test_token_usage_optimization(
        self,
        mock_performance_llm_repository,
        performance_logger
    ):
        """í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # Mock LLM Repository ì„¤ì •
        mock_performance_llm_repository.estimate_tokens.return_value = 150
        mock_performance_llm_repository.validate_prompt.return_value = True
        
        llm_service = LLMAnalysisService(
            llm_repository=mock_performance_llm_repository
        )
        
        # ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ë³„ í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„
        data_sizes = [10, 50, 100, 500]
        token_usage_analysis = {}
        
        performance_logger.info("=== í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” ë¶„ì„ ===")
        
        for size in data_sizes:
            # í¬ê¸°ë³„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            import pandas as pd
            test_df = pd.DataFrame([
                {
                    'peg_name': f'test_peg_{i}',
                    'n_minus_1_value': 1000 + i,
                    'n_value': 1100 + i,
                    'change_pct': (i % 20) - 10  # -10% ~ +10% ë²”ìœ„
                }
                for i in range(size)
            ])
            
            from models import TimeRange
            test_range = TimeRange(
                start_time=datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
                end_time=datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
            )
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¸¡ì •
            start_time = time.time()
            
            result = llm_service.analyze_peg_data(
                processed_df=test_df,
                n1_range=test_range,
                n_range=test_range,
                analysis_type='enhanced'
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # í† í° ì •ë³´ ìˆ˜ì§‘
            estimated_tokens = mock_performance_llm_repository.estimate_tokens.return_value
            tokens_per_record = estimated_tokens / size if size > 0 else 0
            
            token_usage_analysis[size] = {
                'processing_time_ms': processing_time * 1000,
                'estimated_tokens': estimated_tokens,
                'tokens_per_record': tokens_per_record,
                'efficiency_score': size / (estimated_tokens + 1)  # ë ˆì½”ë“œ ìˆ˜ ëŒ€ë¹„ í† í° íš¨ìœ¨ì„±
            }
            
            performance_logger.info(f"ğŸ“Š ë°ì´í„° í¬ê¸° {size}ê°œ:")
            performance_logger.info(f"   ì²˜ë¦¬ì‹œê°„: {processing_time*1000:.2f}ms")
            performance_logger.info(f"   ì¶”ì • í† í°: {estimated_tokens}")
            performance_logger.info(f"   ë ˆì½”ë“œë‹¹ í† í°: {tokens_per_record:.2f}")
        
        # í† í° íš¨ìœ¨ì„± ë¶„ì„
        most_efficient_size = max(token_usage_analysis.keys(), 
                                key=lambda k: token_usage_analysis[k]['efficiency_score'])
        
        performance_logger.info(f"ğŸš€ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°ì´í„° í¬ê¸°: {most_efficient_size}ê°œ ë ˆì½”ë“œ")
        performance_logger.info("âœ… í† í° ì‚¬ìš©ëŸ‰ ìµœì í™” ë¶„ì„ ì™„ë£Œ")
        
        # í† í° íš¨ìœ¨ì„± ê²€ì¦
        for size, metrics in token_usage_analysis.items():
            assert metrics['processing_time_ms'] < 2000, f"í¬ê¸° {size} ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤"
            assert metrics['tokens_per_record'] >= 0, "í† í° ê³„ì‚° ì˜¤ë¥˜"
    
    def test_prompt_strategy_optimization(
        self,
        mock_performance_llm_repository,
        performance_logger
    ):
        """í”„ë¡¬í”„íŠ¸ ì „ëµ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # ë‹¤ì–‘í•œ ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ
        strategies = ['overall', 'enhanced', 'specific']
        strategy_performance = {}
        
        performance_logger.info("=== í”„ë¡¬í”„íŠ¸ ì „ëµ ìµœì í™” ë¶„ì„ ===")
        
        # Mock ì‘ë‹µì„ ì „ëµë³„ë¡œ ë‹¤ë¥´ê²Œ ì„¤ì •
        def mock_analyze_data(prompt, **kwargs):
            # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê¸°ë°˜ í† í° ì¶”ì •
            estimated_tokens = len(prompt.split()) * 1.3  # ëŒ€ëµì ì¸ í† í° ì¶”ì •
            
            if 'overall' in prompt.lower():
                return {
                    'summary': 'ì „ì²´ ë¶„ì„ (ê°„ê²°í•œ ì‘ë‹µ)',
                    'model_used': 'overall-strategy-model',
                    'tokens_used': int(estimated_tokens * 0.8)  # ê°„ê²°í•œ ì‘ë‹µ
                }
            elif 'enhanced' in prompt.lower():
                return {
                    'summary': 'í–¥ìƒëœ ë¶„ì„ (ìƒì„¸í•œ ì‘ë‹µ)',
                    'model_used': 'enhanced-strategy-model',
                    'tokens_used': int(estimated_tokens * 1.2)  # ìƒì„¸í•œ ì‘ë‹µ
                }
            else:
                return {
                    'summary': 'íŠ¹ì • ë¶„ì„ (ì¤‘ê°„ ì‘ë‹µ)',
                    'model_used': 'specific-strategy-model',
                    'tokens_used': int(estimated_tokens * 1.0)  # ì¤‘ê°„ ì‘ë‹µ
                }
        
        mock_performance_llm_repository.analyze_data.side_effect = mock_analyze_data
        
        llm_service = LLMAnalysisService(
            llm_repository=mock_performance_llm_repository
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        import pandas as pd
        test_df = pd.DataFrame([
            {'peg_name': 'strategy_test', 'n_minus_1_value': 1000, 'n_value': 1100, 'change_pct': 10.0}
        ])
        
        from models import TimeRange
        test_range = TimeRange(
            start_time=datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
            end_time=datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
        )
        
        # ê° ì „ëµë³„ ì„±ëŠ¥ ì¸¡ì •
        for strategy in strategies:
            start_time = time.time()
            
            result = llm_service.analyze_peg_data(
                processed_df=test_df,
                n1_range=test_range,
                n_range=test_range,
                analysis_type=strategy
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            strategy_performance[strategy] = {
                'processing_time_ms': processing_time * 1000,
                'tokens_used': result.tokens_used if hasattr(result, 'tokens_used') else 0,
                'model_used': result.model_used if hasattr(result, 'model_used') else 'unknown'
            }
            
            performance_logger.info(f"ğŸ“Š {strategy} ì „ëµ:")
            performance_logger.info(f"   ì²˜ë¦¬ì‹œê°„: {processing_time*1000:.2f}ms")
            performance_logger.info(f"   ì‚¬ìš© í† í°: {strategy_performance[strategy]['tokens_used']}")
            performance_logger.info(f"   ëª¨ë¸: {strategy_performance[strategy]['model_used']}")
        
        # ê°€ì¥ íš¨ìœ¨ì ì¸ ì „ëµ ì‹ë³„
        most_efficient_strategy = min(strategy_performance.keys(),
                                    key=lambda k: strategy_performance[k]['tokens_used'])
        
        fastest_strategy = min(strategy_performance.keys(),
                             key=lambda k: strategy_performance[k]['processing_time_ms'])
        
        performance_logger.info(f"ğŸš€ í† í° íš¨ìœ¨ì  ì „ëµ: {most_efficient_strategy}")
        performance_logger.info(f"âš¡ ê°€ì¥ ë¹ ë¥¸ ì „ëµ: {fastest_strategy}")
        performance_logger.info("âœ… í”„ë¡¬í”„íŠ¸ ì „ëµ ìµœì í™” ë¶„ì„ ì™„ë£Œ")
        
        # ì „ëµë³„ ì„±ëŠ¥ ê²€ì¦
        for strategy, metrics in strategy_performance.items():
            assert metrics['processing_time_ms'] < 1000, f"{strategy} ì „ëµì´ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤"
            assert metrics['tokens_used'] > 0, f"{strategy} ì „ëµ í† í° ì‚¬ìš©ëŸ‰ ì˜¤ë¥˜"
    
    def test_llm_response_caching_simulation(
        self,
        mock_performance_llm_repository,
        performance_logger
    ):
        """LLM ì‘ë‹µ ìºì‹± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
        # ê°„ë‹¨í•œ ìºì‹± ì‹œë®¬ë ˆì´ì…˜
        cache = {}
        cache_hits = 0
        cache_misses = 0
        
        def cached_llm_call(prompt_key, **kwargs):
            nonlocal cache_hits, cache_misses
            
            if prompt_key in cache:
                cache_hits += 1
                return cache[prompt_key]
            else:
                cache_misses += 1
                # ì‹¤ì œ LLM í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
                response = {
                    'summary': f'ìºì‹œëœ ì‘ë‹µ {prompt_key}',
                    'model_used': 'cached-model',
                    'tokens_used': 100
                }
                cache[prompt_key] = response
                return response
        
        # ìºì‹± íš¨ê³¼ í…ŒìŠ¤íŠ¸
        test_prompts = [
            'prompt_1', 'prompt_2', 'prompt_1',  # prompt_1 ë°˜ë³µ
            'prompt_3', 'prompt_2', 'prompt_1'   # ì¶”ê°€ ë°˜ë³µ
        ]
        
        performance_logger.info("=== LLM ì‘ë‹µ ìºì‹± ì‹œë®¬ë ˆì´ì…˜ ===")
        
        total_calls = 0
        total_time = 0
        
        for prompt_key in test_prompts:
            start_time = time.time()
            
            # ìºì‹œ í™•ì¸ ì‹œë®¬ë ˆì´ì…˜
            if prompt_key in cache:
                # ìºì‹œ íˆíŠ¸ (ì¦‰ì‹œ ë°˜í™˜)
                response = cached_llm_call(prompt_key)
                call_time = 0.001  # 1ms (ìºì‹œ ì ‘ê·¼ ì‹œê°„)
            else:
                # ìºì‹œ ë¯¸ìŠ¤ (ì‹¤ì œ LLM í˜¸ì¶œ)
                response = cached_llm_call(prompt_key)
                call_time = 0.1  # 100ms (ì‹¤ì œ LLM í˜¸ì¶œ ì‹œê°„)
            
            end_time = time.time()
            actual_time = end_time - start_time + call_time
            
            total_calls += 1
            total_time += actual_time
        
        # ìºì‹± íš¨ê³¼ ë¶„ì„
        cache_hit_rate = (cache_hits / total_calls) * 100
        avg_call_time = (total_time / total_calls) * 1000  # ms
        
        performance_logger.info("ğŸ“Š ìºì‹± íš¨ê³¼ ë¶„ì„:")
        performance_logger.info(f"   ì´ í˜¸ì¶œ ìˆ˜: {total_calls}")
        performance_logger.info(f"   ìºì‹œ íˆíŠ¸: {cache_hits}íšŒ")
        performance_logger.info(f"   ìºì‹œ ë¯¸ìŠ¤: {cache_misses}íšŒ")
        performance_logger.info(f"   ìºì‹œ ì ì¤‘ë¥ : {cache_hit_rate:.1f}%")
        performance_logger.info(f"   í‰ê·  í˜¸ì¶œ ì‹œê°„: {avg_call_time:.2f}ms")
        
        # ìºì‹± íš¨ê³¼ ê²€ì¦
        assert cache_hit_rate > 30, f"ìºì‹œ ì ì¤‘ë¥ ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {cache_hit_rate:.1f}%"
        assert avg_call_time < 60, f"í‰ê·  í˜¸ì¶œ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {avg_call_time:.2f}ms"  # ìºì‹± ê³ ë ¤í•˜ì—¬ 60msë¡œ ì¡°ì •
        
        performance_logger.info("âœ… LLM ì‘ë‹µ ìºì‹± ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ")
    
    def test_batch_llm_request_optimization(
        self,
        mock_performance_llm_repository,
        performance_logger
    ):
        """ë°°ì¹˜ LLM ìš”ì²­ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # ê°œë³„ ìš”ì²­ vs ë°°ì¹˜ ìš”ì²­ ì„±ëŠ¥ ë¹„êµ
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì—¬ëŸ¬ PEG ì„¸íŠ¸)
        import pandas as pd
        test_datasets = []
        for i in range(5):  # 5ê°œ ë°ì´í„°ì…‹
            df = pd.DataFrame([
                {
                    'peg_name': f'batch_test_peg_{j}',
                    'n_minus_1_value': 1000 + (i*10) + j,
                    'n_value': 1100 + (i*10) + j,
                    'change_pct': (j % 20) - 10
                }
                for j in range(3)  # ê° ë°ì´í„°ì…‹ë‹¹ 3ê°œ PEG
            ])
            test_datasets.append(df)
        
        from models import TimeRange
        test_range = TimeRange(
            start_time=datetime(2025, 1, 1, 9, 0, tzinfo=timezone.utc),
            end_time=datetime(2025, 1, 1, 18, 0, tzinfo=timezone.utc)
        )
        
        llm_service = LLMAnalysisService(
            llm_repository=mock_performance_llm_repository
        )
        
        performance_logger.info("=== ë°°ì¹˜ LLM ìš”ì²­ ìµœì í™” í…ŒìŠ¤íŠ¸ ===")
        
        # 1. ê°œë³„ ìš”ì²­ ì²˜ë¦¬
        start_time = time.time()
        individual_results = []
        
        for i, df in enumerate(test_datasets):
            result = llm_service.analyze_peg_data(
                processed_df=df,
                n1_range=test_range,
                n_range=test_range,
                analysis_type='enhanced'
            )
            individual_results.append(result)
        
        individual_time = time.time() - start_time
        
        # 2. ë°°ì¹˜ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜ (ë°ì´í„° í†µí•© í›„ í•œë²ˆì— ì²˜ë¦¬)
        start_time = time.time()
        
        # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
        combined_df = pd.concat(test_datasets, ignore_index=True)
        
        batch_result = llm_service.analyze_peg_data(
            processed_df=combined_df,
            n1_range=test_range,
            n_range=test_range,
            analysis_type='enhanced'
        )
        
        batch_time = time.time() - start_time
        
        # ì„±ëŠ¥ ë¹„êµ
        time_savings = individual_time - batch_time
        time_savings_pct = (time_savings / individual_time) * 100 if individual_time > 0 else 0
        
        performance_logger.info("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ íš¨ê³¼:")
        performance_logger.info(f"   ê°œë³„ ìš”ì²­ ì‹œê°„: {individual_time*1000:.2f}ms")
        performance_logger.info(f"   ë°°ì¹˜ ìš”ì²­ ì‹œê°„: {batch_time*1000:.2f}ms")
        performance_logger.info(f"   ì‹œê°„ ì ˆì•½: {time_savings*1000:.2f}ms ({time_savings_pct:.1f}%)")
        performance_logger.info(f"   ê°œë³„ ìš”ì²­ ìˆ˜: {len(test_datasets)}")
        
        # ë°°ì¹˜ ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦
        assert len(individual_results) == len(test_datasets), "ê°œë³„ ìš”ì²­ ê²°ê³¼ ìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤"
        assert isinstance(batch_result, LLMAnalysisResult), "ë°°ì¹˜ ìš”ì²­ ê²°ê³¼ íƒ€ì… ì˜¤ë¥˜"
        
        if time_savings > 0:
            performance_logger.info(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ê°€ {time_savings_pct:.1f}% ë” íš¨ìœ¨ì ")
        else:
            performance_logger.info("ğŸ“ ê°œë³„ ì²˜ë¦¬ê°€ ë” íš¨ìœ¨ì  (ì‘ì€ ë°°ì¹˜ í¬ê¸°)")
        
        performance_logger.info("âœ… ë°°ì¹˜ LLM ìš”ì²­ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
