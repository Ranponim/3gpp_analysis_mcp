"""
Data Processor for Transformation and Normalization

ì´ ëª¨ë“ˆì€ N-1ê³¼ N ê¸°ê°„ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³ , ë³€í™”ìœ¨ì„ ê³„ì‚°í•˜ë©°,
ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ëŠ” DataProcessor í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ê¸°ì¡´ AnalysisServiceì˜ ë°ì´í„° ë³€í™˜ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬
ë‹¨ì¼ ì±…ì„ ì›ì¹™ì„ ê°•í™”í•˜ê³  ì¬ì‚¬ìš©ì„±ì„ ë†’ì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import logging
import os

# ì„ì‹œë¡œ ì ˆëŒ€ import ì‚¬ìš© (ë‚˜ì¤‘ì— íŒ¨í‚¤ì§€ êµ¬ì¡° ì •ë¦¬ ì‹œ ìˆ˜ì •)
import sys
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import pandas as pd

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))


# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class AnalyzedPEGResult:
    """
    ë¶„ì„ëœ PEG ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° ëª¨ë¸

    ResponseFormatter(ì‘ì—… 19)ë¥¼ ìœ„í•œ ì¼ê´€ëœ ë°ì´í„° êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    í•„ë“œëª… ê·œì¹™:
    - n_minus_1_avg, n_avg: í‰ê· ê°’ (avgëŠ” í†µê³„ ìš©ì–´ë¡œ ëª…í™•í•¨)
    - dimensions: ì°¨ì› ì •ë³´ (ì˜ˆ: "cNum=52,mcID=0,EstabCause=MO_DATA,QCI=9")
    - í–¥í›„ í™•ì¥: n_minus_1_pct_95, n_minus_1_min, n_minus_1_max ë“±
    """

    peg_name: str
    n_minus_1_avg: Optional[float]  # ìˆ˜ì •: _value â†’ _avg (í‰ê· ê°’ì´ë¯€ë¡œ)
    n_avg: Optional[float]           # ìˆ˜ì •: _value â†’ _avg (í‰ê· ê°’ì´ë¯€ë¡œ)
    absolute_change: Optional[float]
    percentage_change: Optional[float]
    dimensions: Optional[str] = None  # ì¶”ê°€: ì°¨ì› ì •ë³´ (cNum, mcID, QCI ë“±)
    llm_analysis_summary: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜"""
        return {
            "peg_name": self.peg_name,
            "n_minus_1_avg": self.n_minus_1_avg,  # ìˆ˜ì •
            "n_avg": self.n_avg,                   # ìˆ˜ì •
            "absolute_change": self.absolute_change,
            "percentage_change": self.percentage_change,
            "dimensions": self.dimensions,  # ì¶”ê°€
            "llm_analysis_summary": self.llm_analysis_summary,
        }

    def has_complete_data(self) -> bool:
        """ì™„ì „í•œ ë°ì´í„° (N-1, N ëª¨ë‘ ì¡´ì¬)ì¸ì§€ í™•ì¸"""
        return self.n_minus_1_avg is not None and self.n_avg is not None  # ìˆ˜ì •

    def has_change_data(self) -> bool:
        """ë³€í™”ìœ¨ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        return self.absolute_change is not None and self.percentage_change is not None


class DataProcessingError(Exception):
    """
    ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ ì˜¤ë¥˜ ì˜ˆì™¸ í´ë˜ìŠ¤

    DataProcessorì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        message: str,
        details: Optional[Union[str, Dict[str, Any]]] = None,
        processing_step: Optional[str] = None,
        data_context: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        DataProcessingError ì´ˆê¸°í™”

        Args:
            message (str): ì˜¤ë¥˜ ë©”ì‹œì§€
            details (Optional[Union[str, Dict[str, Any]]]): ì¶”ê°€ ìƒì„¸ ì •ë³´
            processing_step (Optional[str]): ì‹¤íŒ¨í•œ ì²˜ë¦¬ ë‹¨ê³„
            data_context (Optional[Dict[str, Any]]): ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
        """
        super().__init__(message)
        self.message = message
        self.details = details
        self.processing_step = processing_step
        self.data_context = data_context

        logger.error("DataProcessingError ë°œìƒ: %s (ë‹¨ê³„: %s)", message, processing_step)

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜"""
        return {
            "error_type": "DataProcessingError",
            "message": self.message,
            "details": self.details,
            "processing_step": self.processing_step,
            "data_context": self.data_context,
        }


class DataProcessor:
    """
    ë°ì´í„° ë³€í™˜ ë° ì •ê·œí™” ì²˜ë¦¬ í´ë˜ìŠ¤

    N-1ê³¼ N ê¸°ê°„ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³ , ë³€í™”ìœ¨ì„ ê³„ì‚°í•˜ë©°,
    LLM ë¶„ì„ ê²°ê³¼ì™€ í†µí•©í•˜ì—¬ ì¼ê´€ëœ ë°ì´í„° êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    ê¸°ì¡´ AnalysisServiceì˜ _assemble_final_result() ë¡œì§ì„ ëª¨ë“ˆí™”í•œ ê²ƒì…ë‹ˆë‹¤.

    ì£¼ìš” ê¸°ëŠ¥:
    1. N-1ê³¼ N ê¸°ê°„ ë°ì´í„° ë³‘í•© ë° ì •ë ¬
    2. ë³€í™”ìœ¨ ê³„ì‚° (ì ˆëŒ€ê°’, ë°±ë¶„ìœ¨)
    3. ë°ì´í„° ì •ê·œí™” ë° í‘œì¤€í™”
    4. LLM ë¶„ì„ ê²°ê³¼ì™€ í†µí•©
    5. ResponseFormatterë¥¼ ìœ„í•œ ì¼ê´€ëœ ë°ì´í„° êµ¬ì¡° ì œê³µ
    """

    def __init__(self):
        """
        DataProcessor ì´ˆê¸°í™”
        """
        self.logger = logging.getLogger(__name__ + ".DataProcessor")

        # ì²˜ë¦¬ ë‹¨ê³„ ì •ì˜
        self.processing_steps = ["change_calculation", "llm_integration", "result_normalization"]

        self.logger.info("DataProcessor ì´ˆê¸°í™” ì™„ë£Œ")

    def get_processor_info(self) -> Dict[str, Any]:
        """í”„ë¡œì„¸ì„œ ì •ë³´ ë°˜í™˜"""
        return {
            "processor_name": "DataProcessor",
            "processing_steps": self.processing_steps,
            "supported_formats": ["pandas.DataFrame", "Dict[str, float]"],
            "output_model": "AnalyzedPEGResult",
        }

    def _integrate_llm_analysis(
        self, peg_results: List[AnalyzedPEGResult], llm_analysis_results: Optional[Dict[str, str]] = None
    ) -> List[AnalyzedPEGResult]:
        """
        LLM ë¶„ì„ ê²°ê³¼ë¥¼ PEG ê²°ê³¼ì— í†µí•©

        Args:
            peg_results (List[AnalyzedPEGResult]): PEG ë¶„ì„ ê²°ê³¼
            llm_analysis_results (Optional[Dict[str, str]]): LLM ë¶„ì„ ê²°ê³¼

        Returns:
            List[AnalyzedPEGResult]: LLM ë¶„ì„ì´ í†µí•©ëœ ê²°ê³¼
        """
        self.logger.debug("_integrate_llm_analysis() í˜¸ì¶œ: LLM ë¶„ì„ í†µí•©")

        if not llm_analysis_results:
            self.logger.info("LLM ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ í†µí•© ê±´ë„ˆëœ€")
            return peg_results

        try:
            # LLM ë¶„ì„ ê²°ê³¼ë¥¼ PEGë³„ë¡œ ë§¤í•‘
            for peg_result in peg_results:
                peg_name = peg_result.peg_name

                # PEG ì´ë¦„ìœ¼ë¡œ LLM ë¶„ì„ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                llm_summary = None
                for llm_peg_name, summary in llm_analysis_results.items():
                    if llm_peg_name.lower() == peg_name.lower():
                        llm_summary = summary
                        break

                # LLM ë¶„ì„ ê²°ê³¼ ì„¤ì •
                peg_result.llm_analysis_summary = llm_summary

                if llm_summary:
                    self.logger.debug("PEG '%s'ì— LLM ë¶„ì„ í†µí•©: %dì", peg_name, len(llm_summary))

            integrated_count = sum(1 for result in peg_results if result.llm_analysis_summary)
            self.logger.info("LLM ë¶„ì„ í†µí•© ì™„ë£Œ: %d/%dê°œ PEGì— ë¶„ì„ ê²°ê³¼", integrated_count, len(peg_results))

            return peg_results

        except Exception as e:
            raise DataProcessingError(
                f"LLM ë¶„ì„ í†µí•© ì‹¤íŒ¨: {e}",
                processing_step="llm_integration",
                data_context={"peg_count": len(peg_results), "llm_keys": len(llm_analysis_results)},
            ) from e

    def process_data(
        self, processed_df: pd.DataFrame, llm_analysis_results: Optional[Dict[str, Any]] = None
    ) -> List[AnalyzedPEGResult]:
        """
        ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        Args:
            processed_df (pd.DataFrame): PEGProcessingServiceì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°
            llm_analysis_results (Optional[Dict[str, Any]]): LLM ë¶„ì„ ê²°ê³¼

        Returns:
            List[AnalyzedPEGResult]: ì •ê·œí™”ëœ ë¶„ì„ ê²°ê³¼

        Raises:
            DataProcessingError: ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
        """
        self.logger.info("process_data() í˜¸ì¶œ: ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì‹œì‘")

        try:
            if processed_df.empty:
                self.logger.info("ì²˜ë¦¬ëœ DataFrameì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤ - ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []

            self.logger.info("1ë‹¨ê³„: ë³€í™”ìœ¨ ê³„ì‚° ë° êµ¬ì¡°í™”")

            # dimensions ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            has_dimensions = 'dimensions' in processed_df.columns
            
            # QCI í•„í„°ë§ (QCI 1, 5, 9ë§Œ ìœ ì§€)
            if has_dimensions:
                initial_count = len(processed_df)
                
                # QCIê°€ í¬í•¨ëœ dimensionsë¥¼ ê°€ì§„ í–‰ ì‹ë³„
                qci_mask = processed_df['dimensions'].notna() & processed_df['dimensions'].str.contains('QCI=', na=False)
                
                if qci_mask.sum() > 0:
                    self.logger.info("ğŸ” QCI í•„í„°ë§ ì‹œì‘: %dê°œ í–‰ì—ì„œ QCI ê²€ì¶œ", qci_mask.sum())
                    
                    # QCI 1, 5, 9ë§Œ ìœ ì§€í•˜ëŠ” ë§ˆìŠ¤í¬
                    allowed_qci_pattern = r'QCI=(1|5|9)(?:,|$)'
                    keep_mask = ~qci_mask | processed_df['dimensions'].str.contains(allowed_qci_pattern, regex=True, na=False)
                    
                    # í•„í„°ë§ ì „í›„ í†µê³„
                    filtered_out = (~keep_mask).sum()
                    if filtered_out > 0:
                        self.logger.info("ğŸ—‘ï¸ QCI í•„í„°ë§: %dê°œ í–‰ ì œê±° (QCI â‰  1,5,9)", filtered_out)
                        
                        # ì œê±°ëœ QCI ê°’ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                        removed_dims = processed_df[~keep_mask]['dimensions'].unique()[:5]
                        self.logger.debug("   ì œê±°ëœ dimensions ìƒ˜í”Œ: %s", removed_dims.tolist())
                    
                    # í•„í„°ë§ ì ìš©
                    processed_df = processed_df[keep_mask].reset_index(drop=True)
                    self.logger.info("âœ… QCI í•„í„°ë§ ì™„ë£Œ: %d â†’ %dê°œ í–‰", initial_count, len(processed_df))
                else:
                    self.logger.debug("QCI ì°¨ì›ì´ í¬í•¨ëœ ë°ì´í„° ì—†ìŒ - í•„í„°ë§ ìŠ¤í‚µ")
            
            # í•„í„°ë§ í›„ ë°ì´í„° ê²€ì¦
            if processed_df.empty:
                self.logger.warning("âš ï¸ QCI í•„í„°ë§ í›„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                return []

            # processed_dfì˜ change_pct ì»¬ëŸ¼ í™•ì¸ (ë””ë²„ê¹…)
            if "change_pct" in processed_df.columns:
                unique_change_values = processed_df["change_pct"].unique()
                non_zero_changes = processed_df[processed_df["change_pct"] != 0]["change_pct"].count()
                self.logger.debug(
                    "processed_df change_pct ë¶„ì„: ê³ ìœ ê°’_ê°œìˆ˜=%d, 0ì´_ì•„ë‹Œ_ê°’=%d, ìƒ˜í”Œ_ê°’=%s",
                    len(unique_change_values),
                    non_zero_changes,
                    unique_change_values[:10].tolist() if len(unique_change_values) > 0 else []
                )
            else:
                self.logger.warning("processed_dfì— change_pct ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")

            # change_map ìƒì„± í›„ íƒ€ì… ê²€ì¦ ë° ì •ì œ
            # dimensionsê°€ ìˆìœ¼ë©´ (peg_name, dimensions) íŠœí”Œë¡œ ê·¸ë£¹í™”
            if has_dimensions:
                self.logger.info("âœ… dimensions ì»¬ëŸ¼ ê°ì§€ - ì°¨ì›ë³„ PEG ìœ ì§€")
                change_map_raw = processed_df.groupby(["peg_name", "dimensions"])["change_pct"].first().to_dict()
            else:
                self.logger.warning("âš ï¸ dimensions ì»¬ëŸ¼ ì—†ìŒ - peg_nameë§Œìœ¼ë¡œ ê·¸ë£¹í™”")
                change_map_raw = processed_df.groupby("peg_name")["change_pct"].first().to_dict()
            
            # change_map íƒ€ì… ê²€ì¦: ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜
            change_map = {}
            invalid_count = 0
            for key, value in change_map_raw.items():
                # keyëŠ” has_dimensionsì— ë”°ë¼ str ë˜ëŠ” (str, str) íŠœí”Œ
                peg_display = f"{key[0]} (dims: {key[1]})" if has_dimensions else str(key)
                
                if value is None or pd.isna(value):
                    change_map[key] = None
                elif isinstance(value, (int, float)):
                    change_map[key] = value
                elif isinstance(value, str):
                    try:
                        change_map[key] = float(value)
                        self.logger.warning(
                            "PEG '%s'ì˜ change_pctê°€ ë¬¸ìì—´('%s')ì…ë‹ˆë‹¤. floatë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.",
                            peg_display, value
                        )
                    except (ValueError, TypeError):
                        self.logger.error(
                            "PEG '%s'ì˜ change_pct('%s')ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Noneìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
                            peg_display, value
                        )
                        change_map[key] = None
                        invalid_count += 1
                else:
                    self.logger.error(
                        "PEG '%s'ì˜ change_pctê°€ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…(%s)ì…ë‹ˆë‹¤. Noneìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
                        peg_display, type(value).__name__
                    )
                    change_map[key] = None
                    invalid_count += 1
            
            if invalid_count > 0:
                self.logger.warning(
                    "âš ï¸ change_map ìƒì„± ì¤‘ %dê°œì˜ ì˜ëª»ëœ íƒ€ì… ë°œê²¬ (Noneìœ¼ë¡œ ì²˜ë¦¬ë¨)",
                    invalid_count
                )
            
            # change_map í†µê³„ í™•ì¸ (ë””ë²„ê¹…) - íƒ€ì… ì•ˆì „í•˜ê²Œ ë¹„êµ
            if change_map:
                # ìˆ«ì íƒ€ì…ì´ê³  0ì´ ì•„ë‹Œ ê°’ë§Œ ì¹´ìš´íŠ¸
                non_zero_in_map = sum(
                    1 for v in change_map.values() 
                    if v is not None and isinstance(v, (int, float)) and v != 0
                )
                sample_items = list(change_map.items())[:5]
                self.logger.debug(
                    "change_map ìƒì„±: ì´=%dê°œ, 0ì´_ì•„ë‹Œ_ê°’=%dê°œ, ìƒ˜í”Œ=%s",
                    len(change_map),
                    non_zero_in_map,
                    sample_items
                )
                
                # í° í­ì˜ ìŒìˆ˜ ë³€í™”ìœ¨ ê²€ì¶œ - íƒ€ì… ì•ˆì „í•˜ê²Œ ë¹„êµ
                large_negative_changes = {
                    k: v for k, v in change_map.items() 
                    if v is not None and isinstance(v, (int, float)) and v < -20
                }
                if large_negative_changes:
                    self.logger.warning(
                        "âš ï¸ change_mapì—ì„œ í° í­ì˜ ê°ì†Œ ê°ì§€: %dê°œ PEG (ë³€í™”ìœ¨ < -20%%)",
                        len(large_negative_changes)
                    )
                    for key, change_pct in large_negative_changes.items():
                        # keyëŠ” has_dimensionsì— ë”°ë¼ str ë˜ëŠ” (str, str) íŠœí”Œ
                        peg_display = f"{key[0]} (dims: {key[1]})" if has_dimensions else str(key)
                        self.logger.warning(f"   {peg_display}: {change_pct:.2f}%")
            else:
                self.logger.warning("change_mapì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")

            # ì¤‘ë³µ ë°ì´í„° ê°ì§€ ë° ë¡œê¹… (pivot ì‹¤íŒ¨ ë°©ì§€)
            self.logger.debug("pivot ì‹¤í–‰ ì „ ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬ ì‹œì‘")
            subset_cols = ['peg_name', 'dimensions', 'period', 'avg_value'] if has_dimensions else ['peg_name', 'period', 'avg_value']
            duplicates = processed_df[processed_df.duplicated(subset=subset_cols, keep=False)]
            
            if not duplicates.empty:
                unique_peg_count = duplicates['peg_name'].nunique()
                self.logger.error("âŒ ì¤‘ë³µ ë°ì´í„° ë°œê²¬! (pivot ì‹¤íŒ¨ ìœ„í—˜)")
                self.logger.error("   ì¤‘ë³µ ê±´ìˆ˜: %dí–‰, %dê°œ PEG", len(duplicates), unique_peg_count)
                
                # ì¤‘ë³µëœ peg_nameë³„ë¡œ ìƒì„¸ ì¶œë ¥ (ìµœëŒ€ 5ê°œë§Œ)
                for idx, peg_name in enumerate(duplicates['peg_name'].unique()[:5]):
                    dup_rows = duplicates[duplicates['peg_name'] == peg_name]
                    self.logger.error(f"   [{idx+1}] PEG: {peg_name} (ì¤‘ë³µ {len(dup_rows)}ê±´)")
                    for _, row in dup_rows.iterrows():
                        period = row.get('period', 'N/A')
                        avg_value = row.get('avg_value', 'N/A')
                        dims = row.get('dimensions', 'N/A') if has_dimensions else 'N/A'
                        self.logger.error(f"       period={period}, avg_value={avg_value}, dimensions={dims}")
                
                if unique_peg_count > 5:
                    self.logger.error(f"   ... ì™¸ {unique_peg_count - 5}ê°œ PEG ë” ìˆìŒ")
            else:
                self.logger.debug("âœ“ ì¤‘ë³µ ë°ì´í„° ì—†ìŒ (pivot ì•ˆì „)")

            # pivot_table ì‚¬ìš© (dimensions í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°)
            index_cols = ["peg_name", "dimensions"] if has_dimensions else "peg_name"
            self.logger.info(f"pivot_table ì‹¤í–‰: index={index_cols}, columns=period, aggfunc=first")
            try:
                pivot_df = (
                    processed_df.pivot_table(
                        index=index_cols,
                        columns="period",
                        values="avg_value",
                        aggfunc='first',  # ì¤‘ë³µ ì‹œ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                        observed=True  # ì„±ëŠ¥ ìµœì í™”
                    )
                    .rename(columns={"N-1": "n_minus_1", "N": "n"})
                )
                self.logger.info("âœ… pivot_table ì™„ë£Œ: %dê°œ í–‰ (ì°¨ì› í¬í•¨ ì‹œ PEGÃ—dimensions ì¡°í•©)", len(pivot_df))
            except Exception as pivot_error:
                self.logger.error("pivot_table ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", pivot_error)
                self.logger.error("processed_df ì •ë³´: shape=%s, columns=%s", 
                                 processed_df.shape, processed_df.columns.tolist())
                raise

            pivot_df = pivot_df.where(pivot_df.notna(), None)

            results: List[AnalyzedPEGResult] = []

            for index_key, row in pivot_df.iterrows():
                # index_keyëŠ” dimensions í¬í•¨ ì‹œ (peg_name, dimensions) íŠœí”Œ, ì•„ë‹ˆë©´ peg_name ë¬¸ìì—´
                if has_dimensions:
                    peg_name, dimensions = index_key
                    peg_display = f"{peg_name} (dims: {dimensions})"
                    change_key = (peg_name, dimensions)
                else:
                    peg_name = index_key
                    dimensions = None
                    peg_display = peg_name
                    change_key = peg_name
                
                n_minus_1_avg = row.get("n_minus_1")  # ìˆ˜ì •: _value â†’ _avg
                n_avg = row.get("n")                   # ìˆ˜ì •: _value â†’ _avg

                absolute_change: Optional[float] = None
                percentage_change: Optional[float] = None

                if n_minus_1_avg is not None and n_avg is not None:
                    absolute_change = n_avg - n_minus_1_avg  # ìˆ˜ì •
                    
                    # change_mapì—ì„œ percentage_change ê°€ì ¸ì˜¤ê¸° (íƒ€ì… ê²€ì¦ í¬í•¨)
                    percentage_change_raw = change_map.get(change_key)
                    
                    # íƒ€ì… ê²€ì¦: None, NaN, ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° Noneìœ¼ë¡œ ì²˜ë¦¬
                    if percentage_change_raw is None or pd.isna(percentage_change_raw):
                        percentage_change = None
                    elif isinstance(percentage_change_raw, (int, float)):
                        # ìˆ«ì íƒ€ì…ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        percentage_change = percentage_change_raw
                    elif isinstance(percentage_change_raw, str):
                        # ë¬¸ìì—´ì´ë©´ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
                        try:
                            percentage_change = float(percentage_change_raw)
                            self.logger.warning(
                                "PEG '%s'ì˜ percentage_changeê°€ ë¬¸ìì—´('%s')ì…ë‹ˆë‹¤. floatë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.",
                                peg_display, percentage_change_raw
                            )
                        except (ValueError, TypeError):
                            self.logger.error(
                                "PEG '%s'ì˜ percentage_change('%s')ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Noneìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
                                peg_display, percentage_change_raw
                            )
                            percentage_change = None
                    else:
                        # ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…
                        self.logger.error(
                            "PEG '%s'ì˜ percentage_changeê°€ ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì…(%s)ì…ë‹ˆë‹¤. Noneìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
                            peg_display, type(percentage_change_raw).__name__
                        )
                        percentage_change = None
                else:
                    self.logger.warning(
                        "PEG '%s' ë°ì´í„° ë¶ˆì™„ì „ (N-1=%s, N=%s)", peg_display, n_minus_1_avg, n_avg  # ìˆ˜ì •
                    )

                results.append(
                    AnalyzedPEGResult(
                        peg_name=peg_name,
                        n_minus_1_avg=n_minus_1_avg,  # ìˆ˜ì •
                        n_avg=n_avg,                   # ìˆ˜ì •
                        absolute_change=absolute_change,
                        percentage_change=percentage_change,
                        dimensions=dimensions,  # ì¶”ê°€
                    )
                )

            # ì •ë ¬: peg_name ê¸°ì¤€, dimensionsê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë„ 2ì°¨ ì •ë ¬
            results.sort(key=lambda x: (x.peg_name, x.dimensions or ""))

            self.logger.info("2ë‹¨ê³„: LLM ë¶„ì„ í†µí•©")
            llm_peg_analysis: Dict[str, str] = {}
            if llm_analysis_results and isinstance(llm_analysis_results, dict):
                peg_insights = llm_analysis_results.get("peg_insights")
                if isinstance(peg_insights, dict):
                    for peg_name, summary in peg_insights.items():
                        if isinstance(summary, str) and summary.strip():
                            llm_peg_analysis[peg_name] = summary

            final_results = self._integrate_llm_analysis(results, llm_peg_analysis)

            self.logger.info("ë°ì´í„° ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: %dê°œ PEG ê²°ê³¼", len(final_results))
            return final_results

        except DataProcessingError:
            # ì´ë¯¸ DataProcessingErrorì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise

        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¥¼ DataProcessingErrorë¡œ ë³€í™˜
            raise DataProcessingError(
                f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}",
                processing_step="unknown",
                data_context={"df_shape": processed_df.shape if not processed_df.empty else None},
            ) from e

    def create_summary_statistics(self, results: List[AnalyzedPEGResult]) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ ìš”ì•½ í†µê³„ ìƒì„±

        Args:
            results (List[AnalyzedPEGResult]): ë¶„ì„ëœ PEG ê²°ê³¼

        Returns:
            Dict[str, Any]: ìš”ì•½ í†µê³„
        """
        self.logger.debug("create_summary_statistics() í˜¸ì¶œ: ìš”ì•½ í†µê³„ ìƒì„±")

        if not results:
            return {
                "total_pegs": 0,
                "complete_data_pegs": 0,
                "incomplete_data_pegs": 0,
                "positive_changes": 0,
                "negative_changes": 0,
                "no_change": 0,
                "avg_percentage_change": None,
            }

        try:
            complete_data_count = sum(1 for r in results if r.has_complete_data())
            incomplete_data_count = len(results) - complete_data_count

            # ë³€í™”ìœ¨ í†µê³„ (íƒ€ì… ê²€ì¦ í¬í•¨)
            # percentage_changeê°€ Noneì´ ì•„ë‹ˆê³ , ìˆ«ì íƒ€ì…(int ë˜ëŠ” float)ì¸ ê²½ìš°ë§Œ í¬í•¨
            valid_changes = [
                r.percentage_change 
                for r in results 
                if r.percentage_change is not None and isinstance(r.percentage_change, (int, float))
            ]
            
            # ë¬¸ìì—´ íƒ€ì…ì˜ percentage_changeê°€ ìˆëŠ”ì§€ í™•ì¸ (ë””ë²„ê¹…ìš©)
            invalid_changes = [
                (r.peg_name, r.percentage_change, type(r.percentage_change).__name__)
                for r in results 
                if r.percentage_change is not None and not isinstance(r.percentage_change, (int, float))
            ]
            if invalid_changes:
                self.logger.warning(
                    "âš ï¸ ìˆ«ìê°€ ì•„ë‹Œ percentage_change ë°œê²¬: %dê°œ (í†µê³„ì—ì„œ ì œì™¸ë¨)",
                    len(invalid_changes)
                )
                for peg_name, value, value_type in invalid_changes[:5]:  # ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
                    self.logger.warning(
                        "   PEG '%s': value='%s', type='%s'",
                        peg_name, value, value_type
                    )

            positive_changes = sum(1 for change in valid_changes if change > 0)
            negative_changes = sum(1 for change in valid_changes if change < 0)
            no_change = sum(1 for change in valid_changes if change == 0)

            avg_percentage_change = sum(valid_changes) / len(valid_changes) if valid_changes else None

            summary = {
                "total_pegs": len(results),
                "complete_data_pegs": complete_data_count,
                "incomplete_data_pegs": incomplete_data_count,
                "positive_changes": positive_changes,
                "negative_changes": negative_changes,
                "no_change": no_change,
                "avg_percentage_change": round(avg_percentage_change, 2) if avg_percentage_change is not None else None,
            }

            self.logger.info(
                "ìš”ì•½ í†µê³„ ìƒì„± ì™„ë£Œ: %dê°œ PEG, ì™„ì „ ë°ì´í„° %dê°œ", summary["total_pegs"], summary["complete_data_pegs"]
            )

            return summary

        except Exception as e:
            raise DataProcessingError(
                f"ìš”ì•½ í†µê³„ ìƒì„± ì‹¤íŒ¨: {e}",
                processing_step="result_normalization",
                data_context={"results_count": len(results)},
            ) from e

    def normalize_for_response_formatter(
        self, results: List[AnalyzedPEGResult], metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ResponseFormatterë¥¼ ìœ„í•œ ë°ì´í„° ì •ê·œí™”

        Args:
            results (List[AnalyzedPEGResult]): ë¶„ì„ëœ PEG ê²°ê³¼
            metadata (Optional[Dict[str, Any]]): ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            Dict[str, Any]: ì •ê·œí™”ëœ ì‘ë‹µ ë°ì´í„°
        """
        self.logger.debug("normalize_for_response_formatter() í˜¸ì¶œ: ì‘ë‹µ ì •ê·œí™”")

        try:
            # ìš”ì•½ í†µê³„ ìƒì„±
            summary_stats = self.create_summary_statistics(results)

            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            peg_results_dict = [result.to_dict() for result in results]

            # ì •ê·œí™”ëœ ì‘ë‹µ êµ¬ì¡°
            normalized_response = {
                "peg_results": peg_results_dict,
                "summary_statistics": summary_stats,
                "metadata": metadata or {},
                "processing_info": {
                    "processor_name": "DataProcessor",
                    "processing_timestamp": datetime.now().isoformat(),
                    "total_processed": len(results),
                },
            }

            self.logger.info("ì‘ë‹µ ì •ê·œí™” ì™„ë£Œ: %dê°œ PEG ê²°ê³¼", len(peg_results_dict))
            return normalized_response

        except Exception as e:
            raise DataProcessingError(
                f"ì‘ë‹µ ì •ê·œí™” ì‹¤íŒ¨: {e}",
                processing_step="result_normalization",
                data_context={"results_count": len(results)},
            ) from e

    def get_processing_status(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {"processing_steps": self.processing_steps, "step_count": len(self.processing_steps), "is_ready": True}
