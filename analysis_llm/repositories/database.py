"""
Database Repository Interface and PostgreSQL Implementation

ì´ ëª¨ë“ˆì€ ë°ì´í„°ë² ì´ìŠ¤ ì•¡ì„¸ìŠ¤ë¥¼ ìœ„í•œ Repository íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤ì™€ PostgreSQL êµ¬ì²´ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤.

ê¸°ì¡´ main.pyì˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì¿¼ë¦¬ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ ê²ƒì…ë‹ˆë‹¤.
"""

from __future__ import annotations

import logging
import os
import time

# ì„ì‹œë¡œ ì ˆëŒ€ import ì‚¬ìš© (ë‚˜ì¤‘ì— íŒ¨í‚¤ì§€ êµ¬ì¡° ì •ë¦¬ ì‹œ ìˆ˜ì •)
import sys
from abc import ABC, abstractmethod
from contextlib import contextmanager
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple

import psycopg2
import psycopg2.extras
import psycopg2.pool

# config ëª¨ë“ˆ ì§€ì—° import
_config_get_settings = None


def get_config_settings():
    """Configuration Managerì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì§€ì—° ë¡œë”©)"""
    global _config_get_settings
    if _config_get_settings is None:
        from config import get_settings

        _config_get_settings = get_settings
    return _config_get_settings()


from ..exceptions import DatabaseError

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class DatabaseRepository(ABC):
    """
    ë°ì´í„°ë² ì´ìŠ¤ Repository ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤

    ë°ì´í„°ë² ì´ìŠ¤ ì•¡ì„¸ìŠ¤ë¥¼ ìœ„í•œ ê³µí†µ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    êµ¬ì²´ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„ì²´ë“¤ì€ ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.

    ì£¼ìš” ê¸°ëŠ¥:
    1. ì—°ê²° ê´€ë¦¬ (connect, disconnect)
    2. ë°ì´í„° ì¡°íšŒ (fetch_data)
    3. ì¿¼ë¦¬ ì‹¤í–‰ (execute_query)
    4. ë™ì  ì¿¼ë¦¬ ìƒì„± ì§€ì›
    """

    @abstractmethod
    def connect(self) -> None:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •

        Raises:
            DatabaseError: ì—°ê²° ì‹¤íŒ¨ ì‹œ
        """

    @abstractmethod
    def disconnect(self) -> None:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ

        Raises:
            DatabaseError: ì—°ê²° í•´ì œ ì‹¤íŒ¨ ì‹œ
        """

    @abstractmethod
    def fetch_data(
        self,
        query: str,
        params: Optional[Dict[str, Any]] = None,
        table_name: Optional[str] = None,
        columns: Optional[List[str]] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        ë°ì´í„° ì¡°íšŒ (SELECT ì¿¼ë¦¬)

        Args:
            query (str): ì‹¤í–‰í•  SQL ì¿¼ë¦¬
            params (Optional[Dict[str, Any]]): ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜
            table_name (Optional[str]): ë™ì  í…Œì´ë¸”ëª… (ì¿¼ë¦¬ì— {table} í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš© ì‹œ)
            columns (Optional[List[str]]): ë™ì  ì»¬ëŸ¼ëª… (ì¿¼ë¦¬ì— {columns} í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš© ì‹œ)
            time_range (Optional[Tuple[datetime, datetime]]): ì‹œê°„ ë²”ìœ„ í•„í„°
            limit (Optional[int]): ê²°ê³¼ ê°œìˆ˜ ì œí•œ

        Returns:
            List[Dict[str, Any]]: ì¡°íšŒ ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)

        Raises:
            DatabaseError: ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ
        """

    @abstractmethod
    def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None, commit: bool = True) -> int:
        """
        ì¿¼ë¦¬ ì‹¤í–‰ (INSERT, UPDATE, DELETE)

        Args:
            query (str): ì‹¤í–‰í•  SQL ì¿¼ë¦¬
            params (Optional[Dict[str, Any]]): ì¿¼ë¦¬ ë§¤ê°œë³€ìˆ˜
            commit (bool): íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì—¬ë¶€

        Returns:
            int: ì˜í–¥ë°›ì€ í–‰ ìˆ˜

        Raises:
            DatabaseError: ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ
        """

    @abstractmethod
    def test_connection(self) -> bool:
        """
        ì—°ê²° í…ŒìŠ¤íŠ¸

        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """

    def build_dynamic_query(
        self,
        base_query: str,
        table_name: Optional[str] = None,
        columns: Optional[List[str]] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        additional_conditions: Optional[List[str]] = None,
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ë™ì  ì¿¼ë¦¬ ìƒì„± (ê³µí†µ ìœ í‹¸ë¦¬í‹°)

        Args:
            base_query (str): ê¸°ë³¸ ì¿¼ë¦¬ í…œí”Œë¦¿
            table_name (Optional[str]): í…Œì´ë¸”ëª…
            columns (Optional[List[str]]): ì»¬ëŸ¼ ëª©ë¡
            time_range (Optional[Tuple[datetime, datetime]]): ì‹œê°„ ë²”ìœ„
            additional_conditions (Optional[List[str]]): ì¶”ê°€ ì¡°ê±´ë“¤

        Returns:
            Tuple[str, Dict[str, Any]]: (ì™„ì„±ëœ ì¿¼ë¦¬, ë§¤ê°œë³€ìˆ˜)
        """
        params = {}

        # í…Œì´ë¸”ëª… ì¹˜í™˜
        if table_name and "{table}" in base_query:
            # SQL ì¸ì ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ ê²€ì¦
            if not table_name.replace("_", "").replace("-", "").isalnum():
                raise DatabaseError("ìœ íš¨í•˜ì§€ ì•Šì€ í…Œì´ë¸”ëª…", details={"table_name": table_name})
            base_query = base_query.replace("{table}", table_name)

        # ì»¬ëŸ¼ëª… ì¹˜í™˜
        if columns and "{columns}" in base_query:
            # SQL ì¸ì ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ ê²€ì¦
            for col in columns:
                if not col.replace("_", "").replace("-", "").isalnum():
                    raise DatabaseError("ìœ íš¨í•˜ì§€ ì•Šì€ ì»¬ëŸ¼ëª…", details={"column": col})
            columns_str = ", ".join(columns)
            base_query = base_query.replace("{columns}", columns_str)

        # ì‹œê°„ ë²”ìœ„ ì¡°ê±´ ì¶”ê°€
        conditions = []
        if time_range:
            start_time, end_time = time_range
            conditions.append("timestamp BETWEEN %(start_time)s AND %(end_time)s")
            params["start_time"] = start_time
            params["end_time"] = end_time

        # ì¶”ê°€ ì¡°ê±´ë“¤
        if additional_conditions:
            conditions.extend(additional_conditions)

        # WHERE ì ˆ ì¶”ê°€
        if conditions:
            if "WHERE" in base_query.upper():
                base_query += " AND " + " AND ".join(conditions)
            else:
                base_query += " WHERE " + " AND ".join(conditions)

        logger.debug("ë™ì  ì¿¼ë¦¬ ìƒì„±: %s (ë§¤ê°œë³€ìˆ˜: %dê°œ)", base_query, len(params))
        return base_query, params


class PostgreSQLRepository(DatabaseRepository):
    """
    PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ Repository êµ¬í˜„ì²´

    psycopg2ë¥¼ ì‚¬ìš©í•˜ì—¬ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì•¡ì„¸ìŠ¤í•©ë‹ˆë‹¤.
    ì—°ê²° í’€ë§, ì˜¤ë¥˜ ì²˜ë¦¬, ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

    ê¸°ì¡´ main.pyì˜ PostgreSQL ì—°ê²° ë¡œì§ì„ ëª¨ë“ˆí™”í•œ ê²ƒì…ë‹ˆë‹¤.
    """

    def __init__(self, config_override: Optional[Dict[str, Any]] = None):
        """
        PostgreSQLRepository ì´ˆê¸°í™”

        Args:
            config_override (Optional[Dict[str, Any]]): ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ (í…ŒìŠ¤íŠ¸ìš©)
        """
        # Configuration Managerì—ì„œ ì„¤ì • ë¡œë“œ
        try:
            settings = get_config_settings()
            self.config = {
                "host": settings.db_host,
                "port": settings.db_port,
                "database": settings.db_name,
                "user": settings.db_user,
                "password": settings.db_password.get_secret_value(),
                "pool_size": settings.db_pool_size,
            }
            logger.info("Configuration Managerì—ì„œ DB ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.warning("Configuration Manager ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: %s", e)
            self.config = {
                "host": os.getenv("DB_HOST", "165.213.69.30"),
                "port": int(os.getenv("DB_PORT", "5442")),
                "database": os.getenv("DB_NAME", "pvt_db"),
                "user": os.getenv("DB_USER", "testuser"),
                "password": os.getenv("DB_PASSWORD", "1234qwer"),
                "pool_size": int(os.getenv("DB_POOL_SIZE", "5")),
            }

        # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ ì ìš© (í…ŒìŠ¤íŠ¸ìš©)
        if config_override:
            self.config.update(config_override)
            logger.debug("DB ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ ì ìš©: %s", list(config_override.keys()))

        # ì—°ê²° í’€ ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        self._pool = None
        self._is_connected = False

        logger.info(
            "PostgreSQLRepository ì´ˆê¸°í™” ì™„ë£Œ: host=%s, database=%s", self.config["host"], self.config["database"]
        )

    def connect(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™”"""
        if self._is_connected:
            logger.debug("connect(): ì´ë¯¸ ì—°ê²°ëœ ìƒíƒœ - host=%s, db=%s, pool_size=%s",
                         self.config.get("host"), self.config.get("database"), self.config.get("pool_size"))
            return

        try:
            logger.info("connect(): ì—°ê²° í’€ ìƒì„± ì‹œì‘ | host=%s, port=%s, db=%s, pool_size=%s",
                        self.config.get("host"), self.config.get("port"), self.config.get("database"), self.config.get("pool_size"))
            t0 = time.perf_counter()
            # ì—°ê²° í’€ ìƒì„±
            self._pool = psycopg2.pool.SimpleConnectionPool(
                minconn=1,
                maxconn=self.config["pool_size"],
                host=self.config["host"],
                port=self.config["port"],
                database=self.config["database"],
                user=self.config["user"],
                password=self.config["password"],
            )

            self._is_connected = True
            elapsed = (time.perf_counter() - t0) * 1000
            logger.info(
                "connect(): ì—°ê²° í’€ ìƒì„± ì™„ë£Œ | host=%s, db=%s, pool_size=%s, %.1fms",
                self.config["host"], self.config["database"], self.config["pool_size"], elapsed
            )

        except psycopg2.Error as e:
            raise DatabaseError(
                "PostgreSQL ì—°ê²° ì‹¤íŒ¨",
                details={
                    "host": self.config["host"],
                    "database": self.config["database"],
                    "error": str(e),
                },
                connection_info=self.get_connection_info(),
            ) from e

    def disconnect(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ í•´ì œ"""
        if not self._is_connected or not self._pool:
            logger.debug("disconnect(): ì—°ê²°ë˜ì§€ ì•Šì€ ìƒíƒœ - ë¬´ì‹œ")
            return

        try:
            logger.info("disconnect(): ì—°ê²° í’€ í•´ì œ ì‹œì‘")
            self._pool.closeall()
            self._pool = None
            self._is_connected = False
            logger.info("disconnect(): ì—°ê²° í’€ í•´ì œ ì™„ë£Œ")

        except Exception as e:
            logger.error("ì—°ê²° í’€ í•´ì œ ì¤‘ ì˜¤ë¥˜: %s", e)
            raise DatabaseError("ì—°ê²° í’€ í•´ì œ ì‹¤íŒ¨") from e

    @contextmanager
    def get_connection(self):
        """
        ì—°ê²° í’€ì—ì„œ ì—°ê²° íšë“ (ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €)

        Yields:
            psycopg2.connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        """
        if not self._is_connected or not self._pool:
            raise DatabaseError("ì—°ê²° í’€ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. connect()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”")

        connection = None
        try:
            t0 = time.perf_counter()
            connection = self._pool.getconn()

            # --- [ìˆ˜ì •] í™˜ê²½ë³€ìˆ˜(APP_TIMEZONE)ë¥¼ ì½ì–´ ì„¸ì…˜ íƒ€ì„ì¡´ ì„¤ì • ---
            try:
                settings = get_config_settings()
                app_timezone = settings.app_timezone
                if app_timezone:
                    with connection.cursor() as cursor:
                        # SQL ì¸ì ì…˜ ë°©ì§€ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ì‚¬ìš©
                        cursor.execute("SET TIME ZONE %(timezone)s", {'timezone': app_timezone})
                    logger.debug("ì„¸ì…˜ íƒ€ì„ì¡´ ì„¤ì • ì™„ë£Œ: %s", app_timezone)
            except Exception as e:
                logger.warning("ì„¸ì…˜ íƒ€ì„ì¡´ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ (APP_TIMEZONE): %s", e)
            # --- [ìˆ˜ì •] ---

            elapsed = (time.perf_counter() - t0) * 1000
            logger.debug("get_connection(): ì—°ê²° íšë“ ì™„ë£Œ (%.1fms)", elapsed)
            yield connection

        except psycopg2.Error as e:
            if connection:
                connection.rollback()
            raise DatabaseError(
                "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜",
                details={"error": str(e)},
                connection_info=self.get_connection_info(),
            ) from e

        finally:
            if connection:
                self._pool.putconn(connection)
                logger.debug("get_connection(): ì—°ê²° ë°˜í™˜ ì™„ë£Œ")

    def test_connection(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if not self._is_connected:
                self.connect()

            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
                    result = cursor.fetchone()
                    success = result is not None

            logger.info("ì—°ê²° í…ŒìŠ¤íŠ¸ %s", "ì„±ê³µ" if success else "ì‹¤íŒ¨")
            return success

        except Exception as e:
            logger.error("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: %s", e)
            return False

    def fetch_data(
        self,
        query: str,
        params: Optional[Dict[str, Any]] = None,
        table_name: Optional[str] = None,
        columns: Optional[List[str]] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        ë°ì´í„° ì¡°íšŒ (SELECT ì¿¼ë¦¬)

        ê¸°ì¡´ main.pyì˜ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ ê²ƒì…ë‹ˆë‹¤.
        """
        logger.debug(
            "fetch_data(): í˜¸ì¶œ | query_len=%d, preview=%s, params_keys=%s, table=%s, time_range=%s, limit=%s",
            len(query or ""), (query or "")[:180].replace("\n", " "),
            list((params or {}).keys()), table_name, time_range, limit
        )

        if not self._is_connected:
            self.connect()

        # ë™ì  ì¿¼ë¦¬ ìƒì„±
        if table_name or columns or time_range:
            query, dynamic_params = self.build_dynamic_query(query, table_name, columns, time_range)
            # ë§¤ê°œë³€ìˆ˜ ë³‘í•©
            if params:
                dynamic_params.update(params)
            params = dynamic_params

        # LIMIT ì ˆ ì¶”ê°€
        if limit and limit > 0:
            query += f" LIMIT {limit}"

        try:
            with self.get_connection() as conn:
                with conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                    # ì¿¼ë¦¬ ì‹¤í–‰
                    t0 = time.perf_counter()
                    cursor.execute(query, params or {})

                    # ê²°ê³¼ ì¡°íšŒ
                    results = cursor.fetchall()

                    # RealDictRowë¥¼ ì¼ë°˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                    data = [dict(row) for row in results]
                    elapsed = (time.perf_counter() - t0) * 1000
                    first_keys = list(data[0].keys()) if data else []
                    logger.info(
                        "fetch_data(): ì¡°íšŒ ì™„ë£Œ | rows=%d, %.1fms, table=%s, window=%s, limit=%s, params_keys=%s, first_keys=%s",
                        len(data), elapsed, table_name, time_range, limit, list((params or {}).keys()), first_keys
                    )
                    return data

        except DatabaseError as e:
            # ì—°ê²° íšë“ ë‹¨ê³„ì—ì„œ ë°œìƒí•œ DatabaseErrorì— ì¿¼ë¦¬/íŒŒë¼ë¯¸í„°/ì—°ê²°ì •ë³´ë¥¼ ë³´ê°•
            error_msg = getattr(e, "message", "ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜")
            logger.error("fetch_data(): ì—°ê²° ì˜¤ë¥˜ | %s", error_msg)
            raise DatabaseError(
                error_msg,
                details={
                    "original": getattr(e, "details", None),
                    "query": (query or "")[:1000],
                    "params": params,
                },
                query=query,
                connection_info=self.get_connection_info(),
            ) from e
        except psycopg2.Error as e:
            error_msg = f"ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            raise DatabaseError(
                error_msg,
                details={
                    "query": query[:200],
                    "params": params,
                    "error_code": e.pgcode if hasattr(e, "pgcode") else None,
                },
                query=query,
                connection_info=self.get_connection_info(),
            ) from e

    def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None, commit: bool = True) -> int:
        """
        ì¿¼ë¦¬ ì‹¤í–‰ (INSERT, UPDATE, DELETE)

        ê¸°ì¡´ main.pyì˜ ì¿¼ë¦¬ ì‹¤í–‰ ë¡œì§ì„ ëª¨ë“ˆí™”í•œ ê²ƒì…ë‹ˆë‹¤.
        """
        logger.debug(
            "execute_query(): í˜¸ì¶œ | query_len=%d, preview=%s, commit=%s, params_keys=%s",
            len(query or ""), (query or "")[:180].replace("\n", " "), commit, list((params or {}).keys())
        )

        if not self._is_connected:
            self.connect()

        try:
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # ì¿¼ë¦¬ ì‹¤í–‰
                    t0 = time.perf_counter()
                    cursor.execute(query, params or {})

                    # ì˜í–¥ë°›ì€ í–‰ ìˆ˜
                    rowcount = cursor.rowcount

                    # íŠ¸ëœì­ì…˜ ì²˜ë¦¬
                    if commit:
                        conn.commit()
                        logger.debug("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì™„ë£Œ")

                    elapsed = (time.perf_counter() - t0) * 1000
                    logger.info("execute_query(): ì™„ë£Œ | affected=%d, %.1fms", rowcount, elapsed)
                    return rowcount

        except DatabaseError as e:
            # ì—°ê²° íšë“ ë‹¨ê³„ì—ì„œ ë°œìƒí•œ DatabaseErrorì— ì¿¼ë¦¬/íŒŒë¼ë¯¸í„°/ì—°ê²°ì •ë³´ë¥¼ ë³´ê°•
            error_msg = getattr(e, "message", "ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜")
            logger.error("execute_query(): ì—°ê²° ì˜¤ë¥˜ | %s", error_msg)
            raise DatabaseError(
                error_msg,
                details={
                    "original": getattr(e, "details", None),
                    "query": (query or "")[:1000],
                    "params": params,
                },
                query=query,
                connection_info=self.get_connection_info(),
            ) from e
        except psycopg2.Error as e:
            error_msg = f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            raise DatabaseError(
                error_msg,
                details={
                    "query": query[:200],
                    "params": params,
                    "error_code": e.pgcode if hasattr(e, "pgcode") else None,
                },
                query=query,
                connection_info=self.get_connection_info(),
            ) from e

    def fetch_peg_data(
        self,
        table_name: str,
        columns: Dict[str, str],
        time_range: Tuple[datetime, datetime],
        filters: Optional[Dict[str, Any]] = None,
        limit: Optional[int] = None,
        peg_filter: Optional[Dict[int, Set[str]]] = None,
    ) -> List[Dict[str, Any]]:
        """
        PEG ë°ì´í„° ì „ìš© ì¡°íšŒ ë©”ì„œë“œ (ê¸°ì¡´ main.py ë¡œì§ ê¸°ë°˜)

        Args:
            table_name (str): í…Œì´ë¸”ëª…
            columns (Dict[str, str]): ì»¬ëŸ¼ ë§¤í•‘ (time, peg_name, value, ne, cellid, host)
            time_range (Tuple[datetime, datetime]): ì‹œê°„ ë²”ìœ„
            filters (Optional[Dict[str, Any]]): ì¶”ê°€ í•„í„° ì¡°ê±´
            limit (Optional[int]): ê²°ê³¼ ê°œìˆ˜ ì œí•œ

        Returns:
            List[Dict[str, Any]]: PEG ë°ì´í„° ëª©ë¡
        """
        # ì…ë ¥ ë”•ì…”ë„ˆë¦¬ ë³´í˜¸: filtersë¥¼ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ ë³µì‚¬ë³¸ ìƒì„±
        # ë²„ê·¸ ìˆ˜ì •: del filters['ne']ë¡œ ì…ë ¥ ë”•ì…”ë„ˆë¦¬ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ê²ƒì„ ë°©ì§€
        if filters is not None:
            filters = filters.copy()  # Noneì´ ì•„ë‹Œ ê²½ìš° ë³µì‚¬ (ë¹ˆ ë”•ì…”ë„ˆë¦¬ í¬í•¨)
        
        logger.info("fetch_peg_data(): í˜¸ì¶œ | table=%s, time_range=%s, filters_keys=%s",
                    table_name, time_range, list((filters or {}).keys()))
        # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ë¡œê·¸
        try:
            logger.info(
                "fetch_peg_data(): ì»¨í…ìŠ¤íŠ¸ | table=%s | start=%s | end=%s | columns_keys=%s | filters=%s | limit=%s",
                table_name,
                time_range[0] if time_range else None,
                time_range[1] if time_range else None,
                list((columns or {}).keys()),
                (filters or {}),
                limit,
            )
        except Exception:
            logger.debug("fetch_peg_data(): ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… ìŠ¤í‚µ")
        # Columns ë§¤í•‘ ë””ë²„ê·¸ ë¡œê·¸ (í‚¤/ê°’ê³¼ JSONB ê°ì§€ ê²°ê³¼ ì¶œë ¥)
        try:
            logger.debug(
                "fetch_peg_data(): columns keys=%s, values=%s",
                list((columns or {}).keys()), list((columns or {}).values()),
            )
        except Exception:
            logger.debug("fetch_peg_data(): columns ë¡œê¹… ì‹¤íŒ¨ (ë¹„ì •í˜• ì…ë ¥)")

        # JSONB ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ì—¬ë¶€ íŒë³„ (values ì¡´ì¬ ì‹œ)
        json_mode = (
            ('values' in (columns or {}))
            or ('values' in list((columns or {}).values()))
            or ('family_id' in (columns or {}))
        )
        logger.debug("fetch_peg_data(): JSONB ê°ì§€ ê²°ê³¼ | json_mode=%s", json_mode)

        # WHERE ì¡°ê±´ êµ¬ì„± ê³µí†µ
        conditions: List[str] = []
        params: Dict[str, Any] = {}
        start_time, end_time = time_range

        if json_mode:
            # ì„¤ì •ì—ì„œ ì¬ê·€ ê¹Šì´ ì œí•œ ê°€ì ¸ì˜¤ê¸°
            try:
                settings = get_config_settings()
                max_recursion_depth = settings.jsonb_max_recursion_depth
                logger.debug("fetch_peg_data(): ì¬ê·€ ê¹Šì´ ì œí•œ=%d (from settings)", max_recursion_depth)
            except Exception as e:
                max_recursion_depth = 5  # ê¸°ë³¸ê°’
                logger.warning("fetch_peg_data(): ì„¤ì • ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì¬ê·€ ê¹Šì´=%d ì‚¬ìš© (%s)", max_recursion_depth, e)
            
            time_col = columns.get('time', 'datetime')
            values_col = columns.get('values', 'values')
            # DB ì»¬ëŸ¼: family_id (int), family_name (char)
            # CSVì—ì„œ ë¡œë“œëœ family_idëŠ” ì •ìˆ˜ë¡œ ìœ ì§€ë¨
            family_id_col = columns.get('family_id', 'family_id')
            family_name_col = columns.get('family_name', 'family_name')
            ne_col = columns.get('ne') or columns.get('ne_key') or 'ne_key'
            swname_col = columns.get('swname', 'swname')
            relver_col = columns.get('rel_ver', 'rel_ver')
            # ì°¨ì›(alias) ë§¤í•‘: JSONB index_name â†’ í•„í„° í‚¤
            dimension_alias_map = {
                'cellid': 'CellIdentity',
                'qci': 'QCI',
                'bpu_id': 'BPU_ID',
            }
            logger.debug(
                "fetch_peg_data(): JSONB ëª¨ë“œ | cols={time:%s,family_id:%s,family_name:%s,values:%s,ne:%s,swname:%s,rel_ver:%s} | dims=%s",
                time_col, family_id_col, family_name_col, values_col, ne_col, swname_col, relver_col, dimension_alias_map
            )

            # WHERE ì¡°ê±´ êµ¬ì„± (CTE Anchorìš©)
            cte_anchor_conditions = [f"t.{time_col} BETWEEN %(start_time)s AND %(end_time)s"]

            # --- [CSV í•„í„° ë¡œì§] ---
            # 1. family_id í•„í„°ë§ (CSVì˜ family_idëŠ” ì •ìˆ˜ë¡œ ìœ ì§€ë¨)
            if peg_filter:
                family_ids_to_filter = list(peg_filter.keys())
                if family_ids_to_filter:
                    # family_id_colì€ DBì˜ family_id ì»¬ëŸ¼ (int)
                    # peg_filterì˜ í‚¤ëŠ” CSVì—ì„œ ë¡œë“œí•œ family_id ì •ìˆ˜ (ì˜ˆ: 5002)
                    placeholders = ",".join([f"%(family_filter_{i})s" for i, _ in enumerate(family_ids_to_filter)])
                    cte_anchor_conditions.append(f"t.{family_id_col} IN ({placeholders})")
                    for i, v in enumerate(family_ids_to_filter):
                        params[f"family_filter_{i}"] = int(v)  # ëª…ì‹œì  ì •ìˆ˜ ë³€í™˜
                    logger.info("CSV í•„í„° ì ìš©: %dê°œ family_idë¡œ í•„í„°ë§ (ê°’: %s)", len(family_ids_to_filter), family_ids_to_filter[:5])
            # --- [ë¡œì§ ì™„ë£Œ] ---
            params['start_time'] = start_time
            params['end_time'] = end_time

            # ne_id í•„í„°ë¥¼ CTE anchorë¡œ ì´ë™
            if filters and 'ne' in filters and filters['ne']:
                ne_values = filters['ne']
                ne_col_name = columns.get('ne') or columns.get('ne_key') or 'ne_key'
                
                logger.info("ğŸ” ne í•„í„° ì ìš©: ì»¬ëŸ¼=%s, ê°’=%s", ne_col_name, ne_values)
                
                if isinstance(ne_values, (list, tuple, set)):
                    # ne_idê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš° IN ì‚¬ìš©
                    placeholders = ",".join([f"%(ne_filter_{i})s" for i, _ in enumerate(ne_values)])
                    cte_anchor_conditions.append(f"t.{ne_col_name} IN ({placeholders})")
                    for i, v in enumerate(ne_values):
                        # ne_key ì»¬ëŸ¼ì€ ì •ìˆ˜ì´ë¯€ë¡œ ë³€í™˜
                        try:
                            params[f"ne_filter_{i}"] = int(v)
                        except (ValueError, TypeError):
                            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê°’ ì‚¬ìš© (ë¡œê¹…)
                            logger.warning("ne í•„í„° ê°’ ë³€í™˜ ì‹¤íŒ¨: %s (ì›ë³¸ ì‚¬ìš©)", v)
                            params[f"ne_filter_{i}"] = v
                    logger.debug("ne í•„í„°: IN ì¡°ê±´ìœ¼ë¡œ %dê°œ ê°’ ì ìš©", len(ne_values))
                else:
                    # ne_idê°€ ë‹¨ì¼ ê°’ì¼ ê²½ìš°
                    cte_anchor_conditions.append(f"t.{ne_col_name} = %(ne_filter)s")
                    # ne_key ì»¬ëŸ¼ì€ ì •ìˆ˜ì´ë¯€ë¡œ ë³€í™˜
                    try:
                        params['ne_filter'] = int(ne_values)
                    except (ValueError, TypeError):
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê°’ ì‚¬ìš© (ë¡œê¹…)
                        logger.warning("ne í•„í„° ê°’ ë³€í™˜ ì‹¤íŒ¨: %s (ì›ë³¸ ì‚¬ìš©)", ne_values)
                        params['ne_filter'] = ne_values
                    logger.debug("ne í•„í„°: ë‹¨ì¼ ê°’ ì¡°ê±´ ì ìš©")
                
                # ì²˜ë¦¬ëœ í•„í„°ëŠ” ë‚˜ì¤‘ì— ì¤‘ë³µ ì ìš©ë˜ì§€ ì•Šë„ë¡ ì œê±°
                del filters['ne']
            else:
                logger.debug("ne í•„í„°: ì ìš©ë˜ì§€ ì•ŠìŒ (filters=%s)", filters.get('ne') if filters else None)

            # index_name í‚¤ëŠ” ë©”íƒ€ë°ì´í„°ì´ë¯€ë¡œ ëª¨ë“  ë ˆë²¨ì—ì„œ ì œì™¸
            cte_anchor_conditions.append("kv.key <> 'index_name'")
            cte_anchor_where_clause = " AND ".join(cte_anchor_conditions)

            # ì¬ê·€ì  JSONB í™•ì¥ (ì¤‘ì²©ëœ index_name êµ¬ì¡° ì™„ì „íˆ í¼ì¹˜ê¸°)
            # 
            # ğŸ”‘ í•µì‹¬: index_nameì€ í˜•ì œ ë…¸ë“œë¡œ ì¡´ì¬í•˜ë¯€ë¡œ ë¶€ëª¨ ê°ì²´ë„ í•¨ê»˜ ì „ë‹¬
            # ì˜ˆì‹œ êµ¬ì¡°: {"20": {...}, "36": {...}, "index_name": "CellIdentity"}
            recursive_cte = f"""
            WITH RECURSIVE flattened AS (
                -- ì´ˆê¸°: ìµœìƒìœ„ valuesì—ì„œ í‚¤-ê°’ ìŒ ì¶”ì¶œ
                SELECT 
                    t.{time_col} AS timestamp,
                    t.{family_id_col} AS family_id,
                    t.{family_name_col} AS family_name,
                    {"t." + ne_col + " AS ne," if ne_col else ""}
                    {"t." + swname_col + " AS swname," if swname_col else ""}
                    {"t." + relver_col + " AS rel_ver," if relver_col else ""}
                    kv.key AS path_key,
                    kv.value AS current_val,
                    t.{values_col} AS parent_obj,  -- ë¶€ëª¨ ê°ì²´ ë³´ì¡´ (í˜•ì œ index_name ì ‘ê·¼ìš©)
                    -- ğŸ”‘ Anchor: parent_obj(ì „ì²´ values)ì—ì„œ index_name ì¶”ì¶œ
                    CASE 
                        WHEN jsonb_extract_path_text(t.{values_col}, 'index_name') IS NOT NULL
                        THEN ARRAY[jsonb_extract_path_text(t.{values_col}, 'index_name')]
                        ELSE ARRAY[]::text[]
                    END AS dimension_names,
                    ARRAY[kv.key] AS dimension_values,
                    0 AS depth
                FROM {table_name} t
                CROSS JOIN LATERAL jsonb_each(t.{values_col}) AS kv(key, value)
                WHERE {cte_anchor_where_clause}
                
                UNION ALL
                
                -- ì¬ê·€: ê°ì²´ë©´ í•œ ë‹¨ê³„ ë” í¼ì¹˜ê¸° + index_name ëˆ„ì 
                SELECT 
                    f.timestamp,
                    f.family_id,
                    f.family_name,
                    {"f.ne," if ne_col else ""}
                    {"f.swname," if swname_col else ""}
                    {"f.rel_ver," if relver_col else ""}
                    kv.key AS path_key,
                    kv.value AS current_val,
                    f.current_val AS parent_obj,  -- í˜„ì¬ ë ˆë²¨ì„ ë‹¤ìŒ ë‹¨ê³„ì˜ ë¶€ëª¨ë¡œ ì „ë‹¬
                    -- ğŸ”‘ í˜„ì¬ ê°ì²´(current_val)ì—ì„œ í˜•ì œ index_name ì¶”ì¶œ
                    -- current_valì´ ê°ì²´ë©´ ê·¸ ì•ˆì—ì„œ index_nameì„ ì°¾ìŒ
                    CASE 
                        WHEN jsonb_typeof(f.current_val) = 'object' 
                             AND jsonb_extract_path_text(f.current_val, 'index_name') IS NOT NULL
                        THEN f.dimension_names || jsonb_extract_path_text(f.current_val, 'index_name')
                        ELSE f.dimension_names
                    END AS dimension_names,
                    f.dimension_values || kv.key AS dimension_values,
                    f.depth + 1 AS depth
                FROM flattened f
                CROSS JOIN LATERAL jsonb_each(f.current_val) AS kv(key, value)
                WHERE jsonb_typeof(f.current_val) = 'object'
                  AND kv.key <> 'index_name'  -- index_nameì€ ë©”íƒ€ë°ì´í„°ì´ë¯€ë¡œ ì œì™¸
                  AND f.depth < %(max_recursion_depth)s  -- ì„¤ì •ëœ ì¬ê·€ ê¹Šì´ ì œí•œ
            )
            """
            
            # ì¬ê·€ ê¹Šì´ íŒŒë¼ë¯¸í„° ì¶”ê°€
            params['max_recursion_depth'] = max_recursion_depth
            
            # ìµœì¢… SELECT: ë¦¬í”„ ë…¸ë“œ(ìŠ¤ì¹¼ë¼ ê°’)ë§Œ ì„ íƒ
            # dimension_namesì™€ dimension_valuesë¥¼ ì¡°í•©í•˜ì—¬ ì°¨ì› ì •ë³´ êµ¬ì„±
            select_parts: List[str] = [
                "timestamp",
                "family_id",
                "family_name",
            ]
            if ne_col:
                select_parts.append("ne")
            if swname_col:
                select_parts.append("swname")
            if relver_col:
                select_parts.append("rel_ver")
            
            # peg_name: path_key (ë¦¬í”„ ë…¸ë“œì˜ í‚¤, ì¦‰ ì‹¤ì œ PEG ë©”íŠ¸ë¦­ëª…)
            select_parts.append("path_key AS peg_name")
            
            # value: ìˆ«ìë©´ ìˆ«ìë¡œ, ë¬¸ìë©´ NULL
            # - JSONB number íƒ€ì… â†’ ìˆ«ìë¡œ ë³€í™˜
            # - JSONB string íƒ€ì…ì´ê³  ìˆ«ìë¡œ ì‹œì‘ â†’ ìˆ«ì ë³€í™˜ ì‹œë„
            # - ê·¸ ì™¸(null, -, NA, N/D ë“±) â†’ NULL (text_valueì— ë³´ì¡´)
            # 
            # ğŸ”‘ ì¤‘ìš”: current_val#>>'{}'ëŠ” JSONB ê°’ì„ ë”°ì˜´í‘œ ì—†ì´ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
            # ì˜ˆ: JSONB "266510.50" â†’ í…ìŠ¤íŠ¸ 266510.50 (ë”°ì˜´í‘œ ì œê±°!)
            select_parts.append(
                "CASE "
                "  WHEN jsonb_typeof(current_val) = 'number' THEN (current_val::text)::double precision "
                "  WHEN jsonb_typeof(current_val) = 'string' AND (current_val#>>'{}') ~ '^\\s*[+-]?\\d' "
                "    THEN (regexp_replace(current_val#>>'{}', '[^0-9\\.\\-eE]', '', 'g'))::double precision "
                "  ELSE NULL "
                "END AS value"
            )
            
            # text_value: ìˆ«ìë¡œ íŒŒì‹± ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ê°’ ë³´ì¡´ (NA, -, N/D, null ë“±)
            # - ìˆ«ìë¡œ íŒŒì‹± ì„±ê³µ ì‹œ â†’ NULL (ì¤‘ë³µ ë°©ì§€)
            # - ìˆ«ìê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ â†’ ì›ë³¸ ë³´ì¡´ (ë””ë²„ê¹…/ë¶„ì„ìš©)
            # 
            # ğŸ¯ ëª©ì : valueì™€ text_valueê°€ ë™ì‹œì— ê°’ì„ ê°–ì§€ ì•Šë„ë¡ í•¨
            # ì˜ˆ: value=266510.50, text_value=NULL âœ…
            #     value=NULL, text_value='NA' âœ…
            select_parts.append(
                "CASE "
                "  WHEN jsonb_typeof(current_val) = 'number' THEN NULL "  # ìˆ«ì íƒ€ì…ì´ë©´ text_valueëŠ” NULL
                "  WHEN jsonb_typeof(current_val) = 'string' THEN "
                "    CASE "
                "      WHEN (current_val#>>'{}') ~ '^\\s*[+-]?\\d' THEN NULL "  # ìˆ«ìë¡œ íŒŒì‹± ê°€ëŠ¥í•˜ë©´ NULL
                "      ELSE current_val#>>'{}' "  # ìˆ«ìê°€ ì•„ë‹ˆë©´ í…ìŠ¤íŠ¸ ë³´ì¡´ (NA, -, N/D ë“±)
                "    END "
                "  ELSE NULL "
                "END AS text_value"
            )
            
            # ì°¨ì› ì •ë³´: CTEì—ì„œ ì´ë¯¸ ê³„ì‚°ëœ dimension_namesì™€ dimension_valuesë¥¼ ì‚¬ìš©
            # WHERE ì ˆì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì™¸ë¶€ ì¿¼ë¦¬ì—ì„œ dimensions ê³„ì‚°
            select_parts.append("dimension_names")
            select_parts.append("dimension_values")
            
            # ê¸°ë³¸ ì¿¼ë¦¬: flattened CTEì—ì„œ ë¦¬í”„ ë…¸ë“œë§Œ ì„ íƒ
            inner_query = (
                f"{recursive_cte} "
                f"SELECT {', '.join(select_parts)} FROM flattened "
                f"WHERE jsonb_typeof(current_val) <> 'object'"  # ë¦¬í”„ ë…¸ë“œë§Œ (ìŠ¤ì¹¼ë¼ ê°’)
            )
            logger.debug("fetch_peg_data(): ì¬ê·€ CTE êµ¬ì„± ì™„ë£Œ | select_parts=%s", select_parts)



            # ì¶”ê°€ í•„í„° (ì¬ê·€ CTE í›„ ì ìš©)
            additional_conditions: List[str] = []
            
            # inner_dataì—ì„œ ì„ íƒ ê°€ëŠ¥í•œ ì»¬ëŸ¼ ëª©ë¡ ì •ì˜
            # ì´ ì»¬ëŸ¼ë“¤ì€ outer_select_partsì—ë„ í¬í•¨ë˜ì–´ì•¼ í•¨
            inner_data_columns = {'timestamp', 'family_id', 'family_name', 'peg_name', 'value', 'text_value', 'dimension_names', 'dimension_values'}
            if ne_col:
                inner_data_columns.add('ne')
            if swname_col:
                inner_data_columns.add('swname')
            if relver_col:
                inner_data_columns.add('rel_ver')

            # --- [CSV í•„í„° ë¡œì§] ---
            # 2. peg_name í•„í„°ë§ (family_idëŠ” ì´ë¯¸ CTE anchorì—ì„œ í•„í„°ë§ë¨)
            if peg_filter:
                peg_name_filter_clauses = []
                # ê° family_idì™€ peg_name ëª©ë¡ì— ëŒ€í•´ OR ì¡°ê±´ ìƒì„±
                for i, (family_id, peg_names) in enumerate(peg_filter.items()):
                    if not peg_names:
                        continue
                    
                    family_param_key = f"csv_family_{i}"
                    
                    # ê° PEG ì´ë¦„ì— ëŒ€í•´ LIKE ì¡°ê±´ ìƒì„±
                    # CSV: "AirMacDLThruAvg" â†’ DB: "AirMacDLThruAvg(Kbps)" ë§¤ì¹­
                    peg_like_conditions = []
                    for j, peg_name in enumerate(peg_names):
                        peg_param_key = f"csv_peg_{i}_{j}"
                        # peg_nameì´ CSV peg_nameìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ë§¤ì¹­ (LIKE 'AirMacDLThruAvg%')
                        peg_like_conditions.append(f"peg_name LIKE %({peg_param_key})s")
                        params[peg_param_key] = f"{peg_name}%"  # ì ‘ë‘ì–´ ë§¤ì¹­
                    
                    # (family_id = %s AND (peg_name LIKE %s OR peg_name LIKE %s ...))
                    # family_idëŠ” DBì˜ family_id ì»¬ëŸ¼ (int)
                    # family_idëŠ” CSVì—ì„œ ë¡œë“œí•œ ì •ìˆ˜ (ì˜ˆ: 5002)
                    peg_conditions_str = " OR ".join(peg_like_conditions)
                    clause = f"(family_id = %({family_param_key})s AND ({peg_conditions_str}))"
                    peg_name_filter_clauses.append(clause)
                    
                    # family_id íŒŒë¼ë¯¸í„° ì¶”ê°€ (ì •ìˆ˜ë¡œ ëª…ì‹œì  ë³€í™˜)
                    params[family_param_key] = int(family_id)
                
                if peg_name_filter_clauses:
                    additional_conditions.append(f"({' OR '.join(peg_name_filter_clauses)})")
                    logger.info("CSV í•„í„° ì ìš©: %dê°œ family_id/peg ì¡°í•©ìœ¼ë¡œ í•„í„°ë§ (LIKE íŒ¨í„´ ë§¤ì¹­)", len(peg_name_filter_clauses))
            # --- [ë¡œì§ ì™„ë£Œ] ---
            
            if filters:
                for key, value in filters.items():
                    if value is None:
                        continue
                    
                    # ì°¨ì› í•„í„° (cellid, qci, bpu_id) - dimensions ì»¬ëŸ¼ì—ì„œ ê²€ìƒ‰
                    if key in dimension_alias_map:
                        dimension_key = dimension_alias_map[key]
                        logger.info("ğŸ” ì°¨ì› í•„í„° ì ìš©: í•„í„°í‚¤=%s, ì°¨ì›í‚¤=%s, ê°’=%s", key, dimension_key, value)
                        
                        # dimensions ë¬¸ìì—´ì—ì„œ "CellIdentity=20" í˜•íƒœë¡œ ê²€ìƒ‰
                        if isinstance(value, (list, tuple, set)) and value:
                            # ë‹¤ì¤‘ ê°’: dimensionsì— í¬í•¨ë˜ëŠ”ì§€ OR ì¡°ê±´ìœ¼ë¡œ ê²€ì‚¬
                            or_conditions = []
                            for i, v in enumerate(value):
                                param_key = f"dim_{key}_{i}"
                                or_conditions.append(f"dimensions LIKE %({param_key})s")
                                params[param_key] = f"%{dimension_key}={v}%"
                            additional_conditions.append(f"({' OR '.join(or_conditions)})")
                            logger.debug("ì°¨ì› í•„í„°: LIKE ì¡°ê±´ìœ¼ë¡œ %dê°œ ê°’ ì ìš©", len(value))
                        else:
                            # ë‹¨ì¼ ê°’
                            param_key = f"dim_{key}"
                            additional_conditions.append(f"dimensions LIKE %({param_key})s")
                            params[param_key] = f"%{dimension_key}={value}%"
                            logger.debug("ì°¨ì› í•„í„°: ë‹¨ì¼ ê°’ LIKE ì¡°ê±´ ì ìš©")
                    else:
                        # í…Œì´ë¸” ì»¬ëŸ¼ ê¸°ë°˜ í•„í„° (ne, swname ë“±)
                        col_name = columns.get(key)
                        if not col_name:
                            continue
                        
                        # inner_dataì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ì¸ì§€ í™•ì¸
                        if key not in inner_data_columns:
                            logger.warning("í•„í„° í‚¤ '%s'ëŠ” inner_dataì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.", key)
                            continue
                        
                        if isinstance(value, (list, tuple, set)) and value:
                            placeholders = ",".join([f"%({key}_{i})s" for i, _ in enumerate(value)])
                            additional_conditions.append(f"{key} IN ({placeholders})")
                            for i, v in enumerate(value):
                                params[f"{key}_{i}"] = v
                        else:
                            additional_conditions.append(f"{key} = %({key})s")
                            params[key] = value

            # ì™¸ë¶€ ì¿¼ë¦¬ êµ¬ì„±: inner_queryë¥¼ ì„œë¸Œì¿¼ë¦¬ë¡œ ì‚¬ìš©í•˜ê³  dimensionsë¥¼ ê³„ì‚°
            # - WITH ì ˆë¡œ inner_queryë¥¼ ì„œë¸Œì¿¼ë¦¬ë¡œ ì‚¬ìš©
            # - dimensionsë¥¼ ê³„ì‚°í•˜ëŠ” ì¤‘ê°„ ë‹¨ê³„ ì¶”ê°€
            # - WHERE ì ˆì—ì„œ dimensionsë¥¼ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ í•¨
            outer_select_parts = [
                "timestamp",
                "family_id", 
                "family_name"
            ]
            if ne_col:
                outer_select_parts.append("ne")
            if swname_col:
                outer_select_parts.append("swname")
            if relver_col:
                outer_select_parts.append("rel_ver")
            outer_select_parts.append("peg_name")
            outer_select_parts.append("value")
            outer_select_parts.append("text_value")
            
            # dimensions ê³„ì‚°ì„ ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰
            outer_select_parts.append(
                "(SELECT string_agg(dimension_names[i] || '=' || dimension_values[i], ',') "
                "FROM generate_subscripts(dimension_names, 1) AS i) AS dimensions"
            )
            
            # ì¤‘ê°„ ë‹¨ê³„: dimensionsë¥¼ ê³„ì‚°í•˜ëŠ” CTE
            query = (
                f"WITH inner_data AS ({inner_query}), "
                f"     data_with_dimensions AS ("
                f"         SELECT {', '.join(outer_select_parts)} FROM inner_data"
                f"     ) "
                f"SELECT * FROM data_with_dimensions"
            )
            
            # ì™¸ë¶€ ì¿¼ë¦¬ì— WHERE ì¡°ê±´ ì¶”ê°€ (dimensions ì‚¬ìš© ê°€ëŠ¥)
            if additional_conditions:
                query += " WHERE " + " AND ".join(additional_conditions)
            
            query += " ORDER BY timestamp"
            if limit and limit > 0:
                query += f" LIMIT {limit}"

            logger.info(
                "fetch_peg_data(): ì¬ê·€ JSONB í™•ì¥ ì¿¼ë¦¬ êµ¬ì„± ì™„ë£Œ | sql_len=%d, params_keys=%s",
                len(query), list(params.keys())
            )
            logger.info("fetch_peg_data(): SQL ì¿¼ë¦¬=\n%s", query)
            logger.info("fetch_peg_data(): SQL íŒŒë¼ë¯¸í„°=%s", params)
            logger.debug("fetch_peg_data(): SQL preview=%s", query[:5000].replace('\n',' '))
            # ì£¼ì˜: ì´ë¯¸ WHERE/ORDER BY/LIMITê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ fetch_dataì— time_range/limit ì „ë‹¬í•˜ì§€ ì•ŠìŒ
            
            # ğŸ” ë””ë²„ê¹…: ì¡°íšŒëœ ë°ì´í„°ì˜ value ì»¬ëŸ¼ í†µê³„
            result_data = self.fetch_data(query, params)
            if result_data:
                logger.debug(
                    "fetch_peg_data() ê²°ê³¼: ì´=%dí–‰, ìƒ˜í”Œ ë°ì´í„°=%s",
                    len(result_data),
                    result_data[:3] if len(result_data) > 0 else []
                )
                
                # value ì»¬ëŸ¼ í†µê³„ (null, 0 ê°œìˆ˜)
                value_list = [row.get('value') for row in result_data]
                null_count = sum(1 for v in value_list if v is None)
                zero_count = sum(1 for v in value_list if v == 0 or v == 0.0)
                non_zero_count = sum(1 for v in value_list if v is not None and v != 0 and v != 0.0)
                
                logger.debug(
                    "fetch_peg_data() value ì»¬ëŸ¼ í†µê³„: null=%dê°œ, 0=%dê°œ, 0ì´_ì•„ë‹Œ_ê°’=%dê°œ, ìƒ˜í”Œ_value=%s",
                    null_count, zero_count, non_zero_count,
                    [v for v in value_list[:10] if v is not None]
                )
            else:
                logger.warning("fetch_peg_data() ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
            return result_data

        # ========================================================================
        # DEPRECATED: ë ˆê±°ì‹œ ëª¨ë“œ (ë¹„-JSONB ìŠ¤í‚¤ë§ˆ)
        # ========================================================================
        # TODO: ì´ ì½”ë“œëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í–¥í›„ ì œê±° ì˜ˆì •.
        # 
        # ì œê±° ëŒ€ìƒ:
        # - Line 864-912: ë ˆê±°ì‹œ ìŠ¤í‚¤ë§ˆ ì²˜ë¦¬ ë¡œì§ ì „ì²´
        # - ë‹¨ìˆœ SELECT ì¿¼ë¦¬ ë°©ì‹ (peg_name, value ì»¬ëŸ¼ ì§ì ‘ ì¡°íšŒ)
        # 
        # í˜„ì¬ëŠ” ëª¨ë“  í…Œì´ë¸”ì´ JSONB ìŠ¤í‚¤ë§ˆ (family_id, family_name, values)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ
        # ì´ ë ˆê±°ì‹œ ê²½ë¡œëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.
        # 
        # ì œê±° ì‹œì : ëª¨ë“  í…Œì´ë¸”ì´ JSONB ìŠ¤í‚¤ë§ˆë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ê²ƒì„ í™•ì¸í•œ í›„
        # ========================================================================
        
        # [DEPRECATED] ë¹„-JSONB ë ˆê±°ì‹œ ìŠ¤í‚¤ë§ˆ: ê¸°ì¡´ ê²½ë¡œ ìœ ì§€
        logger.warning(
            "fetch_peg_data(): âš ï¸ DEPRECATED ë ˆê±°ì‹œ ëª¨ë“œ ì‹¤í–‰ë¨! "
            "ì´ ê²½ë¡œëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤. "
            "columns=%s, table=%s", 
            columns, table_name
        )
        
        select_columns = [
            f"{columns['time']} as timestamp",
            f"{columns['peg_name']} as peg_name",
            f"{columns['value']} as value",
        ]

        # ì„ íƒì  ì»¬ëŸ¼ë“¤ ì¶”ê°€
        optional_columns = ["ne", "cellid", "swname"]
        for col_key in optional_columns:
            if col_key in columns and columns[col_key]:
                select_columns.append(f"{columns[col_key]} as {col_key}")

        # ì¿¼ë¦¬ êµ¬ì„±
        query = f"SELECT {', '.join(select_columns)} FROM {table_name}"

        # ì‹œê°„ ë²”ìœ„ ì¡°ê±´
        conditions.append(f"{columns['time']} BETWEEN %(start_time)s AND %(end_time)s")
        params["start_time"] = start_time
        params["end_time"] = end_time

        # ì¶”ê°€ í•„í„° ì¡°ê±´ (ì»¬ëŸ¼ ë§¤í•‘ëœ í‚¤ë§Œ)
        if filters:
            for key, value in filters.items():
                if key in columns and value is not None:
                    col_name = columns[key]
                    if isinstance(value, (list, tuple, set)) and value:
                        placeholders = ",".join([f"%({key}_{i})s" for i, _ in enumerate(value)])
                        conditions.append(f"{col_name} IN ({placeholders})")
                        for i, v in enumerate(value):
                            params[f"{key}_{i}"] = v
                    else:
                        conditions.append(f"{col_name} = %({key})s")
                        params[key] = value

        # WHERE ì ˆ ì¶”ê°€
        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        # ì •ë ¬ (ì‹œê°„ìˆœ)
        query += f" ORDER BY {columns['time']}"

        # LIMIT ì¶”ê°€
        if limit and limit > 0:
            query += f" LIMIT {limit}"

        logger.debug("fetch_peg_data(): [DEPRECATED ë ˆê±°ì‹œ] SQL preview=%s", query[:5000].replace('\n',' '))
        # ì£¼ì˜: ì´ë¯¸ WHERE/ORDER BY/LIMITê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ fetch_dataì— time_range/limit ì „ë‹¬í•˜ì§€ ì•ŠìŒ
        return self.fetch_data(query, params)
        
        # ========================================================================
        # END DEPRECATED
        # ========================================================================

    def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """
        í…Œì´ë¸” ì •ë³´ ì¡°íšŒ

        Args:
            table_name (str): í…Œì´ë¸”ëª…

        Returns:
            Dict[str, Any]: í…Œì´ë¸” ì •ë³´ (ì»¬ëŸ¼, ì¸ë±ìŠ¤ ë“±)
        """
        logger.debug("get_table_info() í˜¸ì¶œ: table=%s", table_name)

        # ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
        column_query = """
        SELECT column_name, data_type, is_nullable, column_default
        FROM information_schema.columns 
        WHERE table_name = %(table_name)s
        ORDER BY ordinal_position
        """

        columns = self.fetch_data(column_query, {"table_name": table_name})

        # ì¸ë±ìŠ¤ ì •ë³´ ì¡°íšŒ
        index_query = """
        SELECT indexname, indexdef
        FROM pg_indexes 
        WHERE tablename = %(table_name)s
        """

        indexes = self.fetch_data(index_query, {"table_name": table_name})

        return {
            "table_name": table_name,
            "columns": columns,
            "indexes": indexes,
            "column_count": len(columns),
            "index_count": len(indexes),
        }

    def get_connection_info(self) -> Dict[str, Any]:
        """ì—°ê²° ì •ë³´ ë°˜í™˜ (ë¯¼ê° ì •ë³´ ì œì™¸)"""
        return {
            "host": self.config["host"],
            "port": self.config["port"],
            "database": self.config["database"],
            "user": self.config["user"],
            "pool_size": self.config["pool_size"],
            "is_connected": self._is_connected,
            "pool_status": "active" if self._pool else "inactive",
        }

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.connect()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.disconnect()

        # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê·¸ ê¸°ë¡
        if exc_type:
            logger.error("ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ì—ì„œ ì˜ˆì™¸ ë°œìƒ: %s", exc_val)

        return False  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
