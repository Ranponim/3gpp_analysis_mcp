"""
=====================================================================================
Cell ì„±ëŠ¥ LLM ë¶„ì„ê¸° (ì‹œê°„ë²”ìœ„ ì…ë ¥ + PostgreSQL ì§‘ê³„ + í†µí•© ë¶„ì„ + HTML/ë°±ì—”ë“œ POST)
=====================================================================================

## ğŸ“‹ ì‹œìŠ¤í…œ ê°œìš”
3GPP ì´ë™í†µì‹ ë§ì˜ Cell ì„±ëŠ¥ ë°ì´í„°ë¥¼ LLMì„ í™œìš©í•˜ì—¬ ì¢…í•© ë¶„ì„í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
PostgreSQLì—ì„œ PEG(Performance Event Group) ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ê³ , LLMì„ í†µí•´ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ”„ ì£¼ìš” ì²˜ë¦¬ íë¦„
1. **ì‹œê°„ ë²”ìœ„ íŒŒì‹±**: ì‚¬ìš©ì ì…ë ¥ ì‹œê°„ ë²”ìœ„ë¥¼ íŒŒì‹±í•˜ì—¬ ì‹œì‘/ì¢…ë£Œ ì‹œì  ì¶”ì¶œ
2. **ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ**: PostgreSQLì—ì„œ ì§€ì •ëœ ê¸°ê°„ì˜ PEG ë°ì´í„° ì§‘ê³„
3. **íŒŒìƒ PEG ê³„ì‚°**: ì‚¬ìš©ì ì •ì˜ ìˆ˜ì‹ì„ ì´ìš©í•œ íŒŒìƒ ì§€í‘œ ê³„ì‚°
4. **ë°ì´í„° ì²˜ë¦¬**: N-1ê³¼ N ê¸°ê°„ ë°ì´í„° ë³‘í•© ë° ë³€í™”ìœ¨ ê³„ì‚°
5. **LLM ë¶„ì„**: ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë¶„ì„ ë° ê¶Œê³ ì‚¬í•­ ìƒì„±
6. **ê²°ê³¼ ì¶œë ¥**: HTML ë¦¬í¬íŠ¸ ìƒì„± ë° ë°±ì—”ë“œ APIë¡œ ê²°ê³¼ ì „ì†¡

## ğŸ¯ í•µì‹¬ ê¸°ëŠ¥
- **í†µí•© ë¶„ì„**: PEG ë‹¨ìœ„ê°€ ì•„ë‹Œ ì…€ ë‹¨ìœ„ ì „ì²´ ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© ì„±ëŠ¥ í‰ê°€
- **íŠ¹ì • PEG ë¶„ì„**: preferenceë‚˜ selected_pegsë¡œ ì§€ì •ëœ PEGë§Œ ë³„ë„ ë¶„ì„
- **íŒŒìƒ PEG ì§€ì›**: peg_definitionsë¡œ (pegA/pegB)*100 ê°™ì€ ìˆ˜ì‹ ì •ì˜
- **í™˜ê²½ë³€ìˆ˜ ì§€ì›**: ëª¨ë“  ì„¤ì •ê°’ì„ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬ ê°€ëŠ¥
- **í˜ì¼ì˜¤ë²„ ì§€ì›**: LLM API ë‹¤ì¤‘ ì—”ë“œí¬ì¸íŠ¸ ì§€ì›

## ğŸ“ ì‚¬ìš© ì˜ˆì‹œ (MCP tool í˜¸ì¶œ request ì˜ˆ):
{
  "n_minus_1": "2025-07-01_00:00~2025-07-01_23:59",
  "n": "2025-07-02_00:00~2025-07-02_23:59",
  "output_dir": "./analysis_output",
  "backend_url": "http://localhost:8000/api/analysis-result",
  "db": {"host": "127.0.0.1", "port": 5432, "user": "postgres", "password": "pass", "dbname": "netperf"},
  "table": "summary",
  "columns": {"time": "datetime", "peg_name": "peg_name", "value": "value"},
  "preference": "Random_access_preamble_count,Random_access_response",
  "peg_definitions": {
    "telus_RACH_Success": "Random_access_preamble_count/Random_access_response*100"
  }
}

## ğŸ”§ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ìì„¸í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •ì€ ENV_SETTINGS.md íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.
"""

from __future__ import annotations

import base64  # Base64 ì¸ì½”ë”© (ì´ë¯¸ì§€ ë°ì´í„° ì „ì†¡ìš©)
import datetime  # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬ ë° íƒ€ì„ì¡´ ê´€ë¦¬
import io  # ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (ì´ë¯¸ì§€ ë°ì´í„° ë“±)
import json  # JSON ë°ì´í„° ì§ë ¬í™”/ì—­ì§ë ¬í™”
import logging  # ë¡œê¹… ì‹œìŠ¤í…œ
import math  # ìˆ˜í•™ ì—°ì‚° (í† í° ì¶”ì •, NaN ì²˜ë¦¬ ë“±)

# ===========================================
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ imports
# ===========================================
import os  # í™˜ê²½ë³€ìˆ˜ ë° íŒŒì¼ ì‹œìŠ¤í…œ ì ‘ê·¼

# ===========================================
# Configuration Manager í†µí•©
# ===========================================
import sys
import time  # ì„±ëŠ¥ ì¸¡ì • ë° ì‹œê°„ ê´€ë ¨ ê¸°ëŠ¥

from .repositories import LLMClient, PostgreSQLRepository
from .services import AnalysisService, AnalysisServiceError

# ===========================================
# ë¡œì»¬ ëª¨ë“ˆ imports
# ===========================================
from .utils import TimeParsingError, TimeRangeParser

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ë¡œë”©)
_app_settings = None

def get_app_settings():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _app_settings
    if _app_settings is None:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ config ëª¨ë“ˆ import
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        config_path = os.path.join(project_root, 'config')
        if config_path not in sys.path:
            sys.path.insert(0, project_root)
        
        from config import get_settings
        _app_settings = get_settings()
    return _app_settings

# ===========================================
# ê¸€ë¡œë²Œ ì•ˆì „ ìƒìˆ˜ (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)
# ===========================================
# LLM í”„ë¡¬í”„íŠ¸ ê´€ë ¨ ìƒí•œê°’ë“¤ - ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ë³´í˜¸ë¥¼ ìœ„í•œ ì„¤ì •
def get_prompt_limits():
    """í”„ë¡¬í”„íŠ¸ ì œí•œê°’ë“¤ì„ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
    return {
        'max_prompt_tokens': int(os.getenv('DEFAULT_MAX_PROMPT_TOKENS', '24000')),
        'max_prompt_chars': int(os.getenv('DEFAULT_MAX_PROMPT_CHARS', '80000')),
        'max_specific_rows': int(os.getenv('DEFAULT_SPECIFIC_MAX_ROWS', '500')),
        'max_raw_str': int(os.getenv('DEFAULT_MAX_RAW_STR', '4000')),
        'max_raw_array': int(os.getenv('DEFAULT_MAX_RAW_ARRAY', '100'))
    }

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ë“¤ (ì¶”í›„ ì œê±° ì˜ˆì •)
DEFAULT_MAX_PROMPT_TOKENS = int(os.getenv('DEFAULT_MAX_PROMPT_TOKENS', '24000'))
DEFAULT_MAX_PROMPT_CHARS = int(os.getenv('DEFAULT_MAX_PROMPT_CHARS', '80000'))
DEFAULT_SPECIFIC_MAX_ROWS = int(os.getenv('DEFAULT_SPECIFIC_MAX_ROWS', '500'))
DEFAULT_MAX_RAW_STR = int(os.getenv('DEFAULT_MAX_RAW_STR', '4000'))
DEFAULT_MAX_RAW_ARRAY = int(os.getenv('DEFAULT_MAX_RAW_ARRAY', '100'))

# ===========================================
# í† í°/í”„ë¡¬í”„íŠ¸ ê°€ë“œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===========================================

def estimate_prompt_tokens(text: str) -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜
    
    ì•„ì£¼ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í† í° ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤:
    - ì˜ì–´/í•œê¸€ í˜¼í•© í™˜ê²½ì—ì„œ ì•ˆì „ ì¸¡ì„ ìœ„í•´ í‰ê·  3.5 chars/token ê°€ì •
    - ì‹¤ì œ ëª¨ë¸ë³„ í† í¬ë‚˜ì´ì €ì™€ ì°¨ì´ê°€ ìˆìœ¼ë¯€ë¡œ ìƒí•œ ì²´í¬ìš© ë³´ìˆ˜ ì¶”ì •ì¹˜
    
    Args:
        text (str): í† í° ìˆ˜ë¥¼ ì¶”ì •í•  í…ìŠ¤íŠ¸
        
    Returns:
        int: ì¶”ì •ëœ í† í° ìˆ˜
    """
    logging.debug("estimate_prompt_tokens() í˜¸ì¶œ: í…ìŠ¤íŠ¸ ê¸¸ì´=%d", len(text or ""))
    
    if not text:
        logging.debug("ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥: í† í° ìˆ˜=0")
        return 0
    
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ì¶”ì • ë¹„ìœ¨ ì½ê¸° (ê¸°ë³¸ê°’: 3.5ìë‹¹ 1í† í°)
        chars_per_token = float(os.getenv('CHARS_PER_TOKEN_RATIO', '3.5'))
        estimated_tokens = int(math.ceil(len(text) / chars_per_token))
        logging.debug("í† í° ì¶”ì • ì™„ë£Œ: %dì â†’ %dí† í° (ë¹„ìœ¨: %.1f)", len(text), estimated_tokens, chars_per_token)
        return estimated_tokens
    except Exception as e:
        logging.warning("í† í° ì¶”ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ëŒ€ì²´ ë°©ë²• ì‚¬ìš©: %s", e)
        # ëŒ€ì²´ ë°©ë²•: í™˜ê²½ë³€ìˆ˜ì—ì„œ ëŒ€ì²´ ë¹„ìœ¨ ì½ê¸° (ê¸°ë³¸ê°’: 4ìë‹¹ 1í† í°)
        fallback_chars_per_token = float(os.getenv('FALLBACK_CHARS_PER_TOKEN_RATIO', '4.0'))
        fallback_tokens = len(text) // int(fallback_chars_per_token)
        logging.debug("ëŒ€ì²´ í† í° ì¶”ì •: %dì â†’ %dí† í°", len(text), fallback_tokens)
        return fallback_tokens


def clamp_prompt(text: str, max_chars: int) -> tuple[str, bool]:
    """
    í”„ë¡¬í”„íŠ¸ë¥¼ ì§€ì •ëœ ë¬¸ì ìˆ˜ë¡œ ìë¥´ëŠ” ì•ˆì „ ê°€ë“œ í•¨ìˆ˜
    
    LLM APIì˜ í† í°/ë¬¸ì ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìë¦…ë‹ˆë‹¤.
    ìë¥¸ ë¶€ë¶„ì—ëŠ” ëª…í™•í•œ í‘œì‹œë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìê°€ ì¸ì§€í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ìë¥¼ í…ìŠ¤íŠ¸
        max_chars (int): ìµœëŒ€ í—ˆìš© ë¬¸ì ìˆ˜
        
    Returns:
        tuple[str, bool]: (ìë¥¸_í…ìŠ¤íŠ¸, ìë¦„_ì—¬ë¶€)
    """
    logging.debug("clamp_prompt() í˜¸ì¶œ: ì›ë³¸_ê¸¸ì´=%d, ìµœëŒ€_ê¸¸ì´=%d", len(text or ""), max_chars)
    
    if text is None:
        logging.debug("None í…ìŠ¤íŠ¸ ì…ë ¥: ë¹ˆ ë¬¸ìì—´ ë°˜í™˜")
        return "", False
        
    if len(text) <= max_chars:
        logging.debug("í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ìƒ: ìë¥´ê¸° ë¶ˆí•„ìš”")
        return text, False
    
    # ìë¥´ê¸° ìˆ˜í–‰: í™˜ê²½ë³€ìˆ˜ì—ì„œ ì—¬ìœ  ê³µê°„ ì„¤ì • (ê¸°ë³¸ê°’: 200ì)
    buffer_chars = int(os.getenv('PROMPT_TRUNCATE_BUFFER', '200'))
    head = text[: max_chars - buffer_chars]
    tail = "\n\n[...truncated due to safety guard...]\n"
    clamped_text = head + tail
    
    logging.warning("í”„ë¡¬í”„íŠ¸ ìë¦„: %dì â†’ %dì (ì•ˆì „ ê°€ë“œ ì ìš©)", len(text), len(clamped_text))
    return clamped_text, True


def build_results_overview(analysis: dict | str | None) -> dict:
    """
    LLM ë¶„ì„ ê²°ê³¼ì—ì„œ í•µì‹¬ ìš”ì•½ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    LLMì´ ë°˜í™˜í•œ ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¶„ì„ ê²°ê³¼ì—ì„œ ì¼ê´€ëœ í˜•íƒœì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    dict í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ê²°ê³¼ì™€ str í˜•íƒœì˜ í…ìŠ¤íŠ¸ ê²°ê³¼ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    Args:
        analysis (dict | str | None): LLM ë¶„ì„ ê²°ê³¼
        
    Returns:
        dict: í‘œì¤€í™”ëœ ìš”ì•½ ì •ë³´ {
            "summary": str | None,
            "key_findings": list,
            "recommended_actions": list
        }
    """
    logging.debug("build_results_overview() í˜¸ì¶œ: ì…ë ¥ íƒ€ì…=%s", type(analysis).__name__)
    
    # ê¸°ë³¸ êµ¬ì¡° ì´ˆê¸°í™”
    overview: dict = {
        "summary": None,
        "key_findings": [],
        "recommended_actions": []
    }
    
    try:
        if isinstance(analysis, dict):
            logging.debug("ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘")
            
            # ìš”ì•½ ì •ë³´ ì¶”ì¶œ (ì—¬ëŸ¬ í‚¤ ì‹œë„)
            summary = analysis.get("executive_summary") or analysis.get("summary") or None
            logging.debug("ìš”ì•½ ì •ë³´ ì¶”ì¶œ: %s", "ìˆìŒ" if summary else "ì—†ìŒ")
            
            # ê¶Œê³ ì‚¬í•­ ì¶”ì¶œ
            recs = analysis.get("recommended_actions") or analysis.get("actions") or []
            logging.debug("ê¶Œê³ ì‚¬í•­ ì¶”ì¶œ: %dê°œ", len(recs) if isinstance(recs, list) else 0)
            
            # í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
            findings = analysis.get("issues") or analysis.get("alerts") or analysis.get("key_findings") or []
            logging.debug("í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ: %dê°œ", len(findings) if isinstance(findings, list) else 0)
            
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(recs, dict):
                recs = list(recs.values())
                logging.debug("ê¶Œê³ ì‚¬í•­ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜: %dê°œ", len(recs))
                
            if isinstance(findings, dict):
                findings = list(findings.values())
                logging.debug("ë°œê²¬ì‚¬í•­ ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜: %dê°œ", len(findings))
            
            # ê²°ê³¼ í• ë‹¹
            overview["summary"] = summary if isinstance(summary, str) else None
            overview["recommended_actions"] = recs if isinstance(recs, list) else []
            overview["key_findings"] = findings if isinstance(findings, list) else []
            
            logging.info("ë”•ì…”ë„ˆë¦¬ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: ìš”ì•½=%s, ê¶Œê³ ì‚¬í•­=%dê°œ, ë°œê²¬ì‚¬í•­=%dê°œ",
                        "ìˆìŒ" if overview["summary"] else "ì—†ìŒ",
                        len(overview["recommended_actions"]),
                        len(overview["key_findings"]))
                        
        elif isinstance(analysis, str):
            logging.debug("ë¬¸ìì—´ í˜•íƒœì˜ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: ê¸¸ì´=%d", len(analysis))
            
            # ë¬¸ìì—´ ê²°ê³¼ë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš© (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì œí•œ ì„¤ì •, ê¸°ë³¸ê°’: 2000ì)
            summary_limit = int(os.getenv('STRING_SUMMARY_LIMIT', '2000'))
            if len(analysis) > summary_limit:
                overview["summary"] = analysis[:summary_limit] + "..."
                logging.debug("ë¬¸ìì—´ ìš”ì•½ ìë¦„: %dì â†’ %dì", len(analysis), len(overview["summary"]))
            else:
                overview["summary"] = analysis
                
            logging.info("ë¬¸ìì—´ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: ìš”ì•½ ê¸¸ì´=%dì", len(overview["summary"]))
            
        else:
            logging.warning("ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¶„ì„ ê²°ê³¼ íƒ€ì…: %s", type(analysis).__name__)
            
    except Exception as e:
        logging.exception("ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡°ëŠ” ë°˜í™˜
    
    logging.debug("build_results_overview() ì™„ë£Œ: ìš”ì•½=%s, ê¶Œê³ ì‚¬í•­=%dê°œ, ë°œê²¬ì‚¬í•­=%dê°œ",
                 "ìˆìŒ" if overview["summary"] else "ì—†ìŒ",
                 len(overview["recommended_actions"]),
                 len(overview["key_findings"]))
    
    return overview


import ast  # AST íŒŒì‹± (íŒŒìƒ PEG ìˆ˜ì‹ ì•ˆì „ í‰ê°€ìš©)
import math  # ìˆ˜í•™ ì—°ì‚° (ì´ë¯¸ importë¨, ì¤‘ë³µ ì œê±° í•„ìš”)
import re  # ì •ê·œí‘œí˜„ì‹ (ì‹œê°„ ë²”ìœ„ íŒŒì‹±ìš©)

# ===========================================
# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ imports
# ===========================================
from typing import Dict, Optional, Tuple  # íƒ€ì… íŒíŠ¸ ì§€ì›

import pandas as pd  # ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„
import psycopg2  # PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
import psycopg2.extras  # PostgreSQL í™•ì¥ ê¸°ëŠ¥ (DictCursor ë“±)
import requests  # HTTP í´ë¼ì´ì–¸íŠ¸
from fastmcp import FastMCP  # MCP ì„œë²„ í”„ë ˆì„ì›Œí¬
from requests.adapters import HTTPAdapter  # HTTP ì–´ëŒ‘í„° (ì¬ì‹œë„ ë¡œì§)
from urllib3.util.retry import Retry  # ì¬ì‹œë„ ì „ëµ


# ===========================================
# ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì • (í‘œì¤€í™”)
# ===========================================
# ì¤‘ì•™ ì„¤ì •(config.settings)ì˜ setup_loggingì„ ë‹¨ì¼ ì§„ì…ì ìœ¼ë¡œ ì‚¬ìš©
try:
    # get_app_settings()ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ config.get_settings()ë¥¼ í˜¸ì¶œí•˜ë©°,
    # get_settings()ëŠ” settings.setup_logging()ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    _ = get_app_settings()
    logging.getLogger(__name__).info("ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (config.settings)")
except Exception as e:
    # ìµœì¢… í´ë°±: í•˜ë“œì½”ë”©ëœ ê¸°ë³¸ê°’
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logging.getLogger(__name__).error("ë¡œê¹… ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: %s", e)


# ===========================================
# HTTP ì„¸ì…˜ ê´€ë¦¬
# ===========================================

def create_http_session() -> requests.Session:
    """
    ì¬ì‹œë„ ë¡œì§ê³¼ íƒ€ì„ì•„ì›ƒì´ ì„¤ì •ëœ requests ì„¸ì…˜ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    LLM API í˜¸ì¶œì„ ìœ„í•œ ì•ˆì •ì ì¸ HTTP ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ìë™ ì¬ì‹œë„, ë°±ì˜¤í”„ ì „ëµ, íƒ€ì„ì•„ì›ƒ ì„¤ì •ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
    Returns:
        requests.Session: ì„¤ì •ëœ HTTP ì„¸ì…˜ ê°ì²´
    """
    logging.info("create_http_session() í˜¸ì¶œ: HTTP ì„¸ì…˜ ìƒì„± ì‹œì‘")
    
    try:
        # ìƒˆë¡œìš´ ì„¸ì…˜ ê°ì²´ ìƒì„±
        session = requests.Session()
        logging.debug("requests.Session ê°ì²´ ìƒì„± ì™„ë£Œ")
        
        # ì¬ì‹œë„ ì „ëµ ì„¤ì •
        retry_total = int(os.getenv('LLM_RETRY_TOTAL', '3'))
        retry_backoff = float(os.getenv('LLM_RETRY_BACKOFF', '1.0'))
        timeout_seconds = int(os.getenv('LLM_TIMEOUT', '180'))
        
        logging.debug("ì¬ì‹œë„ ì„¤ì •: ì´_ì¬ì‹œë„=%d, ë°±ì˜¤í”„_íŒ©í„°=%.1f, íƒ€ì„ì•„ì›ƒ=%ds", 
                     retry_total, retry_backoff, timeout_seconds)
        
        retry_strategy = Retry(
            total=retry_total,                                    # ì´ ì¬ì‹œë„ íšŸìˆ˜
            backoff_factor=retry_backoff,                         # ì¬ì‹œë„ ê°„ê²© ë°°ìˆ˜ (1.0 = 1ì´ˆ, 2ì´ˆ, 4ì´ˆ...)
            status_forcelist=[429, 500, 502, 503, 504],          # ì¬ì‹œë„í•  HTTP ìƒíƒœ ì½”ë“œ
            allowed_methods=["POST"]                              # POST ìš”ì²­ë§Œ ì¬ì‹œë„
        )
        
        # HTTP ì–´ëŒ‘í„° ì„¤ì • ë° ë§ˆìš´íŠ¸
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        logging.debug("HTTP/HTTPS ì–´ëŒ‘í„° ë§ˆìš´íŠ¸ ì™„ë£Œ")
        
        # ê¸°ë³¸ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        session.timeout = timeout_seconds
        logging.debug("ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì„¤ì •: %ds", session.timeout)
        
        # User-Agent ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ê°€ëŠ¥)
        user_agent = os.getenv('HTTP_USER_AGENT', 'Cell-Performance-LLM-Analyzer/1.0')
        session.headers.update({
            'User-Agent': user_agent
        })
        logging.debug("User-Agent í—¤ë” ì„¤ì • ì™„ë£Œ")
        
        logging.info("HTTP ì„¸ì…˜ ìƒì„± ì™„ë£Œ: retry_total=%d, timeout=%ds, backoff=%.1f", 
                    retry_strategy.total, session.timeout, retry_backoff)
        
        return session
        
    except Exception as e:
        logging.exception("HTTP ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
        raise


# ===========================================
# MCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# ===========================================
# FastMCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
# ì´ ì¸ìŠ¤í„´ìŠ¤ëŠ” MCP(Model Context Protocol) ë„êµ¬ë“¤ì„ ë“±ë¡í•˜ê³  ì„œë¹™í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
mcp = FastMCP(name="Cell LLM ì¢…í•© ë¶„ì„ê¸°")
logging.info("FastMCP ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ: name=%s", mcp.name)


# ===========================================
# ì‹œê°„ ë²”ìœ„ íŒŒì‹± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ - DEPRECATED
# ì´ í•¨ìˆ˜ë“¤ì€ utils.TimeRangeParserë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤
# ===========================================

def _get_default_tzinfo() -> datetime.tzinfo:
    """
    í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ íƒ€ì„ì¡´ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    í™˜ê²½ ë³€ìˆ˜ `DEFAULT_TZ_OFFSET`(ì˜ˆ: "+09:00")ë¥¼ ì½ì–´ì„œ tzinfo ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì„¤ì •ì´ ì—†ê±°ë‚˜ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš° UTCë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        datetime.tzinfo: íƒ€ì„ì¡´ ì •ë³´ ê°ì²´
    """
    logging.debug("_get_default_tzinfo() í˜¸ì¶œ: ê¸°ë³¸ íƒ€ì„ì¡´ ì •ë³´ ìƒì„± ì‹œì‘")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ íƒ€ì„ì¡´ ì˜¤í”„ì…‹ ì½ê¸° (ê¸°ë³¸ê°’: +09:00)
    offset_text = os.getenv("DEFAULT_TZ_OFFSET", "+09:00").strip()
    logging.debug("íƒ€ì„ì¡´ ì˜¤í”„ì…‹ í™˜ê²½ë³€ìˆ˜: %s", offset_text)
    
    try:
        # ë¶€í˜¸ í™•ì¸ (+ ë˜ëŠ” -)
        if offset_text.startswith("+"):
            sign = 1
            offset_clean = offset_text[1:]
        elif offset_text.startswith("-"):
            sign = -1
            offset_clean = offset_text[1:]
        else:
            # ë¶€í˜¸ê°€ ì—†ìœ¼ë©´ ì–‘ìˆ˜ë¡œ ì²˜ë¦¬
            sign = 1
            offset_clean = offset_text
            logging.debug("ë¶€í˜¸ê°€ ì—†ëŠ” íƒ€ì„ì¡´ ì˜¤í”„ì…‹, ì–‘ìˆ˜ë¡œ ì²˜ë¦¬: %s", offset_text)
        
        # ì‹œê°„ê³¼ ë¶„ ë¶„ë¦¬
        hh_mm = offset_clean.split(":")
        if len(hh_mm) != 2:
            raise ValueError(f"ì˜ëª»ëœ íƒ€ì„ì¡´ í˜•ì‹: {offset_text} (ì˜ˆìƒ: +09:00)")
        
        hours = int(hh_mm[0])
        minutes = int(hh_mm[1])
        
        # ì‹œê°„ê³¼ ë¶„ì˜ ìœ íš¨ì„± ê²€ì‚¬
        if hours < 0 or hours > 23:
            raise ValueError(f"ì˜ëª»ëœ ì‹œê°„: {hours} (0-23 ë²”ìœ„)")
        if minutes < 0 or minutes > 59:
            raise ValueError(f"ì˜ëª»ëœ ë¶„: {minutes} (0-59 ë²”ìœ„)")
        
        # timedelta ê°ì²´ ìƒì„±
        delta = datetime.timedelta(hours=hours * sign, minutes=minutes * sign)
        tzinfo = datetime.timezone(delta)
        
        logging.info("íƒ€ì„ì¡´ ì •ë³´ ìƒì„± ì„±ê³µ: %s â†’ %s", offset_text, tzinfo)
        return tzinfo
        
    except Exception as e:
        logging.warning("DEFAULT_TZ_OFFSET íŒŒì‹± ì‹¤íŒ¨, UTC ì‚¬ìš©: %s (ì˜¤ë¥˜: %s)", offset_text, e)
        return datetime.timezone.utc

def parse_time_range(range_text: str) -> Tuple[datetime.datetime, datetime.datetime]:
    """
    ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì‹œì‘/ì¢…ë£Œ datetime ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    ì§€ì›í•˜ëŠ” ì…ë ¥ í˜•ì‹:
    1. ë²”ìœ„ í˜•ì‹: "YYYY-MM-DD_HH:MM~YYYY-MM-DD_HH:MM" ë˜ëŠ” "YYYY-MM-DD-HH:MM~YYYY-MM-DD-HH:MM"
    2. ë‹¨ì¼ ë‚ ì§œ: "YYYY-MM-DD" (00:00:00 ~ 23:59:59ë¡œ ìë™ í™•ì¥)
    
    íŠ¹ì§•:
    - ìœ ì—°í•œ í¬ë§·: ë‚ ì§œì™€ ì‹œê°„ êµ¬ë¶„ìë¡œ '_' ë˜ëŠ” '-' í—ˆìš©
    - íƒ€ì„ì¡´ ì§€ì›: í™˜ê²½ë³€ìˆ˜ DEFAULT_TZ_OFFSETì—ì„œ íƒ€ì„ì¡´ ì •ë³´ ì½ê¸°
    - ìƒì„¸í•œ ì˜¤ë¥˜ ì²˜ë¦¬: í˜•ì‹/ê°’/ë…¼ë¦¬/íƒ€ì… ì˜¤ë¥˜ë¥¼ ì„¸ë¶„í™”í•˜ì—¬ ëª…í™•í•œ ì˜ˆì™¸ ë©”ì‹œì§€ ì œê³µ
    
    Args:
        range_text (str): íŒŒì‹±í•  ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´
        
    Returns:
        Tuple[datetime.datetime, datetime.datetime]: (ì‹œì‘_ì‹œê°, ì¢…ë£Œ_ì‹œê°) - ë‘˜ ë‹¤ tz-aware
        
    Raises:
        TypeError: ì…ë ¥ì´ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°
        ValueError: í˜•ì‹, ê°’, ë…¼ë¦¬ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
    """
    logging.info("parse_time_range() í˜¸ì¶œ: ì…ë ¥ ë¬¸ìì—´ íŒŒì‹± ì‹œì‘: %s", range_text)

    # ===========================================
    # 1ë‹¨ê³„: íƒ€ì… ê²€ì¦
    # ===========================================
    logging.debug("1ë‹¨ê³„: íƒ€ì… ê²€ì¦ ì‹œì‘")
    if not isinstance(range_text, str):
        msg = {
            "code": "TYPE_ERROR",
            "message": "ì…ë ¥ì€ ë¬¸ìì—´(str)ì´ì–´ì•¼ í•©ë‹ˆë‹¤",
            "input": str(range_text),
            "received_type": type(range_text).__name__
        }
        logging.error("parse_time_range() íƒ€ì… ì˜¤ë¥˜: %s", msg)
        raise TypeError(json.dumps(msg, ensure_ascii=False))
    logging.debug("íƒ€ì… ê²€ì¦ í†µê³¼: str íƒ€ì… í™•ì¸ë¨")

    # ===========================================
    # 2ë‹¨ê³„: ì „ì²˜ë¦¬ ë° ê¸°ë³¸ ê²€ì¦
    # ===========================================
    logging.debug("2ë‹¨ê³„: ì „ì²˜ë¦¬ ë° ê¸°ë³¸ ê²€ì¦ ì‹œì‘")
    text = (range_text or "").strip()
    logging.debug("ì…ë ¥ ë¬¸ìì—´ ì „ì²˜ë¦¬: ì›ë³¸='%s' â†’ ì •ì œ='%s'", range_text, text)
    
    if text == "":
        msg = {
            "code": "FORMAT_ERROR",
            "message": "ë¹ˆ ë¬¸ìì—´ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "input": range_text,
            "hint": "ì˜ˆ: 2025-08-08_15:00~2025-08-08_19:00 ë˜ëŠ” 2025-08-08-15:00~2025-08-08-19:00 ë˜ëŠ” 2025-08-08"
        }
        logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
        raise ValueError(json.dumps(msg, ensure_ascii=False))
    logging.debug("ê¸°ë³¸ ê²€ì¦ í†µê³¼: ë¹ˆ ë¬¸ìì—´ ì•„ë‹˜")

    tzinfo = _get_default_tzinfo()

    # ì •ê·œì‹ íŒ¨í„´ (ìœ ì—°í•œ í¬ë§·: _ ë˜ëŠ” - êµ¬ë¶„ì í—ˆìš©)
    date_pat = r"\d{4}-\d{2}-\d{2}"
    time_pat = r"\d{2}:\d{2}"
    dt_pat_flexible = rf"{date_pat}[_-]{time_pat}"  # _ ë˜ëŠ” - í—ˆìš©

    # ë²”ìœ„ êµ¬ë¶„ì í—ˆìš©: ~ ì•ë’¤ ê³µë°± í—ˆìš©. ë‹¤ë¥¸ êµ¬ë¶„ì ì‚¬ìš©ì€ ì˜¤ë¥˜ ì²˜ë¦¬
    if "~" in text:
        # '~'ê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ì˜¤ë¥˜
        if text.count("~") != 1:
            msg = {
                "code": "FORMAT_ERROR",
                "message": "ë²”ìœ„ êµ¬ë¶„ì '~'ê°€ ì—†ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤",
                "input": text,
                "hint": "ì˜ˆ: 2025-08-08_15:00~2025-08-08_19:00 ë˜ëŠ” 2025-08-08-15:00~2025-08-08-19:00"
            }
            logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        # ê³µë°± í—ˆìš© ë¶„ë¦¬
        parts = [p.strip() for p in text.split("~")]
        if len(parts) != 2 or not parts[0] or not parts[1]:
            msg = {
                "code": "FORMAT_ERROR",
                "message": "ì‹œì‘/ì¢…ë£Œ ì‹œê°ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤",
                "input": text
            }
            logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        left, right = parts[0], parts[1]

        # ê° í† í° í˜•ì‹ ê²€ì¦ (ìœ ì—°í•œ íŒ¨í„´ ì‚¬ìš©)
        if not re.fullmatch(dt_pat_flexible, left):
            msg = {
                "code": "FORMAT_ERROR",
                "message": "ì™¼ìª½ ì‹œê° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (YYYY-MM-DD_HH:MM ë˜ëŠ” YYYY-MM-DD-HH:MM)",
                "input": left
            }
            logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))
        if not re.fullmatch(dt_pat_flexible, right):
            msg = {
                "code": "FORMAT_ERROR",
                "message": "ì˜¤ë¥¸ìª½ ì‹œê° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (YYYY-MM-DD_HH:MM ë˜ëŠ” YYYY-MM-DD-HH:MM)",
                "input": right
            }
            logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        # ë‚´ë¶€ ì²˜ë¦¬ë¥¼ ìœ„í•´ í‘œì¤€ _ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        def normalize_datetime_format(dt_str: str) -> str:
            """ë‚ ì§œ-ì‹œê°„ êµ¬ë¶„ìë¥¼ í‘œì¤€ '_' í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
            # YYYY-MM-DD-HH:MM í˜•íƒœë¥¼ YYYY-MM-DD_HH:MMë¡œ ë³€í™˜
            # ë§ˆì§€ë§‰ '-'ë§Œ '_'ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ rsplit ì‚¬ìš©
            if '-' in dt_str and dt_str.count('-') >= 3:
                # ë‚ ì§œ ë¶€ë¶„(ì²˜ìŒ 3ê°œ '-')ê³¼ ì‹œê°„ ë¶€ë¶„ì„ ë¶„ë¦¬
                parts = dt_str.rsplit('-', 1)
                if len(parts) == 2 and ':' in parts[1]:
                    return f"{parts[0]}_{parts[1]}"
            return dt_str

        left_normalized = normalize_datetime_format(left)
        right_normalized = normalize_datetime_format(right)
        
        logging.info("ì…ë ¥ ì •ê·œí™”: %s â†’ %s, %s â†’ %s", left, left_normalized, right, right_normalized)

        # ê°’ ê²€ì¦ (ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚ ì§œ/ì‹œê°„ ë“±)
        try:
            start_dt = datetime.datetime.strptime(left_normalized, "%Y-%m-%d_%H:%M")
            end_dt = datetime.datetime.strptime(right_normalized, "%Y-%m-%d_%H:%M")
        except Exception as e:
            msg = {
                "code": "VALUE_ERROR",
                "message": f"ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ/ì‹œê°„ì…ë‹ˆë‹¤: {e}",
                "input": text
            }
            logging.error("parse_time_range() ê°’ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        # tz ë¶€ì—¬
        if start_dt.tzinfo is None:
            start_dt = start_dt.replace(tzinfo=tzinfo)
        if end_dt.tzinfo is None:
            end_dt = end_dt.replace(tzinfo=tzinfo)

        # ë…¼ë¦¬ ê²€ì¦
        if start_dt == end_dt:
            msg = {
                "code": "LOGIC_ERROR",
                "message": "ë™ì¼í•œ ì‹œê° ë²”ìœ„ëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                "input": text
            }
            logging.error("parse_time_range() ë…¼ë¦¬ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))
        if start_dt > end_dt:
            msg = {
                "code": "LOGIC_ERROR",
                "message": "ì‹œì‘ ì‹œê°ì€ ì¢…ë£Œ ì‹œê°ë³´ë‹¤ ë¹ ë¼ì•¼ í•©ë‹ˆë‹¤",
                "input": text
            }
            logging.error("parse_time_range() ë…¼ë¦¬ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        logging.info("parse_time_range() ì„±ê³µ: %s ~ %s", start_dt, end_dt)
        return start_dt, end_dt

    # ë‹¨ì¼ ë‚ ì§œ ì¼€ì´ìŠ¤
    if re.fullmatch(date_pat, text):
        try:
            day = datetime.datetime.strptime(text, "%Y-%m-%d").date()
        except Exception as e:
            msg = {
                "code": "VALUE_ERROR",
                "message": f"ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œì…ë‹ˆë‹¤: {e}",
                "input": text
            }
            logging.error("parse_time_range() ê°’ ì˜¤ë¥˜: %s", msg)
            raise ValueError(json.dumps(msg, ensure_ascii=False))

        start_dt = datetime.datetime.combine(day, datetime.time(0, 0, 0, tzinfo=tzinfo))
        end_dt = datetime.datetime.combine(day, datetime.time(23, 59, 59, tzinfo=tzinfo))
        logging.info("parse_time_range() ì„±ê³µ(ë‹¨ì¼ ë‚ ì§œ í™•ì¥): %s ~ %s", start_dt, end_dt)
        return start_dt, end_dt

    # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ í˜•ì‹ ì˜¤ë¥˜
    # í”í•œ ì˜¤íƒ€ ì¼€ì´ìŠ¤ íŒíŠ¸ ì œê³µ
    uses_space_instead_separator = re.search(r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}", text) is not None
    time_with_dash = re.search(r"\d{2}-\d{2}", text) is not None and not re.search(dt_pat_flexible, text)

    hint = "ì˜ˆ: 2025-08-08_15:00~2025-08-08_19:00 ë˜ëŠ” 2025-08-08-15:00~2025-08-08-19:00 ë˜ëŠ” 2025-08-08"
    if uses_space_instead_separator:
        hint = "ë‚ ì§œì™€ ì‹œê°„ì€ ê³µë°±ì´ ì•„ë‹ˆë¼ '_' ë˜ëŠ” '-'ë¡œ êµ¬ë¶„í•˜ì„¸ìš”"
    elif time_with_dash:
        hint = "ì‹œê°„ì€ '15-00'ì´ ì•„ë‹ˆë¼ '15:00' í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤"

    msg = {
        "code": "FORMAT_ERROR",
        "message": "ì…ë ¥ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (YYYY-MM-DD_HH:MM~YYYY-MM-DD_HH:MM ë˜ëŠ” YYYY-MM-DD-HH:MM~YYYY-MM-DD-HH:MM ë˜ëŠ” YYYY-MM-DD)",
        "input": text,
        "hint": hint
    }
    logging.error("parse_time_range() í˜•ì‹ ì˜¤ë¥˜: %s", msg)
    raise ValueError(json.dumps(msg, ensure_ascii=False))


# --- DB ì—°ê²° ---
def get_db_connection(db: Dict[str, str]):
    """
    PostgreSQL ì—°ê²°ì„ ë°˜í™˜í•©ë‹ˆë‹¤. (psycopg2)

    db: {host, port, user, password, dbname}
    """
    # ì™¸ë¶€ DB ì—°ê²°: ë„¤íŠ¸ì›Œí¬/ê¶Œí•œ/í™˜ê²½ ë³€ìˆ˜ ë¬¸ì œë¡œ ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ìƒì„¸ ë¡œê·¸ë¥¼ ë‚¨ê¸´ë‹¤
    logging.info("get_db_connection() í˜¸ì¶œ: DB ì—°ê²° ì‹œë„")
    try:
        conn = psycopg2.connect(
            host=db.get("host", os.getenv("DEFAULT_DB_HOST", "127.0.0.1")),
            port=db.get("port", os.getenv("DEFAULT_DB_PORT", "5432")),
            user=db.get("user", os.getenv("DEFAULT_DB_USER", "postgres")),
            password=db.get("password", os.getenv("DEFAULT_DB_PASSWORD", "")),
            dbname=db.get("dbname", os.getenv("DEFAULT_DB_NAME", "postgres")),
        )
        # ë¯¼ê°ì •ë³´(password)ëŠ” ë¡œê·¸ì— ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤
        logging.info("DB ì—°ê²° ì„±ê³µ (host=%s, dbname=%s)", 
                    db.get("host", os.getenv("DEFAULT_DB_HOST", "127.0.0.1")), 
                    db.get("dbname", os.getenv("DEFAULT_DB_NAME", "postgres")))
        return conn
    except Exception as e:
        logging.exception("DB ì—°ê²° ì‹¤íŒ¨: %s", e)
        raise


# --- DB ì¡°íšŒ: ê¸°ê°„ë³„ ì…€ í‰ê·  ì§‘ê³„ ---
def fetch_cell_averages_for_period(
    conn,
    table: str,
    columns: Dict[str, str],
    start_dt: datetime.datetime,
    end_dt: datetime.datetime,
    period_label: str,
    ne_filters: Optional[list] = None,
    cellid_filters: Optional[list] = None,
    host_filters: Optional[list] = None,
) -> pd.DataFrame:
    """
    ì£¼ì–´ì§„ ê¸°ê°„ì— ëŒ€í•´ PEG ë‹¨ìœ„ í‰ê· ê°’ì„ ì§‘ê³„í•©ë‹ˆë‹¤.

    ë°˜í™˜ ì»¬ëŸ¼: [peg_name, period, avg_value]
    """
    logging.info("fetch_cell_averages_for_period() í˜¸ì¶œ: %s ~ %s, period=%s", start_dt, end_dt, period_label)
    time_col = columns.get("time", "datetime")
    # README ìŠ¤í‚¤ë§ˆ ê¸°ì¤€: peg_name ì»¬ëŸ¼ ì‚¬ìš©. columns ì‚¬ì „ì— 'peg' ë˜ëŠ” 'peg_name' í‚¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    peg_col = columns.get("peg") or columns.get("peg_name", "peg_name")
    value_col = columns.get("value", "value")
    ne_col = columns.get("ne", "ne")
    cell_col = columns.get("cell") or columns.get("cellid", "cellid")

    sql = f"SELECT {peg_col} AS peg_name, AVG({value_col}) AS avg_value FROM {table} WHERE {time_col} BETWEEN %s AND %s"
    params = [start_dt, end_dt]

    # ì„ íƒì  í•„í„°: ne, cellid
    if ne_filters:
        ne_vals = [str(x).strip() for x in (ne_filters or []) if str(x).strip()]
        if len(ne_vals) == 1:
            sql += f" AND {ne_col} = %s"
            params.append(ne_vals[0])
        elif len(ne_vals) > 1:
            placeholders = ",".join(["%s"] * len(ne_vals))
            sql += f" AND {ne_col} IN ({placeholders})"
            params.extend(ne_vals)

    if cellid_filters:
        cid_vals = [str(x).strip() for x in (cellid_filters or []) if str(x).strip()]
        if len(cid_vals) == 1:
            sql += f" AND {cell_col} = %s"
            params.append(cid_vals[0])
        elif len(cid_vals) > 1:
            placeholders = ",".join(["%s"] * len(cid_vals))
            sql += f" AND {cell_col} IN ({placeholders})"
            params.extend(cid_vals)

    # ì„ íƒì  í•„í„°: host (ì‹ ê·œ ì¶”ê°€)
    if host_filters:
        host_col = columns.get("host", "host")
        host_vals = [str(x).strip() for x in (host_filters or []) if str(x).strip()]
        if len(host_vals) == 1:
            sql += f" AND {host_col} = %s"
            params.append(host_vals[0])
        elif len(host_vals) > 1:
            placeholders = ",".join(["%s"] * len(host_vals))
            sql += f" AND {host_col} IN ({placeholders})"
            params.extend(host_vals)

    sql += f" GROUP BY {peg_col}"
    try:
        # ë™ì  í…Œì´ë¸”/ì»¬ëŸ¼ êµ¬ì„±ì´ë¯€ë¡œ ì‹¤í–‰ ì „ì— êµ¬ì„±ê°’ì„ ë¡œê·¸ë¡œ ë‚¨ê²¨ ë””ë²„ê¹…ì„ ë•ëŠ”ë‹¤
        logging.info(
            "ì§‘ê³„ SQL ì‹¤í–‰: table=%s, time_col=%s, peg_col=%s, value_col=%s, ne_col=%s, cell_col=%s, host_col=%s",
            table, time_col, peg_col, value_col, ne_col, cell_col, columns.get("host", "host"),
        )
        with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:
            cur.execute(sql, tuple(params))
            rows = cur.fetchall()
        # ì¡°íšŒ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ)
        df = pd.DataFrame(rows, columns=["peg_name", "avg_value"]) if rows else pd.DataFrame(columns=["peg_name", "avg_value"]) 
        df["period"] = period_label
        logging.info("fetch_cell_averages_for_period() ê±´ìˆ˜: %d (period=%s)", len(df), period_label)
        return df
    except Exception as e:
        logging.exception("ê¸°ê°„ë³„ í‰ê·  ì§‘ê³„ ì¿¼ë¦¬ ì‹¤íŒ¨: %s", e)
        raise


# --- íŒŒìƒ PEG ê³„ì‚°: ìˆ˜ì‹ ì •ì˜ë¥¼ ì•ˆì „í•˜ê²Œ í‰ê°€í•˜ì—¬ ìƒˆë¡œìš´ PEG ìƒì„± ---
def _safe_eval_expr(expr_text: str, variables: Dict[str, float]) -> float:
    """
    ê°„ë‹¨í•œ ì‚°ìˆ  ìˆ˜ì‹(expr_text)ì„ ì•ˆì „í•˜ê²Œ í‰ê°€í•©ë‹ˆë‹¤.
    í—ˆìš© í† í°: ìˆ«ì, ë³€ìˆ˜ëª…(peg_name), +, -, *, /, (, )
    ë³€ìˆ˜ê°’ì€ variables ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    logging.info("_safe_eval_expr() í˜¸ì¶œ: expr=%s", expr_text)
    try:
        node = ast.parse(expr_text, mode='eval')

        def _eval(node):
            if isinstance(node, ast.Expression):
                return _eval(node.body)
            if isinstance(node, ast.BinOp):
                left = _eval(node.left)
                right = _eval(node.right)
                if isinstance(node.op, ast.Add):
                    return float(left) + float(right)
                if isinstance(node.op, ast.Sub):
                    return float(left) - float(right)
                if isinstance(node.op, ast.Mult):
                    return float(left) * float(right)
                if isinstance(node.op, ast.Div):
                    return float(left) / float(right)
                raise ValueError("í—ˆìš©ë˜ì§€ ì•Šì€ ì—°ì‚°ì")
            if isinstance(node, ast.UnaryOp) and isinstance(node.op, (ast.UAdd, ast.USub)):
                operand = _eval(node.operand)
                return +float(operand) if isinstance(node.op, ast.UAdd) else -float(operand)
            if isinstance(node, ast.Num):
                return float(node.n)
            if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):
                return float(node.value)
            if isinstance(node, ast.Name):
                name = node.id
                if name not in variables:
                    raise KeyError(f"ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜: {name}")
                return float(variables[name])
            if isinstance(node, ast.Call):
                raise ValueError("í•¨ìˆ˜ í˜¸ì¶œì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            if isinstance(node, (ast.Attribute, ast.Subscript, ast.List, ast.Dict, ast.Tuple)):
                raise ValueError("í—ˆìš©ë˜ì§€ ì•Šì€ í‘œí˜„ì‹ í˜•ì‹")
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” AST ë…¸ë“œ")

        return float(_eval(node))
    except ZeroDivisionError:
        logging.warning("ìˆ˜ì‹ í‰ê°€ ì¤‘ 0ìœ¼ë¡œ ë‚˜ëˆ” ë°œìƒ: %s", expr_text)
        return float('nan')
    except Exception as e:
        logging.error("ìˆ˜ì‹ í‰ê°€ ì‹¤íŒ¨: %s (expr=%s)", e, expr_text)
        return float('nan')


def compute_derived_pegs_for_period(period_df: pd.DataFrame, definitions: Dict[str, str], period_label: str) -> pd.DataFrame:
    """
    period_df: [peg_name, avg_value] í˜•íƒœì˜ ë‹¨ì¼ ê¸°ê°„ ì§‘ê³„ ë°ì´í„°
    definitions: {derived_name: expr_text} í˜•íƒœì˜ íŒŒìƒ PEG ìˆ˜ì‹ ì •ì˜
    ë°˜í™˜: ë™ì¼ ì»¬ëŸ¼ì„ ê°–ëŠ” íŒŒìƒ PEG ë°ì´í„°í”„ë ˆì„
    """
    logging.info("compute_derived_pegs_for_period() í˜¸ì¶œ: period=%s, defs=%d", period_label, len(definitions or {}))
    if not isinstance(definitions, dict) or not definitions:
        return pd.DataFrame(columns=["peg_name", "avg_value", "period"])  # ë¹ˆ DF

    # ë³€ìˆ˜ ì‚¬ì „ êµ¬ì„± (peg_name -> avg_value)
    base_map: Dict[str, float] = {}
    try:
        for row in period_df.itertuples(index=False):
            base_map[str(row.peg_name)] = float(row.avg_value)
    except Exception:
        # ì»¬ëŸ¼ëª…ì´ ë‹¤ë¥¼ ê°€ëŠ¥ì„± ìµœì†Œí™”ë¥¼ ìœ„í•´ ë³´í˜¸
        ser = period_df.set_index('peg_name')['avg_value'] if 'peg_name' in period_df and 'avg_value' in period_df else None
        if ser is not None:
            base_map = {str(k): float(v) for k, v in ser.items()}

    rows = []
    for new_name, expr in definitions.items():
        try:
            value = _safe_eval_expr(str(expr), base_map)
            rows.append({"peg_name": str(new_name), "avg_value": float(value), "period": period_label})
            logging.info("íŒŒìƒ PEG ê³„ì‚°: %s = %s (period=%s)", new_name, value, period_label)
        except Exception as e:
            logging.error("íŒŒìƒ PEG ê³„ì‚° ì‹¤íŒ¨: %s (name=%s, period=%s)", e, new_name, period_label)
            continue
    return pd.DataFrame(rows, columns=["peg_name", "avg_value", "period"]) if rows else pd.DataFrame(columns=["peg_name", "avg_value", "period"]) 

# --- ì²˜ë¦¬: N-1/N ë³‘í•© + ë³€í™”ìœ¨ ê³„ì‚° ---
def process_and_analyze(n1_df: pd.DataFrame, n_df: pd.DataFrame) -> pd.DataFrame:
    """
    ë‘ ê¸°ê°„ì˜ PEG ì§‘ê³„ ë°ì´í„°ë¥¼ ë³‘í•©í•´ diff/pct_change ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    ë°˜í™˜:
      - processed_df: ['peg_name', 'avg_n_minus_1', 'avg_n', 'diff', 'pct_change']
    """
    # í•µì‹¬ ì²˜ë¦¬ ë‹¨ê³„: ë³‘í•© â†’ í”¼ë²— â†’ ë³€í™”ìœ¨ ì‚°ì¶œ
    logging.info("process_and_analyze() í˜¸ì¶œ: ë°ì´í„° ë³‘í•© ë° ì²˜ë¦¬ ì‹œì‘")
    try:
        all_df = pd.concat([n1_df, n_df], ignore_index=True)
        logging.info("ë³‘í•© ë°ì´í„°í”„ë ˆì„ í¬ê¸°: %sí–‰ x %sì—´", all_df.shape[0], all_df.shape[1])
        pivot = all_df.pivot(index="peg_name", columns="period", values="avg_value").fillna(0)
        logging.info("í”¼ë²— ê²°ê³¼ ì»¬ëŸ¼: %s", list(pivot.columns))
        if "N-1" not in pivot.columns or "N" not in pivot.columns:
            raise ValueError("N-1 ë˜ëŠ” N ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‹œê°„ ë²”ìœ„ ë˜ëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # ëª…ì„¸ ì»¬ëŸ¼ êµ¬ì„±
        out = pd.DataFrame({
            "peg_name": pivot.index,
            "avg_n_minus_1": pivot["N-1"],
            "avg_n": pivot["N"],
        })
        out["diff"] = out["avg_n"] - out["avg_n_minus_1"]
        out["pct_change"] = (out["diff"] / out["avg_n_minus_1"].replace(0, float("nan"))) * 100
        processed_df = out.round(2)

        logging.info("process_and_analyze() ì™„ë£Œ: processed_df=%dí–‰", len(processed_df))
        return processed_df
    except Exception as e:
        logging.exception("process_and_analyze() ì‹¤íŒ¨: %s", e)
        raise


 


# (YAML ê¸°ë°˜ í†µí•© í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì‚¬ìš©; êµ¬ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ ì œê±°ë¨)


# (YAML ê¸°ë°˜ í†µí•© í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì‚¬ìš©; êµ¬ íŠ¹ì • PEG í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ ì œê±°ë¨)



# --- LLM API í˜¸ì¶œ í•¨ìˆ˜ (requests ê¸°ë°˜) ---
def query_llm(prompt: str, enable_mock: bool = False) -> dict:
    """ë‚´ë¶€ vLLM ì„œë²„ë¡œ ë¶„ì„ ìš”ì²­. ì‘ë‹µ ë³¸ë¬¸ì—ì„œ JSONë§Œ ì¶”ì¶œ.
    ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ì—”ë“œí¬ì¸íŠ¸ë¡œ í˜ì¼ì˜¤ë²„.
    
    Args:
        prompt: LLMì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸
        enable_mock: Trueë©´ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ ì‹œ ê°€ìƒ ì‘ë‹µ ë°˜í™˜
    """
    # ì¥ì•  ëŒ€ë¹„ë¥¼ ìœ„í•´ ë³µìˆ˜ ì—”ë“œí¬ì¸íŠ¸ë¡œ í˜ì¼ì˜¤ë²„. ì‘ë‹µì—ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
    logging.info("query_llm() í˜¸ì¶œ: vLLM ë¶„ì„ ìš”ì²­ ì‹œì‘ (enable_mock=%s)", enable_mock)

    # Configuration Managerì—ì„œ LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    try:
        settings = get_app_settings()
        llm_config = settings.get_llm_config_dict()
        
        # ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • (í´ë°±: í™˜ê²½ë³€ìˆ˜)
        llm_endpoints_str = os.getenv('LLM_ENDPOINTS', 'http://10.251.204.93:10000,http://100.105.188.117:8888')
        endpoints = [endpoint.strip() for endpoint in llm_endpoints_str.split(',') if endpoint.strip()]

        if not endpoints:
            raise ValueError("LLM_ENDPOINTS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # Configuration Managerì—ì„œ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
        llm_model = llm_config['model']
        timeout = llm_config['timeout']
        
        payload = {
            "model": llm_model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": llm_config['temperature'],
            "max_tokens": llm_config['max_tokens'],
        }
        
        logging.debug("Configuration Managerì—ì„œ LLM í˜ì´ë¡œë“œ êµ¬ì„±: ëª¨ë¸=%s, ì˜¨ë„=%.1f, ìµœëŒ€í† í°=%d", 
                     llm_model, llm_config['temperature'], llm_config['max_tokens'])
        
    except Exception as e:
        # í´ë°±: í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©
        logging.warning("Configuration Manager ë¡œë”© ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©: %s", e)
        
        llm_endpoints_str = os.getenv('LLM_ENDPOINTS', 'http://10.251.204.93:10000,http://100.105.188.117:8888')
        endpoints = [endpoint.strip() for endpoint in llm_endpoints_str.split(',') if endpoint.strip()]
        
        if not endpoints:
            raise ValueError("LLM_ENDPOINTS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        llm_model = os.getenv('LLM_MODEL', 'Gemma-3-27B')
    timeout = int(os.getenv('LLM_TIMEOUT', '180'))

    payload = {
        "model": llm_model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": float(os.getenv('LLM_TEMPERATURE', '0.2')),
        "max_tokens": int(os.getenv('LLM_MAX_TOKENS', '4096')),
    }
    
    logging.info("LLM ìš”ì²­ ì¤€ë¹„: endpoints=%d, prompt_length=%d, timeout=%ds", 
                len(endpoints), len(prompt), timeout)

    # HTTP ì„¸ì…˜ ìƒì„±
    session = create_http_session()

    for endpoint in endpoints:
        try:
            logging.info("ì—”ë“œí¬ì¸íŠ¸ ì ‘ì† ì‹œë„: %s", endpoint)
            
            # requestsë¥¼ ì‚¬ìš©í•œ POST ìš”ì²­
            response = session.post(
                f'{endpoint}/v1/chat/completions',
                json=payload,
                headers={'Content-Type': 'application/json'},
                timeout=timeout
            )
            
            # HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
            if response.status_code != 200:
                logging.error("HTTP ì—ëŸ¬ ì‘ë‹µ ìˆ˜ì‹  (%s): status=%d, body=%s", 
                            endpoint, response.status_code, response.text[:500])
                continue
                
            if not response.text:
                logging.error("ì‘ë‹µ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (%s)", endpoint)
                continue
                
            response_json = response.json()
            
            # API ì—ëŸ¬ í™•ì¸
            if 'error' in response_json:
                logging.error("API ì—ëŸ¬ ì‘ë‹µ ìˆ˜ì‹  (%s): %s", endpoint, response_json['error'])
                continue
                
            if 'choices' not in response_json or not response_json['choices']:
                logging.error("'choices' ì—†ìŒ ë˜ëŠ” ë¹„ì–´ìˆìŒ (%s): %s", endpoint, response_json)
                continue
                
            analysis_content = response_json['choices'][0]['message']['content']
            if not analysis_content or not analysis_content.strip():
                logging.error("'content' ë¹„ì–´ìˆìŒ (%s)", endpoint)
                continue

            # JSON ì¶”ì¶œ ë° íŒŒì‹±
            cleaned_json_str = analysis_content
            if '{' in cleaned_json_str:
                start_index, end_index = cleaned_json_str.find('{'), cleaned_json_str.rfind('}')
                if start_index != -1 and end_index != -1:
                    cleaned_json_str = cleaned_json_str[start_index: end_index + 1]
                    logging.info("ì‘ë‹µ ë¬¸ìì—´ì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ ì„±ê³µ")
                else:
                    logging.error("JSON ë²”ìœ„ ì¶”ì¶œ ì‹¤íŒ¨ (%s)", endpoint)
                    continue
            else:
                logging.error("ì‘ë‹µì— '{' ì—†ìŒ (%s)", endpoint)
                continue

            analysis_result = json.loads(cleaned_json_str)
            # ê²°ê³¼ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ì£¼ìš” í‚¤ë¥¼ ê¸°ë¡
            logging.info(
                "LLM ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹  ì„±ê³µ (%s): keys=%s",
                endpoint, list(analysis_result.keys()) if isinstance(analysis_result, dict) else type(analysis_result)
            )
            
            # ì„¸ì…˜ ì •ë¦¬
            session.close()
            return analysis_result
            
        except requests.exceptions.Timeout as e:
            logging.error("ìš”ì²­ íƒ€ì„ì•„ì›ƒ (%s): %s", endpoint, e)
            continue
        except requests.exceptions.ConnectionError as e:
            logging.error("ì—°ê²° ì˜¤ë¥˜ (%s): %s", endpoint, e)
            continue
        except requests.exceptions.RequestException as e:
            logging.error("requests ì˜ˆì™¸ (%s): %s", endpoint, e)
            continue
        except json.JSONDecodeError as e:
            logging.error("JSON íŒŒì‹± ì‹¤íŒ¨ (%s): %s", endpoint, e)
            logging.error("íŒŒì‹± ì‹œë„ ë‚´ìš©(1000ì): %s...", cleaned_json_str[:1000])
            continue
        except Exception as e:
            logging.error("ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ (%s): %s", type(e).__name__, e, exc_info=True)
            continue
    
    # ì„¸ì…˜ ì •ë¦¬
    session.close()
    
    # ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
    raise ConnectionError("ëª¨ë“  LLM API ì—”ë“œí¬ì¸íŠ¸ì— ì—°ê²°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")







# --- ë°±ì—”ë“œ POST ---
def post_results_to_backend(url: str, payload: dict, timeout: int = 15) -> Optional[dict]:
    """ë¶„ì„ JSON ê²°ê³¼ë¥¼ FastAPI ë°±ì—”ë“œë¡œ POST ì „ì†¡í•©ë‹ˆë‹¤."""
    # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜/íƒ€ì„ì•„ì›ƒ ëŒ€ë¹„. ìƒíƒœì½”ë“œ/ë³¸ë¬¸ íŒŒì‹± ê²°ê³¼ë¥¼ ê¸°ë¡í•´ ì›ì¸ ì¶”ì ì„ ìš©ì´í•˜ê²Œ í•¨
    logging.info("post_results_to_backend() í˜¸ì¶œ: %s", url)
    
    def _sanitize_for_json(value):
        """NaN/Infinity ë° ë„˜íŒŒì´ ìˆ˜ì¹˜ë¥¼ JSON í˜¸í™˜ìœ¼ë¡œ ì •ê·œí™”í•œë‹¤."""
        try:
            # dict/list ì¬ê·€ ì²˜ë¦¬
            if isinstance(value, dict):
                return {k: _sanitize_for_json(v) for k, v in value.items()}
            if isinstance(value, list):
                return [_sanitize_for_json(v) for v in value]
            # ìˆ˜ì¹˜í˜•: ë„˜íŒŒì´ í¬í•¨ì„ float()ë¡œ í¡ìˆ˜
            if isinstance(value, (int, float)):
                return value if math.isfinite(float(value)) else None
            # ë¬¸ìì—´ íƒ€ì…ì€ float ë³€í™˜í•˜ì§€ ì•ŠìŒ (cellid, ne ë“± ID ê°’ ë³´ì¡´)
            if isinstance(value, str):
                return value
            # ê¸°íƒ€ íƒ€ì…: ë„˜íŒŒì´ ìŠ¤ì¹¼ë¼ ë“±ì€ float() ì‹œë„ (ë‹¨, ë¬¸ìì—´ ì œì™¸)
            try:
                f = float(value)  # numpy.float64 ë“±
                return f if math.isfinite(f) else None
            except Exception:
                return value
        except Exception:
            return value
    
    safe_payload = _sanitize_for_json(payload)
    
    try:
        # allow_nan=False ë³´ì¥ ì§ë ¬í™” í›„ ì „ì†¡ (ì„œë²„ì™€ ê·œê²© ì¼ì¹˜)
        json_text = json.dumps(safe_payload, ensure_ascii=False, allow_nan=False)
        try:
            parsed_preview = json.loads(json_text)
        except Exception:
            parsed_preview = safe_payload
        logging.info("PAYLOAD %s", json.dumps({
            "url": url,
            "method": "POST",
            "payload": parsed_preview,
        }, ensure_ascii=False, indent=2))

        # POST ì‹œë„
        resp = requests.post(
            url,
            data=json_text.encode('utf-8'),
            headers={'Content-Type': 'application/json; charset=utf-8'},
            timeout=timeout
        )

        # ì„±ê³µì ì¸ ìƒíƒœì½”ë“œ í™•ì¸ (200, 201, 202 ë“±)
        if resp.status_code in [200, 201, 202]:
            logging.info("ë°±ì—”ë“œ POST ì„±ê³µ: status=%s", resp.status_code)
            try:
                return resp.json()
            except Exception:
                # JSON ì‘ë‹µì´ ì—†ê±°ë‚˜ íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                return {"status": "success", "message": "POST ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."}
        
        # ê·¸ ì™¸ ìƒíƒœì½”ë“œëŠ” ì˜ˆì™¸ë¡œ ì²˜ë¦¬
        logging.error("ë°±ì—”ë“œ POST ì‹¤íŒ¨: status=%s body=%s", resp.status_code, resp.text[:500])
        resp.raise_for_status()
        return None
    except Exception as e:
        logging.exception("ë°±ì—”ë“œ POST ì‹¤íŒ¨: %s", e)
        return None


# --- MCP Handler (Presentation Layer) ---
class MCPHandler:
    """
    MCP ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” Presentation Layer Handler
    
    ê¸°ì¡´ì˜ monolithic _analyze_cell_performance_logic()ë¥¼ leaní•œ í•¸ë“¤ëŸ¬ë¡œ ë¦¬íŒ©í† ë§
    AnalysisServiceì— ìœ„ì„í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ë¡œì§ì„ ì²˜ë¦¬í•˜ê³ ,
    MCP ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ë§Œ ë‹´ë‹¹
    """
    
    def __init__(self, analysis_service: AnalysisService = None):
        """
        MCPHandler ì´ˆê¸°í™”
        
        Args:
            analysis_service (AnalysisService, optional): ë¶„ì„ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        """
        self.analysis_service = analysis_service
        self.logger = logging.getLogger(__name__ + '.MCPHandler')
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        self._load_default_settings()
        
        self.logger.info("MCPHandler ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_default_settings(self) -> None:
        """ê¸°ë³¸ ì„¤ì • ë¡œë“œ (Configuration Manager ìš°ì„ , í™˜ê²½ë³€ìˆ˜ í´ë°±)"""
        try:
            settings = get_app_settings()
            self.default_backend_url = str(settings.backend_service_url)
            self.default_db = {
                "host": settings.db_host,
                "port": settings.db_port,
                "user": settings.db_user,
                "password": settings.db_password.get_secret_value(),
                "dbname": settings.db_name
            }
            self.logger.debug("Configuration Managerì—ì„œ ê¸°ë³¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            self.logger.warning("Configuration Manager ë¡œë”© ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©: %s", e)
            self.default_backend_url = os.getenv('DEFAULT_BACKEND_URL', 'http://165.213.69.30:8000/api/analysis/results/')
            self.default_db = {
                "host": os.getenv('DEFAULT_DB_HOST', '127.0.0.1'),
                "port": int(os.getenv('DEFAULT_DB_PORT', '5432')),
                "user": os.getenv('DEFAULT_DB_USER', 'postgres'),
                "password": os.getenv('DEFAULT_DB_PASSWORD', ''),
                "dbname": os.getenv('DEFAULT_DB_NAME', 'postgres')
            }
    
    def _validate_basic_request(self, request: dict) -> None:
        """
        ê¸°ë³¸ ìš”ì²­ ê²€ì¦
        
        Args:
            request (dict): MCP ìš”ì²­ ë°ì´í„°
            
        Raises:
            ValueError: í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ë˜ëŠ” ì˜ëª»ëœ í˜•ì‹
        """
        self.logger.debug("_validate_basic_request() í˜¸ì¶œ: ê¸°ë³¸ ìš”ì²­ ê²€ì¦ ì‹œì‘")
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        n1_text = request.get('n_minus_1') or request.get('n1')
        n_text = request.get('n')
        
        if not n1_text or not n_text:
            raise ValueError("'n_minus_1'ì™€ 'n' ì‹œê°„ ë²”ìœ„ë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ê¸°ë³¸ íƒ€ì… ê²€ì¦
        if not isinstance(request, dict):
            raise ValueError("ìš”ì²­ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
        
        self.logger.info("ê¸°ë³¸ ìš”ì²­ ê²€ì¦ í†µê³¼: n_minus_1=%s, n=%s", n1_text, n_text)
    
    def _parse_request_to_analysis_format(self, request: dict) -> dict:
        """
        MCP ìš”ì²­ì„ AnalysisService í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            request (dict): ì›ë³¸ MCP ìš”ì²­
            
        Returns:
            dict: AnalysisService í˜¸í™˜ í˜•ì‹
        """
        self.logger.debug("_parse_request_to_analysis_format() í˜¸ì¶œ: ìš”ì²­ í˜•ì‹ ë³€í™˜")
        
        # ê¸°ë³¸ í•„ë“œ
        analysis_request = {
            'n_minus_1': request.get('n_minus_1') or request.get('n1'),
            'n': request.get('n'),
            'output_dir': request.get('output_dir', os.path.abspath('./analysis_output')),
            'analysis_type': 'enhanced',  # ê¸°ë³¸ê°’
            'enable_mock': False  # í”„ë¡œë•ì…˜ ëª¨ë“œ
        }
        
        # ë°±ì—”ë“œ URL ì„¤ì •
        analysis_request['backend_url'] = request.get('backend_url') or self.default_backend_url
        
        # DB ì„¤ì •
        analysis_request['db'] = request.get('db', self.default_db)
        
        # í…Œì´ë¸” ë° ì»¬ëŸ¼ ì„¤ì •
        analysis_request['table'] = request.get('table') or os.getenv('DEFAULT_TABLE', 'summary')
        analysis_request['columns'] = request.get('columns', {
            "time": os.getenv('DEFAULT_TIME_COLUMN', 'datetime'),
            "peg_name": os.getenv('DEFAULT_PEG_COLUMN', 'peg_name'),
            "value": os.getenv('DEFAULT_VALUE_COLUMN', 'value'),
            "ne": os.getenv('DEFAULT_NE_COLUMN', 'ne'),
            "cellid": os.getenv('DEFAULT_CELL_COLUMN', 'cellid'),
            "host": os.getenv('DEFAULT_HOST_COLUMN', 'host')
        })
        
        # í•„í„° ì„¤ì •
        filters = {}
        if request.get('ne'):
            filters['ne'] = request['ne']
        if request.get('cellid') or request.get('cell'):
            filters['cellid'] = request.get('cellid') or request.get('cell')
        if request.get('host'):
            filters['host'] = request['host']
        analysis_request['filters'] = filters
        
        # PEG ê´€ë ¨ ì„¤ì •
        if request.get('preference') or request.get('selected_pegs'):
            analysis_request['selected_pegs'] = request.get('selected_pegs') or request.get('preference')
        
        if request.get('peg_definitions'):
            analysis_request['peg_definitions'] = request['peg_definitions']
        
        # í”„ë¡¬í”„íŠ¸ ì œí•œ ì„¤ì •
        analysis_request['max_prompt_tokens'] = request.get('max_prompt_tokens', DEFAULT_MAX_PROMPT_TOKENS)
        analysis_request['max_prompt_chars'] = request.get('max_prompt_chars', DEFAULT_MAX_PROMPT_CHARS)
        
        self.logger.info("ìš”ì²­ í˜•ì‹ ë³€í™˜ ì™„ë£Œ: %dê°œ í•„ë“œ", len(analysis_request))
        return analysis_request
    
    def _create_analysis_service(self) -> AnalysisService:
        """
        AnalysisService ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì˜ì¡´ì„± ì£¼ì…)
        
        Returns:
            AnalysisService: êµ¬ì„±ëœ ë¶„ì„ ì„œë¹„ìŠ¤
        """
        self.logger.debug("_create_analysis_service() í˜¸ì¶œ: AnalysisService ìƒì„±")
        
        # DatabaseRepository ìƒì„±
        db_repo = PostgreSQLRepository()
        
        # LLMRepository ìƒì„±
        llm_repo = LLMClient()
        
        # AnalysisService ìƒì„± (ì˜ì¡´ì„± ì£¼ì…)
        service = AnalysisService(
            database_repository=db_repo,
            llm_analysis_service=None  # LLMAnalysisServiceëŠ” ë‚´ë¶€ì—ì„œ ìƒì„±
        )
        
        self.logger.info("AnalysisService ìƒì„± ì™„ë£Œ")
        return service
    
    def _format_response_for_mcp(self, analysis_result: dict) -> dict:
        """
        AnalysisService ê²°ê³¼ë¥¼ MCP í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            analysis_result (dict): AnalysisService ê²°ê³¼
            
        Returns:
            dict: MCP í˜¸í™˜ ì‘ë‹µ í˜•ì‹
        """
        self.logger.debug("_format_response_for_mcp() í˜¸ì¶œ: ì‘ë‹µ í˜•ì‹ ë³€í™˜")
        
        # ì¤‘ë³µ ì œê±°: AnalysisService ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  í•„ìš”í•œ í•„ë“œë§Œ ì¶”ê°€/ë³€ê²½
        mcp_response = analysis_result.copy()
        
        # MCP ì „ìš© í•„ë“œ ì¶”ê°€
        if mcp_response.get("status") == "success":
            mcp_response["message"] = "ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        elif mcp_response.get("status") == "error":
            mcp_response["message"] = mcp_response.get("message", "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # ë°±ì—”ë“œì—ì„œ ì¤‘ë³µ ì²˜ë¦¬í•˜ë¯€ë¡œ MCP ì¸¡ ì¤‘ë³µ ì½”ë“œ ì œê±°ë¨
        # í•˜ìœ„ í˜¸í™˜ì„±ì€ ë°±ì—”ë“œ ì‘ë‹µì—ì„œ ì²˜ë¦¬
        
        # ë°±ì—”ë“œ ì „ì†¡ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í¬í•¨ (ì´ë¯¸ analysis_resultì— ìˆìœ¼ë©´ ì¤‘ë³µ ë°©ì§€)
        if "backend_response" in analysis_result and "backend_response" not in mcp_response:
            mcp_response["backend_response"] = analysis_result["backend_response"]
        
        self.logger.info("MCP ì‘ë‹µ í˜•ì‹ ë³€í™˜ ì™„ë£Œ: %dê°œ í‚¤ (ì¤‘ë³µ ì œê±°ë¨)", len(mcp_response))
        return mcp_response
    
    def handle_request(self, request: dict) -> dict:
        """
        MCP ìš”ì²­ ì²˜ë¦¬ ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
        
        Args:
            request (dict): MCP ìš”ì²­ ë°ì´í„°
            
        Returns:
            dict: MCP ì‘ë‹µ ë°ì´í„°
            
        Raises:
            ValueError: ìš”ì²­ ê²€ì¦ ì‹¤íŒ¨
            Exception: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
        """
        self.logger.info("=" * 20 + " MCP Handler ìš”ì²­ ì²˜ë¦¬ ì‹œì‘ " + "=" * 20)
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ìš”ì²­ ê²€ì¦
            self.logger.info("1ë‹¨ê³„: ê¸°ë³¸ ìš”ì²­ ê²€ì¦")
            self._validate_basic_request(request)
            
            # 2ë‹¨ê³„: ìš”ì²­ í˜•ì‹ ë³€í™˜
            self.logger.info("2ë‹¨ê³„: ìš”ì²­ í˜•ì‹ ë³€í™˜")
            analysis_request = self._parse_request_to_analysis_format(request)
            
            # 3ë‹¨ê³„: AnalysisService ìƒì„± (í•„ìš”ì‹œ)
            if not self.analysis_service:
                self.logger.info("3ë‹¨ê³„: AnalysisService ìƒì„±")
                self.analysis_service = self._create_analysis_service()
            
            # 4ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰
            self.logger.info("4ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰ (AnalysisService ìœ„ì„)")
            analysis_result = self.analysis_service.perform_analysis(analysis_request)
            
            # 5ë‹¨ê³„: ì‘ë‹µ í˜•ì‹ ë³€í™˜
            self.logger.info("5ë‹¨ê³„: MCP ì‘ë‹µ í˜•ì‹ ë³€í™˜")
            mcp_response = self._format_response_for_mcp(analysis_result)
            
            self.logger.info("=" * 20 + " MCP Handler ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ " + "=" * 20)
            return mcp_response
            
        except ValueError as e:
            # ê²€ì¦ ì˜¤ë¥˜ (400 Bad Request)
            self.logger.error("ìš”ì²­ ê²€ì¦ ì‹¤íŒ¨: %s", e)
            return {
                "status": "error",
                "error_type": "validation_error",
                "message": str(e),
                "details": "ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }
            
        except AnalysisServiceError as e:
            # ë¶„ì„ ì„œë¹„ìŠ¤ ì˜¤ë¥˜ (500 Internal Server Error)
            self.logger.error("ë¶„ì„ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: %s", e)
            return {
                "status": "error", 
                "error_type": "analysis_error",
                "message": str(e),
                "details": e.to_dict() if hasattr(e, 'to_dict') else None,
                "workflow_step": getattr(e, 'workflow_step', None)
            }
            
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (500 Internal Server Error)
            self.logger.exception("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: %s", e)
            return {
                "status": "error",
                "error_type": "internal_error", 
                "message": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "details": str(e)
            }
    
    def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.analysis_service:
            self.analysis_service.close()
        self.logger.info("MCPHandler ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.close()
        return False


# --- ê¸°ì¡´ Monolithic ë¡œì§ (DEPRECATED) ---
def _analyze_cell_performance_logic(request: dict) -> dict:
    """
    ìš”ì²­ íŒŒë¼ë¯¸í„°:
      - n_minus_1: "yyyy-mm-dd_hh:mm~yyyy-mm-dd_hh:mm"
      - n: "yyyy-mm-dd_hh:mm~yyyy-mm-dd_hh:mm"
      - output_dir: str (ê¸°ë³¸ ./analysis_output)
      - backend_url: str (ì„ íƒ)
      - db: {host, port, user, password, dbname}
      - table: str (ê¸°ë³¸ 'summary')
      - columns: {time: 'datetime', peg_name: 'peg_name', value: 'value'}
      - ne: ë¬¸ìì—´ ë˜ëŠ” ë°°ì—´. ì˜ˆ: "nvgnb#10000" ë˜ëŠ” ["nvgnb#10000","nvgnb#20000"]
      - cellid|cell: ë¬¸ìì—´(ì‰¼í‘œ êµ¬ë¶„) ë˜ëŠ” ë°°ì—´. ì˜ˆ: "2010,2011" ë˜ëŠ” [2010,2011]
      - host: ë¬¸ìì—´ ë˜ëŠ” ë°°ì—´. ì˜ˆ: "192.168.1.1" ë˜ëŠ” ["host01","192.168.1.10"]
        â†’ ì œê³µ ì‹œ DB ì§‘ê³„ì—ì„œ í•´ë‹¹ ì¡°ê±´ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ PEG í‰ê· ì„ ê³„ì‚°
      - preference: ì‰¼í‘œ êµ¬ë¶„ ë¬¸ìì—´ ë˜ëŠ” ë°°ì—´. ì •í™•í•œ peg_nameë§Œ ì¸ì‹í•˜ì—¬ 'íŠ¹ì • peg ë¶„ì„' ëŒ€ìƒ ì„ ì •
      - selected_pegs: ë°°ì—´. ëª…ì‹œì  ì„ íƒ ëª©ë¡ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
      - peg_definitions: {íŒŒìƒPEGì´ë¦„: ìˆ˜ì‹ ë¬¸ìì—´}. ì˜ˆ: {"telus_RACH_Success": "A/B*100"}
        ìˆ˜ì‹ ì§€ì›: ìˆ«ì, ë³€ìˆ˜(peg_name), +, -, *, /, (), ë‹¨í•­ +/-. 0 ë‚˜ëˆ—ì…ˆì€ NaN ì²˜ë¦¬
        ì ìš© ì‹œì : N-1, N ê°ê°ì˜ ì§‘ê³„ ê²°ê³¼ì— ëŒ€í•´ ê³„ì‚° í›„ ì›ë³¸ê³¼ ë³‘í•© â†’ ì „ì²´ ì²˜ë¦¬/ë¶„ì„ì— í¬í•¨
    """
    logging.info("=" * 20 + " Cell ì„±ëŠ¥ ë¶„ì„ ë¡œì§ ì‹¤í–‰ ì‹œì‘ " + "=" * 20)
    try:
        # íŒŒë¼ë¯¸í„° íŒŒì‹±
        n1_text = request.get('n_minus_1') or request.get('n1')
        n_text = request.get('n')
        if not n1_text or not n_text:
            raise ValueError("'n_minus_1'ì™€ 'n' ì‹œê°„ ë²”ìœ„ë¥¼ ëª¨ë‘ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

        output_dir = request.get('output_dir', os.path.abspath('./analysis_output'))
        # ë°±ì—”ë“œ ì—…ë¡œë“œ URL: ìš”ì²­ê°’ > Configuration Manager > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’ ìˆœìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ì ìš©
        try:
            settings = get_app_settings()
            default_backend_url = str(settings.backend_service_url)
            logging.debug("Configuration Managerì—ì„œ ë°±ì—”ë“œ URL ë¡œë“œ: %s", default_backend_url)
        except Exception as e:
            logging.warning("Configuration Manager ë¡œë”© ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©: %s", e)
            default_backend_url = os.getenv('DEFAULT_BACKEND_URL', 'http://165.213.69.30:8000/api/analysis/results/')
        
        backend_url = request.get('backend_url') or default_backend_url

        # DB ì„¤ì •: ìš”ì²­ê°’ > Configuration Manager > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’ ìˆœìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ì ìš©
        try:
            settings = get_app_settings()
            default_db = {
                "host": settings.db_host,
                "port": settings.db_port,
                "user": settings.db_user,
                "password": settings.db_password.get_secret_value(),
                "dbname": settings.db_name
            }
            logging.debug("Configuration Managerì—ì„œ DB ê¸°ë³¸ê°’ ë¡œë“œ")
        except Exception as e:
            logging.warning("Configuration Manager ë¡œë”© ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì‚¬ìš©: %s", e)
            default_db = {
            "host": os.getenv('DEFAULT_DB_HOST', '127.0.0.1'),
            "port": int(os.getenv('DEFAULT_DB_PORT', '5432')),
            "user": os.getenv('DEFAULT_DB_USER', 'postgres'),
            "password": os.getenv('DEFAULT_DB_PASSWORD', ''),
            "dbname": os.getenv('DEFAULT_DB_NAME', 'postgres')
            }
        
        db = request.get('db', default_db)
        
        # í…Œì´ë¸” ë° ì»¬ëŸ¼ ì„¤ì •: ìš”ì²­ê°’ > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’
        table = request.get('table') or os.getenv('DEFAULT_TABLE', 'summary')
        columns = request.get('columns', {
            "time": os.getenv('DEFAULT_TIME_COLUMN', 'datetime'),
            "peg_name": os.getenv('DEFAULT_PEG_COLUMN', 'peg_name'),
            "value": os.getenv('DEFAULT_VALUE_COLUMN', 'value'),
            "ne": os.getenv('DEFAULT_NE_COLUMN', 'ne'),
            "cellid": os.getenv('DEFAULT_CELL_COLUMN', 'cellid'),
            "host": os.getenv('DEFAULT_HOST_COLUMN', 'host')
        })

        # íŒŒë¼ë¯¸í„° ìš”ì•½ ë¡œê·¸: ë¯¼ê°ì •ë³´ëŠ” ê¸°ë¡í•˜ì§€ ì•ŠìŒ
        logging.info(
            "ìš”ì²­ ìš”ì•½: output_dir=%s, backend_url=%s, table=%s, columns=%s",
            output_dir, bool(backend_url), table, columns
        )

        # ì‹œê°„ ë²”ìœ„ íŒŒì‹± (ìƒˆë¡œìš´ TimeRangeParser ì‚¬ìš©)
        time_parser = TimeRangeParser()
        try:
            n1_start, n1_end = time_parser.parse(n1_text)
            n_start, n_end = time_parser.parse(n_text)
            logging.info("ì‹œê°„ ë²”ìœ„: N-1(%s~%s), N(%s~%s)", n1_start, n1_end, n_start, n_end)
        except TimeParsingError as e:
            # TimeParsingErrorë¥¼ ê¸°ì¡´ í˜•ì‹ì˜ ValueErrorë¡œ ë³€í™˜í•˜ì—¬ í˜¸í™˜ì„± ìœ ì§€
            error_json = e.to_json_error()
            logging.error("ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: %s", error_json)
            raise ValueError(json.dumps(error_json, ensure_ascii=False))

        # DB ì¡°íšŒ
        conn = get_db_connection(db)
        try:
            # ì„ íƒì  ì…ë ¥ í•„í„° ìˆ˜ì§‘: ne, cellid, host
            # request ì˜ˆì‹œ: { "ne": "nvgnb#10000" } ë˜ëŠ” { "ne": ["nvgnb#10000","nvgnb#20000"], "cellid": "2010,2011", "host": "192.168.1.1" }
            ne_raw = request.get('ne')
            cell_raw = request.get('cellid') or request.get('cell')
            host_raw = request.get('host')

            def to_list(raw):
                if raw is None:
                    return []
                if isinstance(raw, str):
                    return [t.strip() for t in raw.split(',') if t.strip()]
                if isinstance(raw, list):
                    return [str(t).strip() for t in raw if str(t).strip()]
                return [str(raw).strip()]

            ne_filters = to_list(ne_raw)
            cellid_filters = to_list(cell_raw)
            host_filters = to_list(host_raw)

            logging.info("ì…ë ¥ í•„í„°: ne=%s (type: %s), cellid=%s (type: %s), host=%s (type: %s)",
                        ne_filters, [type(x).__name__ for x in ne_filters] if ne_filters else '[]',
                        cellid_filters, [type(x).__name__ for x in cellid_filters] if cellid_filters else '[]',
                        host_filters, [type(x).__name__ for x in host_filters] if host_filters else '[]')

            n1_df = fetch_cell_averages_for_period(conn, table, columns, n1_start, n1_end, "N-1", ne_filters=ne_filters, cellid_filters=cellid_filters, host_filters=host_filters)
            n_df = fetch_cell_averages_for_period(conn, table, columns, n_start, n_end, "N", ne_filters=ne_filters, cellid_filters=cellid_filters, host_filters=host_filters)
        finally:
            conn.close()
            logging.info("DB ì—°ê²° ì¢…ë£Œ")

        logging.info("ì§‘ê³„ ê²°ê³¼ í¬ê¸°: n-1=%dí–‰, n=%dí–‰", len(n1_df), len(n_df))
        if len(n1_df) == 0 or len(n_df) == 0:
            logging.warning("í•œìª½ ê¸°ê°„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ: ë¶„ì„ ì‹ ë¢°ë„ê°€ ë‚®ì•„ì§ˆ ìˆ˜ ìˆìŒ")

        # íŒŒìƒ PEG ì •ì˜ ì²˜ë¦¬ (ì‚¬ìš©ì ì •ì˜ ìˆ˜ì‹)
        # ì…ë ¥ ì˜ˆ: "peg_definitions": {"telus_RACH_Success": "Random_access_preamble_count/Random_access_response*100"}
        derived_defs = request.get('peg_definitions') or {}
        derived_n1 = compute_derived_pegs_for_period(n1_df, derived_defs, "N-1") if derived_defs else pd.DataFrame(columns=["peg_name","avg_value","period"])
        derived_n  = compute_derived_pegs_for_period(n_df, derived_defs, "N") if derived_defs else pd.DataFrame(columns=["peg_name","avg_value","period"])

        if not derived_n1.empty or not derived_n.empty:
            n1_df = pd.concat([n1_df, derived_n1], ignore_index=True)
            n_df = pd.concat([n_df, derived_n], ignore_index=True)
            logging.info("íŒŒìƒ PEG ë³‘í•© ì™„ë£Œ: n-1=%dí–‰, n=%dí–‰", len(n1_df), len(n_df))

        # ì²˜ë¦¬ & ë¶„ì„ (íŒŒìƒ í¬í•¨) - ì›ë³¸ ë°ì´í„° ì§ì ‘ ì‚¬ìš©
        logging.info("ì›ë³¸ ë°ì´í„° ì§ì ‘ ì‚¬ìš©: n-1=%dí–‰, n=%dí–‰", len(n1_df), len(n_df))

        processed_df = process_and_analyze(n1_df, n_df)
        logging.info("ì²˜ë¦¬ ì™„ë£Œ: processed_df=%dí–‰", len(processed_df))

        # LLM í”„ë¡¬í”„íŠ¸ & ë¶„ì„ (ëª¨í‚¹ ì œê±°: í•­ìƒ ì‹¤ì œ í˜¸ì¶œ)
        test_mode = False
        # í†µí•© í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ(YAML + Python í¬ë§¤íŒ…) ì‚¬ìš©
        try:
            from analysis_llm.config.prompt_loader import PromptLoader
            from analysis_llm.utils.data_formatter import format_dataframe_for_prompt

            loader = PromptLoader('config/prompts/v1.yaml')
            data_preview = format_dataframe_for_prompt(processed_df)
            prompt = loader.format_prompt(
                'enhanced',
                n1_range=n1_text,
                n_range=n_text,
                data_preview=data_preview,
            )
        except Exception as e:
            logging.exception("í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨(enhanced). ìµœì†Œ ë¬¸ìì—´ í´ë°± ì‚¬ìš©: %s", e)
            try:
                from analysis_llm.utils.data_formatter import format_dataframe_for_prompt as _fmt
                _fb_preview = _fmt(processed_df)
            except Exception:
                try:
                    _fb_preview = processed_df.head(100).to_string(index=False)
                except Exception:
                    _fb_preview = "<ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨>"
            prompt = (
                f"[Fallback Prompt]\n"
                f"ê¸°ê°„ n-1: {n1_text}\n"
                f"ê¸°ê°„ n: {n_text}\n\n"
                f"[ë°ì´í„° í‘œ]\n{_fb_preview}\n"
                f"[ë¶„ì„ ì§€ì¹¨]\n- ì£¼ìš” ë³€í™” ìš”ì•½, ê°€ì„¤, ê²€ì¦ ê³„íšì„ JSONìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”."
            )
        prompt_tokens = estimate_prompt_tokens(prompt)
        logging.info("í”„ë¡¬í”„íŠ¸ ê¸¸ì´: %dì, ì¶”ì • í† í°=%d", len(prompt), prompt_tokens)

        # í•˜ë“œ ê°€ë“œ: ì•ˆì „ ìƒí•œ ì ìš© (ìš”ì²­ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        max_tokens = int(request.get('max_prompt_tokens', DEFAULT_MAX_PROMPT_TOKENS))
        max_chars = int(request.get('max_prompt_chars', DEFAULT_MAX_PROMPT_CHARS))
        logging.info(
            "í”„ë¡¬í”„íŠ¸ ìƒí•œ ì„¤ì •: max_tokens=%d, max_chars=%d",
            max_tokens, max_chars
        )
        if prompt_tokens > max_tokens or len(prompt) > max_chars:
            logging.warning(
                "í”„ë¡¬í”„íŠ¸ ìƒí•œ ì´ˆê³¼: tokens=%d/%d, chars=%d/%d â†’ ìë™ ì¶•ì•½",
                prompt_tokens, max_tokens, len(prompt), max_chars
            )
            prompt, clamped = clamp_prompt(prompt, max_chars)
            logging.info("í”„ë¡¬í”„íŠ¸ ì¶•ì•½ ê²°ê³¼: ê¸¸ì´=%dì, clamped=%s", len(prompt), clamped)
        
        try:
            t0 = time.perf_counter()
            llm_analysis = query_llm(prompt, enable_mock=False)
            elapsed_ms = (time.perf_counter() - t0) * 1000.0
            try:
                import json as _json  # ì§€ì—­ importë¡œ ì•ˆì „ ì‚¬ìš©
                result_size = len(_json.dumps(llm_analysis, ensure_ascii=False).encode('utf-8')) if isinstance(llm_analysis, (dict, list)) else len(str(llm_analysis).encode('utf-8'))
            except Exception:
                result_size = -1
            logging.info(
                "LLM í˜¸ì¶œ ì™„ë£Œ: ì†Œìš”=%.1fms, ê²°ê³¼íƒ€ì…=%s, ê²°ê³¼í¬ê¸°=%dB",
                elapsed_ms,
                type(llm_analysis),
                result_size,
            )
            if isinstance(llm_analysis, dict):
                logging.info("LLM ê²°ê³¼ í‚¤: %s", list(llm_analysis.keys()))
        except ConnectionError as e:
            # ì‹¤íŒ¨ ì»¨í…ìŠ¤íŠ¸ ë¡œê¹…(í”„ë¡¬í”„íŠ¸ ì¼ë¶€, ìƒí•œê°’)
            prompt_head = (prompt or "")[:1000]
            logging.error(
                "LLM í˜¸ì¶œ ì‹¤íŒ¨: %s\n- prompt head: %s\n- limits: tokens=%d, chars=%d",
                e,
                prompt_head,
                max_tokens,
                max_chars,
            )
            # ëª¨í‚¹ ì œê±°: ì‹¤íŒ¨ ì‹œ ìƒìœ„ë¡œ ì˜ˆì™¸ ì „íŒŒ
            raise

        # 'íŠ¹ì • peg ë¶„ì„' ì²˜ë¦¬: preference / peg_definitions / selected_pegs
        try:
            selected_pegs: list[str] = []
            # 1) ëª…ì‹œì  ì„ íƒ ëª©ë¡
            explicit_list = request.get('selected_pegs')
            if isinstance(explicit_list, list):
                selected_pegs.extend([str(x) for x in explicit_list])

            # 2) preference ê¸°ë°˜ ì„ íƒ (ì •í™•í•œ peg_nameë¡œë§Œ í•´ì„)
            pref = request.get('preference')
            if isinstance(pref, str):
                pref_tokens = [t.strip() for t in pref.split(',') if t.strip()]
            elif isinstance(pref, list):
                pref_tokens = [str(t).strip() for t in pref if str(t).strip()]
            else:
                pref_tokens = []

            if pref_tokens:
                valid_names_set = set(processed_df['peg_name'].astype(str).tolist())
                for token in pref_tokens:
                    if token in valid_names_set:
                        selected_pegs.append(token)

            # ìœ ë‹ˆí¬ + ìˆœì„œ ìœ ì§€ + ì‹¤ë°ì´í„° ì¡´ì¬ í•„í„°ë§
            unique_selected = []
            seen = set()
            valid_names = set(processed_df['peg_name'].astype(str).tolist())
            for name in selected_pegs:
                if name in valid_names and name not in seen:
                    unique_selected.append(name)
                    seen.add(name)

            logging.info("íŠ¹ì • PEG ì„ íƒ ê²°ê³¼: %dê°œ", len(unique_selected))

            if unique_selected:
                subset_df = processed_df[processed_df['peg_name'].isin(unique_selected)].copy()
                # LLMì— ë³´ë‚¼ ìˆ˜ ìˆëŠ” í–‰ìˆ˜/í† í° ë³´í˜¸ë¥¼ ìœ„í•´ ìƒí•œì„ ë‘˜ ìˆ˜ ìˆìŒ(ì˜ˆ: 500í–‰). í•„ìš” ì‹œ ì¡°ì •
                max_rows = int(request.get('specific_max_rows', DEFAULT_SPECIFIC_MAX_ROWS))
                if len(subset_df) > max_rows:
                    logging.warning("ì„ íƒ PEG ì„œë¸Œì…‹ì´ %dí–‰ìœ¼ë¡œ í¼. ìƒí•œ %dí–‰ìœ¼ë¡œ ì ˆë‹¨", len(subset_df), max_rows)
                    subset_df = subset_df.head(max_rows)

                # í†µí•© í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì‚¬ìš© (specific_pegs)
                try:
                    from analysis_llm.config.prompt_loader import PromptLoader
                    from analysis_llm.utils.data_formatter import format_dataframe_for_prompt

                    loader = PromptLoader('config/prompts/v1.yaml')
                    sp_data_preview = format_dataframe_for_prompt(subset_df)
                    sp_prompt = loader.format_prompt(
                        'specific_pegs',
                        n1_range=n1_text,
                        n_range=n_text,
                        data_preview=sp_data_preview,
                        selected_pegs_str=', '.join(unique_selected),
                    )
                except Exception as e:
                    logging.exception("í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨(specific_pegs). ìµœì†Œ ë¬¸ìì—´ í´ë°± ì‚¬ìš©: %s", e)
                    try:
                        from analysis_llm.utils.data_formatter import format_dataframe_for_prompt as _fmt
                        _sp_fb_preview = _fmt(subset_df)
                    except Exception:
                        try:
                            _sp_fb_preview = subset_df.head(100).to_string(index=False)
                        except Exception:
                            _sp_fb_preview = "<ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì‹¤íŒ¨>"
                    sp_prompt = (
                        f"[Fallback Prompt - Specific PEGs]\n"
                        f"ëŒ€ìƒ PEG: {', '.join(unique_selected)}\n"
                        f"ê¸°ê°„ n-1: {n1_text}\n"
                        f"ê¸°ê°„ n: {n_text}\n\n"
                        f"[ë°ì´í„° í‘œ]\n{_sp_fb_preview}\n"
                        f"[ë¶„ì„ ì§€ì¹¨]\n- ê° PEG í†µì°°ê³¼ ìš°ì„ ìˆœìœ„ ì•¡ì…˜ì„ JSONìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”."
                    )
                sp_tokens = estimate_prompt_tokens(sp_prompt)
                logging.info("íŠ¹ì • PEG í”„ë¡¬í”„íŠ¸ ê¸¸ì´: %dì, ì¶”ì • í† í°=%d, ì„ íƒ PEG=%dê°œ", len(sp_prompt), sp_tokens, len(unique_selected))
                if sp_tokens > max_tokens or len(sp_prompt) > max_chars:
                    logging.warning(
                        "íŠ¹ì • PEG í”„ë¡¬í”„íŠ¸ ìƒí•œ ì´ˆê³¼: tokens=%d/%d, chars=%d/%d â†’ ì¶•ì•½",
                        sp_tokens, max_tokens, len(sp_prompt), max_chars
                    )
                    sp_prompt, sp_clamped = clamp_prompt(sp_prompt, max_chars)
                    logging.info("íŠ¹ì • PEG í”„ë¡¬í”„íŠ¸ ì¶•ì•½: ê¸¸ì´=%dì, clamped=%s", len(sp_prompt), sp_clamped)
                
                sp_result = query_llm(sp_prompt, enable_mock=False)
                
                if isinstance(llm_analysis, dict):
                    llm_analysis['specific_peg_analysis'] = {
                        "selected_pegs": unique_selected,
                        **(sp_result if isinstance(sp_result, dict) else {"summary": str(sp_result)})
                    }
                logging.info("íŠ¹ì • PEG ë¶„ì„ ì™„ë£Œ: keys=%s", list((llm_analysis.get('specific_peg_analysis') or {}).keys()))
        except Exception as e:
            logging.exception("íŠ¹ì • PEG ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: %s", e)

        # HTML ë¦¬í¬íŠ¸ ìƒì„± ìƒëµ

        # ë°±ì—”ë“œ POST payload êµ¬ì„± (AnalysisResultCreate ìŠ¤í‚¤ë§ˆì— ë§ì¶¤)
        # - stats: {period, kpi_name, avg} ë°°ì—´ë¡œ ë³€í™˜
        # - ì¶”ê°€ ë©”íƒ€ëŠ” analysis ë˜ëŠ” request_paramsë¡œ ìˆ˜ìš©

        def _to_stats(df: pd.DataFrame, period_label: str) -> list[dict]:
            items: list[dict] = []
            if df is None or df.empty:
                return items
            # ê¸°ëŒ€ ì»¬ëŸ¼: peg_name, avg_value
            try:
                for row in df.itertuples(index=False):
                    items.append({
                        "period": period_label,
                        "kpi_name": str(getattr(row, "peg_name")),
                        "avg": float(getattr(row, "avg_value"))
                    })
            except Exception:
                # ì»¬ëŸ¼ ëª…ì´ ë‹¤ë¥¼ ê²½ìš° ë³´í˜¸ì  ì ‘ê·¼
                if "peg_name" in df.columns and "avg_value" in df.columns:
                    for peg, val in zip(df["peg_name"], df["avg_value"]):
                        items.append({"period": period_label, "kpi_name": str(peg), "avg": float(val)})
            return items

        stats_records: list[dict] = []
        try:
            stats_records.extend(_to_stats(n1_df, "N-1"))
            stats_records.extend(_to_stats(n_df, "N"))
        except Exception as e:
            logging.warning("stats ë³€í™˜ ì‹¤íŒ¨, ë¹ˆ ë°°ì—´ë¡œ ëŒ€ì²´: %s", e)
            stats_records = []

        # ìš”ì²­ íŒŒë¼ë¯¸í„°(ì…ë ¥ ì»¨í…ìŠ¤íŠ¸) ìˆ˜ì§‘
        request_params = {
            "db": db,
            "table": table,
            "columns": columns,
            "time_ranges": {
                "n_minus_1": {"start": n1_start.isoformat(), "end": n1_end.isoformat()},
                "n": {"start": n_start.isoformat(), "end": n_end.isoformat()}
            },
            "filters": {
                "ne": ne_filters,
                "cellid": cellid_filters
            },
            "preference": request.get("preference"),
            "selected_pegs": request.get("selected_pegs"),
            "peg_definitions": request.get("peg_definitions")
        }

        # ëŒ€í‘œ ne/cell ID (ì—†ìœ¼ë©´ ALL) - ëª…ì‹œì  string ë³€í™˜ìœ¼ë¡œ íƒ€ì… ë³´ì¥
        ne_id_repr = str(ne_filters[0]).strip() if ne_filters else "ALL"
        cell_id_repr = str(cellid_filters[0]).strip() if cellid_filters else "ALL"
        logging.info("ëŒ€í‘œ ID ì„¤ì •: ne_id_repr=%s (type: %s), cell_id_repr=%s (type: %s)",
                    ne_id_repr, type(ne_id_repr).__name__, cell_id_repr, type(cell_id_repr).__name__)

        # ë¶„ì„ ì„¹ì…˜ì— LLM ê²°ê³¼ + ì°¨íŠ¸/ê°€ì •/ì›ë³¸ ë©”íƒ€ í¬í•¨
        analysis_section = {
            **(llm_analysis if isinstance(llm_analysis, dict) else {"summary": str(llm_analysis)}),
            "assumptions": {"same_environment": True},
            "source_metadata": {
                "db_config": db,
                "table": table,
                "columns": columns,
                "ne_id": ne_id_repr,
                "cell_id": cell_id_repr
            }
        }

        # ê²°ê³¼ ìš”ì•½ ìƒì„±
        results_overview = build_results_overview(llm_analysis)

        # ìµœì¢… payload (ëª¨ë¸ aliasë¥¼ ì‚¬ìš©: analysisDate, neId, cellId) - íƒ€ì… ë³´ì¥
        result_payload = {
            # ì„œë²„ Pydantic ëª¨ë¸ì€ by_alias=Falseë¡œ ì €ì¥í•˜ë¯€ë¡œ snake_case ë³´ì¥
            "analysis_type": "llm_analysis",
            "analysisDate": datetime.datetime.now(tz=_get_default_tzinfo()).isoformat(),
            "neId": str(ne_id_repr).strip() if ne_id_repr != "ALL" else "ALL",
            "cellId": str(cell_id_repr).strip() if cell_id_repr != "ALL" else "ALL",
            "status": "success",
            "results": [],
            "stats": stats_records,
            "analysis": analysis_section,
            "resultsOverview": results_overview,
            "request_params": request_params
        }
        try:
            import json as _json
            payload_size = len(_json.dumps(result_payload, ensure_ascii=False).encode('utf-8'))
            logging.info("ë°±ì—”ë“œ ì „ì†¡ payload í¬ê¸°: %dB, stats_rows=%d", payload_size, len(result_payload.get("stats", [])))
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ payload í¬ê¸° ì œí•œ ì„¤ì • (ê¸°ë³¸ê°’: 1MB)
            max_payload_size = int(os.getenv('MAX_PAYLOAD_SIZE_MB', '1')) * 1024 * 1024
            if payload_size > max_payload_size:
                logging.warning("payload í¬ê¸° %dMB ì´ˆê³¼: %dB", max_payload_size // (1024 * 1024), payload_size)
        except Exception as _e:
            logging.warning("payload í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: %s", _e)
        logging.info("payload ì¤€ë¹„ ì™„ë£Œ: stats_rows=%d, neId=%s (type: %s), cellId=%s (type: %s)",
                    len(result_payload.get("stats", [])),
                    result_payload.get("neId"), type(result_payload.get("neId")).__name__,
                    result_payload.get("cellId"), type(result_payload.get("cellId")).__name__)

        backend_response = None
        if backend_url:
            backend_response = post_results_to_backend(backend_url, result_payload)
            logging.info("ë°±ì—”ë“œ ì‘ë‹µ íƒ€ì…: %s", type(backend_response))

        logging.info("=" * 20 + " Cell ì„±ëŠ¥ ë¶„ì„ ë¡œì§ ì‹¤í–‰ ì¢…ë£Œ (ì„±ê³µ) " + "=" * 20)
        return {
            "status": "success",
            "message": "ë¶„ì„ ì™„ë£Œ",
            "backend_response": backend_response,
            "analysis": llm_analysis,
            "stats": processed_df.to_dict(orient='records'),
        }
    except ValueError as e:
        logging.error("ì…ë ¥/ì²˜ë¦¬ ì˜¤ë¥˜: %s", e)
        return {"status": "error", "message": f"ì…ë ¥/ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"}
    except ConnectionError as e:
        logging.error("ì—°ê²° ì˜¤ë¥˜: %s", e)
        return {"status": "error", "message": f"ì—°ê²° ì˜¤ë¥˜: {str(e)}"}
    except Exception as e:
        logging.exception("ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ")
        return {"status": "error", "message": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"}


@mcp.tool
def analyze_cell_performance_with_llm(request: dict) -> dict:
    """
    MCP ì—”ë“œí¬ì¸íŠ¸: ì‹œê°„ ë²”ìœ„ ê¸°ë°˜ í†µí•© ì…€ ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
    
    ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜: MCPHandler -> AnalysisService -> ê°ì¢… Repository/Service íŒ¨í„´
    """
    # ìƒˆë¡œìš´ MCPHandler ì‚¬ìš©
    with MCPHandler() as handler:
        return handler.handle_request(request)


# ===========================================
# End-to-End Integration & Testing
# ===========================================

def initialize_integrated_components():
    """
    ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ í†µí•© ì´ˆê¸°í™”
    
    Returns:
        tuple: (mcp_handler, analysis_service, logger) í†µí•©ëœ ì»´í¬ë„ŒíŠ¸ë“¤
    """
    logger = logging.getLogger(__name__ + '.integration')
    logger.info("=== í†µí•© ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘ ===")
    
    try:
        # 1ë‹¨ê³„: Configuration Managerì™€ ë¡œê¹… ì´ˆê¸°í™”
        logger.info("1ë‹¨ê³„: Configuration Manager ì´ˆê¸°í™”")
        settings = get_app_settings()
        logger.info("âœ… Configuration Manager ë¡œë“œ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: Core Utilities ì´ˆê¸°í™” (ìµœì†Œ ì˜ì¡´ì„±)
        logger.info("2ë‹¨ê³„: Core Utilities ì´ˆê¸°í™”")
        from services import PEGCalculator
        from utils import DataProcessor, RequestValidator, ResponseFormatter, TimeRangeParser
        
        time_parser = TimeRangeParser()
        peg_calculator = PEGCalculator()
        data_processor = DataProcessor()
        request_validator = RequestValidator(time_parser=time_parser)
        response_formatter = ResponseFormatter()
        
        logger.info("âœ… Core Utilities ì´ˆê¸°í™” ì™„ë£Œ: TimeRangeParser, PEGCalculator, DataProcessor, RequestValidator, ResponseFormatter")
        
        # 3ë‹¨ê³„: Repository Layer ì´ˆê¸°í™”
        logger.info("3ë‹¨ê³„: Repository Layer ì´ˆê¸°í™”")
        from repositories import LLMClient, PostgreSQLRepository

        # PostgreSQL Repository (DB ì„¤ì • ì£¼ì…)
        db_repository = PostgreSQLRepository()
        logger.info("âœ… PostgreSQLRepository ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM Repository (LLM ì„¤ì • ì£¼ì…)
        llm_repository = LLMClient()
        logger.info("âœ… LLMClient ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 4ë‹¨ê³„: Service Layer ì´ˆê¸°í™”
        logger.info("4ë‹¨ê³„: Service Layer ì´ˆê¸°í™”")
        from services import AnalysisService, LLMAnalysisService, PEGProcessingService

        # PEG Processing Service (DB Repository + PEG Calculator ì£¼ì…)
        peg_processing_service = PEGProcessingService(
            database_repository=db_repository,
            peg_calculator=peg_calculator
        )
        logger.info("âœ… PEGProcessingService ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM Analysis Service (LLM Repository ì£¼ì…)
        llm_analysis_service = LLMAnalysisService(
            llm_repository=llm_repository
        )
        logger.info("âœ… LLMAnalysisService ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Analysis Service (ëª¨ë“  ì„œë¹„ìŠ¤ í†µí•©)
        analysis_service = AnalysisService(
            database_repository=db_repository,
            peg_processing_service=peg_processing_service,
            llm_analysis_service=llm_analysis_service,
            time_parser=time_parser,
            data_processor=data_processor
        )
        logger.info("âœ… AnalysisService ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 5ë‹¨ê³„: Presentation Layer ì´ˆê¸°í™” (MCPHandler)
        logger.info("5ë‹¨ê³„: Presentation Layer ì´ˆê¸°í™”")
        
        # MCPHandler (ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì£¼ì…)
        mcp_handler = MCPHandler(
            request_validator=request_validator,
            analysis_service=analysis_service,
            response_formatter=response_formatter
        )
        logger.info("âœ… MCPHandler ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("=== í†µí•© ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ ===")
        logger.info("ì´ˆê¸°í™”ëœ ì»´í¬ë„ŒíŠ¸: MCPHandler, AnalysisService, 7ê°œ ìœ í‹¸ë¦¬í‹°/ì„œë¹„ìŠ¤")
        
        return mcp_handler, analysis_service, logger
        
    except Exception as e:
        logger.error("í†µí•© ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: %s", e)
        logger.exception("ì´ˆê¸°í™” ì˜¤ë¥˜ ìƒì„¸ ì •ë³´")
        raise


def run_end_to_end_test():
    """
    End-to-End í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ í†µí•©ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    logger = logging.getLogger(__name__ + '.e2e_test')
    logger.info("=== End-to-End í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    try:
        # 1ë‹¨ê³„: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logger.info("1ë‹¨ê³„: í†µí•© ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”")
        mcp_handler, analysis_service, integration_logger = initialize_integrated_components()
        
        # 2ë‹¨ê³„: ìƒ˜í”Œ MCP ìš”ì²­ ì •ì˜
        logger.info("2ë‹¨ê³„: ìƒ˜í”Œ MCP ìš”ì²­ ì •ì˜")
        sample_request = {
            "n_minus_1": "2025-01-01_09:00~2025-01-01_18:00",
            "n": "2025-01-02_09:00~2025-01-02_18:00",
            "output_dir": "./test_analysis_output",
            "table": "summary",
            "analysis_type": "enhanced",
            "enable_mock": True,  # í…ŒìŠ¤íŠ¸ìš© Mock ëª¨ë“œ
            "max_prompt_tokens": 8000,
            "db": {
                "host": "localhost",
                "port": 5432,
                "dbname": "test_db",
                "user": "test_user",
                "password": "test_pass"
            },
            "filters": {
                "ne": "nvgnb#10000",
                "cellid": ["2010", "2011"],
                "host": "192.168.1.1"
            },
            "selected_pegs": ["preamble_count", "response_count"],
            "peg_definitions": {
                "success_rate": "response_count/preamble_count*100"
            }
        }
        
        logger.info("âœ… ìƒ˜í”Œ ìš”ì²­ ì •ì˜ ì™„ë£Œ: %dê°œ í•„ë“œ", len(sample_request))
        
        # 3ë‹¨ê³„: End-to-End ìš”ì²­ ì²˜ë¦¬
        logger.info("3ë‹¨ê³„: End-to-End ìš”ì²­ ì²˜ë¦¬ ì‹¤í–‰")
        
        start_time = time.time()
        response = mcp_handler.handle_request(sample_request)
        end_time = time.time()
        
        processing_time = end_time - start_time
        logger.info("âœ… ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: %.2fì´ˆ ì†Œìš”", processing_time)
        
        # 4ë‹¨ê³„: ì‘ë‹µ ê²€ì¦
        logger.info("4ë‹¨ê³„: ì‘ë‹µ ê²€ì¦")
        
        if not response:
            raise ValueError("ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
        
        if not isinstance(response, dict):
            raise ValueError(f"ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(response)}")
        
        response_status = response.get('status', 'unknown')
        response_keys = list(response.keys())
        
        logger.info("ì‘ë‹µ ìƒíƒœ: %s", response_status)
        logger.info("ì‘ë‹µ í‚¤: %s", response_keys)
        
        if response_status == 'success':
            logger.info("âœ… End-to-End í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            
            # ì‘ë‹µ ì„¸ë¶€ ì •ë³´ ë¡œê¹…
            if 'data_summary' in response:
                data_summary = response['data_summary']
                logger.info("ë°ì´í„° ìš”ì•½: %s", data_summary)
            
            if 'peg_analysis' in response:
                peg_analysis = response['peg_analysis']
                peg_count = len(peg_analysis.get('results', []))
                logger.info("PEG ë¶„ì„ ê²°ê³¼: %dê°œ", peg_count)
            
            if 'llm_analysis' in response:
                llm_analysis = response['llm_analysis']
                model_used = llm_analysis.get('model_used', 'unknown')
                logger.info("LLM ë¶„ì„ ëª¨ë¸: %s", model_used)
            
            if 'metadata' in response:
                metadata = response['metadata']
                request_id = metadata.get('request_id', 'unknown')
                logger.info("ìš”ì²­ ID: %s", request_id)
        
        elif response_status == 'error':
            error_message = response.get('message', 'Unknown error')
            logger.warning("âš ï¸ End-to-End í…ŒìŠ¤íŠ¸ì—ì„œ ì˜¤ë¥˜ ì‘ë‹µ ë°›ìŒ: %s", error_message)
            
            # ì˜¤ë¥˜ì—¬ë„ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë‹µì„ ë°˜í™˜í–ˆìœ¼ë¯€ë¡œ ë¶€ë¶„ì  ì„±ê³µ
            logger.info("âœ… ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ í™•ì¸")
        
        else:
            logger.warning("âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ ìƒíƒœ: %s", response_status)
        
        # 5ë‹¨ê³„: ì»´í¬ë„ŒíŠ¸ ìƒíƒœ ê²€ì¦
        logger.info("5ë‹¨ê³„: ì»´í¬ë„ŒíŠ¸ ìƒíƒœ ê²€ì¦")
        
        # AnalysisService ìƒíƒœ í™•ì¸
        service_info = analysis_service.get_service_info()
        workflow_status = analysis_service.get_workflow_status()
        
        logger.info("AnalysisService ì •ë³´: %s", service_info)
        logger.info("ì›Œí¬í”Œë¡œìš° ìƒíƒœ: %s", workflow_status)
        
        logger.info("=== End-to-End í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        logger.info("ì „ì²´ ì²˜ë¦¬ ì‹œê°„: %.2fì´ˆ", processing_time)
        logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼: %s", "ì„±ê³µ" if response_status in ['success', 'error'] else "ë¶€ë¶„ì  ì„±ê³µ")
        
        return response
        
    except Exception as e:
        logger.error("End-to-End í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: %s", e)
        logger.exception("í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ ìƒì„¸ ì •ë³´")
        
        # ì‹¤íŒ¨í•´ë„ ì˜¤ë¥˜ ì •ë³´ë¥¼ ë°˜í™˜
        return {
            "status": "test_error",
            "message": f"End-to-End í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}",
            "error_type": type(e).__name__
        }


if __name__ == '__main__':
    import sys

    # End-to-End í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if len(sys.argv) > 1 and sys.argv[1] == "--e2e-test":
        print("=" * 60)
        print("End-to-End Integration Test")
        print("=" * 60)
        
        # ë¡œê¹… ì„¤ì •
        # ê³µí†µ ì„¤ì • ì‚¬ìš©(ì´ë¯¸ get_app_settings() í˜¸ì¶œ ì‹œ ì„¤ì •ë¨). ì¶”ê°€ íŒŒì¼ í•¸ë“¤ëŸ¬ë§Œ ë§ë¶™ì„
        from config.settings import get_settings
        settings = get_settings()
        if settings.log_file_enabled:
            pass  # íŒŒì¼ ë¡œê¹…ì€ settings.setup_loggingì—ì„œ ì²˜ë¦¬ë¨
        else:
            fh = logging.FileHandler('e2e_test.log', mode='w', encoding='utf-8')
            fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
            logging.getLogger().addHandler(fh)
        
        try:
            result = run_end_to_end_test()
            print("\n" + "=" * 60)
            print("í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(json.dumps(result, ensure_ascii=False, indent=2))
            print("=" * 60)
            
            # ì„±ê³µ ì‹œ 0, ì‹¤íŒ¨ ì‹œ 1ë¡œ ì¢…ë£Œ
            sys.exit(0 if result.get('status') in ['success', 'error'] else 1)
            
        except Exception as e:
            print(f"\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            sys.exit(1)
    
    # CLI ëª¨ë“œ ì§€ì›: Backendì—ì„œ í”„ë¡œì„¸ìŠ¤ë¡œ í˜¸ì¶œ ì‹œ ì‚¬ìš©
    elif len(sys.argv) > 2 and sys.argv[1] == "--request":
        try:
            request_json = sys.argv[2]
            request_data = json.loads(request_json)
            
            logging.info("CLI ëª¨ë“œë¡œ LLM ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            logging.info("ìš”ì²­ ë°ì´í„°: %s", json.dumps(request_data, ensure_ascii=False, indent=2))
            
            # í†µí•©ëœ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
            mcp_handler, _, _ = initialize_integrated_components()
            result = mcp_handler.handle_request(request_data)
            
            # JSON ê²°ê³¼ ì¶œë ¥ (Backendì—ì„œ capture)
            print(json.dumps(result, ensure_ascii=False))
            
            # ì„±ê³µ ì¢…ë£Œ
            sys.exit(0)
            
        except Exception as e:
            logging.exception("CLI ëª¨ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", e)
            error_result = {
                "status": "error",
                "message": f"CLI ëª¨ë“œ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
            }
            print(json.dumps(error_result, ensure_ascii=False))
            sys.exit(1)
    else:
        logging.info("streamable-http ëª¨ë“œë¡œ MCPë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        mcp.run(transport="streamable-http", host="0.0.0.0", port=8001)